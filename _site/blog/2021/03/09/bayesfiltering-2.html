<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="ie ie6" lang="en"> <![endif]-->
<!--[if IE 7 ]><html class="ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html lang="en"> <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Giseop Kim - Bayesian Filtering [2편] — Recursive estimation 의 시작 (칼만필터 유도)</title>
  <meta name="author" content="Giseop Kim" />
  <meta name="description" content="The blog of Giseop Kim" />
  <link rel="canonical" href="http://localhost:4000/blog/2021/03/09/bayesfiltering-2.html" />

  <link href="//fonts.googleapis.com/css?family=Open+Sans:600,800" rel="stylesheet" type="text/css">
  <link rel="shortcut icon" href="/favicon_logo.png" type="image/vnd.microsoft.icon">
  <link rel="alternate" type="application/atom+xml" title="Giseop Kim" href="http://localhost:4000/atom.xml" />

  <!-- see the how-to-use of academicon here http://jpswalsh.github.io/academicons/ -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/all.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">

  <!-- 
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
          inlineMath: [ ['$', '$'] ],
          displayMath: [ ['$$', '$$'] ],
          processEscapes: true,
        }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->

<!-- Mathjax Support -->
<!-- <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script> -->

   -->

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
    </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

    
</head>
<body>
  <div class="container">
    <div class="four columns sidebar">
      <nav>
  <!-- <h1>Hi,</h1> -->

  <!-- <hr class="solid" style="margin-top:+20px"> -->
  
  <p style="margin-top:-7px"> </p>

  <a href="/">
    
    <img src="/logo.png" id="logo" alt="Blog logo" width="150"/>
    
  </a>

  <h2 style="margin-top:-15px"> 
    <!-- <a href="/" style="font-weight:bold" target="_blank">Giseop Kim</a> <a href="/"> blog </a>  -->
    <a href="/" style="font-weight:bold">Giseop Kim</a> <a href="/"> blog </a> 
  </h2>

  <hr class="solid" style="margin-top:+15px">

  <p style="margin-top:-15px">
  </p>

  <div id="bio">
    <br><br> 
    <p style="margin-top:-65px">
      <!-- <span style="vertical-align:150%"> </span><br> -->
      <center> Ph.D. candidate at <a href="https://irap.kaist.ac.kr/" target="_blank"> KAIST </a> </center>
      <!-- <span style="vertical-align:-15%"> </span><br> -->
      <center style="font-size:0.95em"> SLAM researcher/engineer </center>
      <center style="font-size:0.95em"> Spatial AI Enthusiast </center>
    </p>  
    <p>
      <!-- <span style="vertical-align:-15%"> </span><br> -->
      <center style = "margin-top:-10px; font-size:0.9em; letter-spacing:1.3px;"> paulgkim@kaist.ac.kr </center> 
      <!-- font-family:Serif; -->
    </p>
    <!-- <p>
      Edit me in <code>_includes/sidebar.html</code>
    </p> -->
  </div>

  <p style="margin-top:-5px"> </p>
  <div id="social">
    <!-- <center> My channels </center> -->
<center>
<div id="stalker">

  <a title="home" href="/">
    <i class="fa fa-home"></i> 
  </a>

  <a title="notion cv blog" href="/aboutmedetail/" target="_blank">
    <i class="fa fa-user"></i> 
  </a>

  <a title="CV" href="https://github.com/gisbi-kim/cv-gskim" target="_blank">
    <i class="ai ai-cv"></i> 
  </a>
  
  <a title=" on Scholar" href="https://scholar.google.com/citations?user=9mKOLX8AAAAJ&hl=en" target="_blank">
    <i class="ai ai-google-scholar"></i> 
    <!-- google -->
  </a>

  
  <a title="gisbi-kim on Github" href="https://github.com/gisbi-kim" target="_blank">
    <i class="fa fa-github-square"></i> 
  </a>
  

  <a title=" on Youtube" href="https://www.youtube.com/channel/UCrmVMJ3KEFbDD9EtnAmDT6g" target="_blank">
    <i class="fa fa-youtube-square"></i> 
  </a>

  

  

  

  

  <!-- 
  <a title="GiseopK on Twitter" href="https://twitter.com/GiseopK" target="_blank">
    <i class="fa fa-twitter-square"></i>
  </a>
   -->

  

  

  

  

  

  
</div>
</center>
  </div>

</nav>

    </div>

    <div class="eleven columns content">

      <div class="header">
        <p style="margin-top:-38px"> </p>

<!-- <div class="disclaimer" style="margin-top:+5px"> -->

  <!-- <center style="margin-bottom:-15px">  <a href="/"> Giseop Kim blog </a> </center> -->
  <!-- <a title="home" style="float:right" href="/latestnews/">
    Latest News -->
    <!-- <i class="fa fa-home"></i>  -->
  <!-- </a> -->
  
  <!-- <span>
    <a title="home" style="float:left" href="/aboutme/"> -->
      <!-- About Me  -->
      <!-- <i class="fa fa-home"></i>  -->
    <!-- </a>  
  </span>   -->

  <!-- <span>
    <a title="home" style="float:right" href="/aboutme/"> 
      About Me  -->
      <!-- <i class="fa fa-home"></i>  -->
    <!-- </a>  
  </span>   -->

<!-- </div> -->
      </div> 
      <br>
      <hr class="solid" style="margin-top:+10px">
      <p style="margin-bottom:-20px">
      
      <p class="meta">
  
    [Filter-based SLAM] ― March 09, 2021   
   
  
  <a href="/">
    <i class="home fa fa-home"></i>
  </a>
</p>

<h1 class="title">Bayesian Filtering [2편] — Recursive estimation 의 시작 (칼만필터 유도)</h1>
<hr class="solidlight" style="margin-top:+20px">

<div id="post">
  <h1 id="개요">개요</h1>

<p><a href="/blog/2021/03/09/bayesfiltering-1.html#proofdone"> 앞선 포스팅 </a>에서 우리는 posterior 의 mean 과 covariance 가<br />
prior 와 likelihood 로부터 closed form 으로 유도됨을 살펴보았다.</p>

<p>근데 지난포스팅에서도 몇번 이야기 했듯이:</p>
<blockquote>
  <p>Bayesian analysis 의 묘미는 현재 턴의 posterior 가 다음 턴의 prior 로 쓰이는 것</p>
</blockquote>

<p>이다.</p>

<p><span style="color:gray"> [Filter-based SLAM] </span> 시리즈의 <a href="/blog/2021/03/09/bayesfiltering-1.html"> 이전 편 </a> 을 마무리했던 그 식 을 다시 들고와보자.</p>

\[\begin{equation}
  p(\textbf{x} | \textbf{z}) = \mathcal{N} \left( \text{mean: } \left(\textbf{P}_0^{-1} + \frac{1}{\sigma^2}\textbf{H}^{T}\textbf{H} \right)^{-1} \left( \textbf{P}_0^{-1} \textbf{m}_0 + \frac{1}{\sigma^2}\textbf{H}^{T}\textbf{z} \right), \\
  \ \text{covariance: } \left(\textbf{P}_0^{-1} + \frac{1}{\sigma^2}\textbf{H}^{T}\textbf{H} \right)^{-1} \right) \tag{eq.1}\label{eq:1}
\end{equation}\]

<p>이 식은 batch estimation 이었다고 할 수 있다.</p>
<ul>
  <li>즉 measurement ($\textbf{z} = \textbf{z}_{1:k}$) 들이 한번에 모두 주어져있고,</li>
  <li>우리가 알고싶은 state $\textbf{x}$ 에 대한 초기 mean $\textbf{m}_0$, covariance $\textbf{P}_0$ 가 initial value 로써 주어지는 상황이었다.</li>
</ul>

<p>이번 포스팅에서는 이것을 recursive version 으로 바꾸어 볼 것이다.</p>

<hr />
<h1 id="recursive-estimation">Recursive estimation</h1>

<p>사실 여기서부터는 쉽다.<br />
“현재 턴의 posterior 가 다음 턴의 prior 로 쓰인다”는 묘미를 기억하자.</p>

<p>식 \eqref{eq:1} 에서의 posterior의 mean 과 covariance 가<br />
각각 $\textbf{m}_{t-1}$, $\textbf{P}_{t-1}$ 였다고 하자.</p>

<p>그리고 지금은 time$=t$ 가 되어서,<br />
새로운 measurement $z_t$ 가 하나 들어온 상황이다.<br />
(꼭 딱 하나일 필요는 없지만 편의상 하나라고 해보자)</p>

<p>그러면 위의 식 \eqref{eq:1} 에서,<br />
time$=t-1$의 posterior 가 다음 턴 time$=t$의 prior 로 쓰이기 때문에,</p>
<ul>
  <li>$\textbf{P}_0$ 자리에 $\textbf{P}_{t-1}$ 라고 말만 다시 바꿔달면 된다.</li>
  <li>$\textbf{m}_0$ 자리에 $\textbf{m}_{t-1}$ 라고 말만 다시 바꿔달면 된다.</li>
  <li>measurement $\textbf{z}$ 자리에 $z_{t}$ 라고 말만 다시 쓰면 된다.</li>
  <li>그리고 measurement 의 jacobian 인 $\textbf{H}$는 현재 턴에 새로 들어온 measurement 의 것이므로 위의 식에서 $\textbf{H}$ 를 $\textbf{H}_{t}$ 라고 써주면 말이 된다.</li>
</ul>

<p>그러면 식은 다음과 같다:</p>

\[\begin{align}
  p(\textbf{x}_{t} | z_t) = \mathcal{N} \left(  \text{mean: } \left(\textbf{P}_{t-1}^{-1} + \frac{1}{\sigma^2}\textbf{H}_{t}^{T}\textbf{H}_{t} \right)^{-1} \left( \textbf{P}_{t-1}^{-1} \textbf{m}_{t-1} + \frac{1}{\sigma^2}\textbf{H}_{t}^{T}z_t \right), \\
   \text{covariance: } \left(\textbf{P}_{t-1}^{-1} + \frac{1}{\sigma^2}\textbf{H}_{t}^{T}\textbf{H}_{t} \right)^{-1} \right) \tag{eq.2}\label{eq:2}
\end{align}\]

<p>완성!</p>

<p>참 쉽죠?</p>

<p>우리는 이제 관측 데이터가 순차적으로 하나씩 들어오더라도,</p>

<p>예전 예측을 반영해서 최적해를 update 해나갈 수 있게 되었다!</p>

<hr />
<h1 id="recursive-estimation-continued">Recursive estimation: continued</h1>

<p>식을 조금만 더 수리해보자.</p>

<p>재밌는게 나올 수 있다.</p>

<p>위의 recursive 식에서 posterior (즉, update 된) 의 covariance 가<br />
$\left(\textbf{P}_{t-1}^{-1} + \frac{1}{\sigma^2}\textbf{H}_{t}^{T}\textbf{H}_{t} \right)^{-1}$ 였다.</p>

<p>지금 measurement 의 noise 를 scalar 라고 가정해서 식이 저런건데<br />
사실은 이렇게 써져있는 거라고 머릿속에서 식의 생김새를 생각해주자</p>

\[\begin{equation}
    \textbf{P}_{t} = \left(\textbf{P}_{t-1}^{-1} + \textbf{H}_{t}^{T}(\sigma^{-2}\textbf{I})\textbf{H}_{t} \right)^{-1}
    \tag{eq.3}\label{eq:3}
\end{equation}\]

<p><span style="color:gray"> (추후에 저 $\sigma^{-2}\textbf{I}$ 자리에 대신 임의의 noise matrix $\textbf{R}$ 같은걸로 교체해주기만 하면된다) </span></p>

<p>근데 선형대수 꿀팁 중에 matrix inversion lemma 라는 것이 있다 <sup id="fnref:woodbury" role="doc-noteref"><a href="#fn:woodbury" class="footnote">1</a></sup>.</p>

<p>대략 이런걸 해준다.</p>

\[\begin{equation}
  (\textbf{A} + \textbf{C} \textbf{B} \textbf{C}^{T})^{-1} = \textbf{A}^{-1} - \textbf{A}^{-1}\textbf{C} ( \textbf{C}^{T}\textbf{A}^{-1} \textbf{C} + \textbf{B}^{-1})^{-1} \textbf{C}^{T} \textbf{A}^{-1}
  \tag{eq.4}\label{eq:4}
\end{equation}\]

<p>근데 우리의 covariance \eqref{eq:3} 가 딱 보니 \eqref{eq:4} 의 좌변의 생김새와 완전히 같지 않은가?</p>

<p>이렇게 치환해서</p>

\[\begin{equation}
    \textbf{A} = \textbf{P}_{t-1}^{-1} \\ 
    \textbf{B} = \sigma^{-2}\textbf{I} \\ 
    \textbf{C} = \textbf{H}_{t}^{T}
\end{equation}\]

<p>식을 쭉 전개하면</p>

\[\begin{align}
    \textbf{P}_{t} &amp;= \left(\textbf{P}_{t-1}^{-1} + \textbf{H}_{t}^{T}(\sigma^{-2}\textbf{I})\textbf{H}_{t} \right)^{-1} \\
    &amp;= \textbf{P}_{t-1} - \textbf{P}_{t-1}\textbf{H}_{t}^{T} \left(  \textbf{H}_{t} \textbf{P}_{t-1} \textbf{H}_{t}^{T} + \sigma^{2}  \right)^{-1} \textbf{H}_{t}\textbf{P}_{t-1} \tag{eq.5}\label{eq:5}
\end{align}\]

<p>\eqref{eq:5} 가 된다!</p>

<p>와~~~</p>

<hr />
<h1 id="recursive-estimation-kalman-filter-등장">Recursive estimation: Kalman Filter 등장</h1>

<p>그래서 어쩌라고? 싶겠지만</p>

<p>한 번 더 식을 다듬어 보자!</p>

\[\begin{align}
    \textbf{P}_{t} &amp;= \left(\textbf{P}_{t-1}^{-1} + \textbf{H}_{t}^{T}(\sigma^{-2}\textbf{I})\textbf{H}_{t} \right)^{-1} \\
    &amp;= \textbf{P}_{t-1} - \underbrace{ \textbf{P}_{t-1}\textbf{H}_{t}^{T} \left( \underbrace{ \textbf{H}_{t} \textbf{P}_{t-1} \textbf{H}_{t}^{T} + \sigma^{2} }_{\textbf{S}_t} \right)^{-1} }_{\textbf{K}_t} \textbf{H}_{t}\textbf{P}_{t-1}  \\
    &amp;= \textbf{P}_{t-1} - \textbf{K}_t \textbf{S}_t  \textbf{K}_t^{T} \  \text{ // or } \\ 
    &amp;= \textbf{P}_{t-1} - \textbf{K}_t \textbf{H}_t \textbf{P}_{t-1} \  \text{ // seems more frequently used form} \\ 
    &amp;= ( \textbf{I} - \textbf{K}_t \textbf{H}_t)  \textbf{P}_{t-1} 
    \tag{eq.6}\label{eq:6}
\end{align}\]

<p>covariance 에 대한 update 식이 조금 더 깔쌈하게 만들어진 것을 알 수 있다.</p>

<p>근데 이거 어디서 좀 본 모양 같은데…?</p>

<p>학부 기계전기 등 로보틱스 관련수업에서 무조건 배우는 Kalman Filter 아닌가 …?</p>

<p>호옴 …</p>

<p>mean 에 대한 update 식도 좀 더 다듬어 보자.</p>

\[\begin{align}
\textbf{m}_{t} &amp;= \textbf{P}_t \left( \textbf{P}_{t-1}^{-1}\textbf{m}_{t-1} + \frac{1}{\sigma^2}\textbf{H}_{t}^{T}z_t \right) \\ 
    &amp;= ( \textbf{I} - \textbf{K}_t \textbf{H}_t)  \textbf{P}_{t-1}  \left( \textbf{P}_{t-1}^{-1}\textbf{m}_{t-1} + \frac{1}{\sigma^2}\textbf{H}_{t}^{T}z_t \right) \\ 
    &amp;= ( \textbf{I} - \textbf{K}_t \textbf{H}_t)  \left( \textbf{P}_{t-1}\textbf{P}_{t-1}^{-1}\textbf{m}_{t-1} + \textbf{P}_{t-1}\frac{1}{\sigma^2}\textbf{H}_{t}^{T}z_t \right) \\ 
    &amp;= ( \textbf{I} - \textbf{K}_t \textbf{H}_t)  \left( \textbf{m}_{t-1} + \frac{1}{\sigma^2}\textbf{P}_{t-1}\textbf{H}_{t}^{T} z_t \right) \\
    &amp;= \textbf{m}_{t-1} - \textbf{K}_t \textbf{H}_t \textbf{m}_{t-1} + \frac{1}{\sigma^{2}}( \textbf{I} - \textbf{K}_t \textbf{H}_t) (\textbf{P}_{t-1}\textbf{H}_{t}^{T}) z_t
    \tag{eq.7}\label{eq:7}
\end{align}\]

<p>근데 재밌는게</p>

\[\begin{align}
    \frac{1}{\sigma^{2}}( \textbf{I} - \textbf{K}_t \textbf{H}_t) (\textbf{P}_{t-1}\textbf{H}_{t}^{T}) = \textbf{K}_t
    \tag{eq.8}\label{eq:8}
\end{align}\]

<p>이다!<br />
양변에 곱하고 빼고 요리조리하면 direct 증명이 쉽게 가능하지만 굳이 또 적어보자면:</p>

\[\begin{align}
    \frac{1}{\sigma^{2}}( \textbf{I} - \textbf{K}_t \textbf{H}_t) (\textbf{P}_{t-1}\textbf{H}_{t}^{T}) &amp;= \textbf{K}_t \\
    ( \textbf{I} - \textbf{K}_t \textbf{H}_t) (\textbf{P}_{t-1}\textbf{H}_{t}^{T}) &amp;= \sigma^{2}\textbf{K}_t \\ 
    \textbf{P}_{t-1}\textbf{H}_{t}^{T} - \textbf{K}_t \textbf{H}_t \textbf{P}_{t-1}\textbf{H}_{t}^{T} &amp;= \sigma^{2}\textbf{K}_t \\ 
    \textbf{P}_{t-1}\textbf{H}_{t}^{T} &amp;= \textbf{K}_t \textbf{H}_t \textbf{P}_{t-1}\textbf{H}_{t}^{T} + \sigma^{2}\textbf{K}_t \\ 
    \textbf{P}_{t-1}\textbf{H}_{t}^{T} &amp;= \textbf{K}_t \left( \textbf{H}_t \textbf{P}_{t-1}\textbf{H}_{t}^{T} + \sigma^{2} \right) \\ 
    \textbf{P}_{t-1}\textbf{H}_{t}^{T} &amp;= \textbf{K}_t \textbf{S}_t \\ 
    \textbf{P}_{t-1}\textbf{H}_{t}^{T}\textbf{S}_t^{-1} &amp;= \textbf{K}_t    
\end{align}\]

<p>마지막 줄은 $\textbf{K}_t$의 정의였으므로 자명하고 따라서 성립한다.</p>

<p>암튼 그래서 \eqref{eq:8} 을 \eqref{eq:7} 에 대입하면,</p>

\[\begin{align}
\textbf{m}_{t} &amp;= \textbf{m}_{t-1} - \textbf{K}_t \textbf{H}_t \textbf{m}_{t-1} + \textbf{K}_t z_t \\
    &amp;= \textbf{m}_{t-1} + \textbf{K}_t \left( z_t - \textbf{H}_t \textbf{m}_{t-1} \right)
    \tag{eq.9}\label{eq:9} 
\end{align}\]

<p>오잉 근데 이거도 어디서 좀 많이 본 거 같은데 …?</p>

<p><a href="http://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/pdf/slam04-ekf.pdf"> Cyrill 교수님의 강의자료 </a>를 보면</p>

<figure id="kf">
  <img src="/figs/2021-03-12-bayesfiltering-2/kf.png" style="width:90%" />
  <figcaption> 
        <center> <a href="#kf"> Figure: KF equation </a> </center>
  </figcaption>
</figure>
<p><span> </span></p>

<p>4, 5, 6 번째 줄이 우리가 방금 유도한 $\textbf{K}_t$, $\textbf{m}_t$, $\textbf{P}_t$ 와 생김새가 같음을 알 수 있다!</p>

<p>그렇다면 <a href="#kf"> 위의 slide </a> 에서 2, 3번째 줄은 뭘까?</p>
<ul>
  <li>바로 Kalman filter 에서 prediction step 이라 불리는 부분이다.</li>
</ul>

<p>그리고 우리가 오늘 유도한 것은</p>
<ul>
  <li>Kalman filter 의 update step 에 해당하는 부분이다.</li>
</ul>

<p>즉, 우리는</p>
<ul>
  <li><a href="/blog/2021/03/09/bayesfiltering-1.html#proofdone"> 지난 포스팅 </a> 에서 batch 로 bayesian update 를 수행했고,</li>
  <li>이것이 자연스럽게 recursive 한 식이 됨을 이번 포스팅 앞 부분에서 소개했고,</li>
  <li>그것의 모양을 요래조래 하다보니 Kalman filter 의 update step 와 동일하다는 사실까지 오게되었다.</li>
</ul>

<p>즉, 오늘 우리는 그 유명한 Kalman filter 의 special 버전 (i.e., update step만 있는) 을 유도했다.</p>

<p>근데 update step만 있다는 것은 무슨 의미일까?</p>

<p>바로 우리가 예측하고자 하는 state $\textbf{x}$ 값이 들어오는 measurement 에 관계없이 “고정” 된 상태라는 뜻이다.</p>

<p>예를 들어서 line fitting 할 때 line 의 계수 같은 게 될 수 있을 것이다.</p>

<p>하지만 그 $\textbf{x}$ 가 “움직이는” robot 의 위치처럼, 계속 바뀌는 값이라면?</p>

<p>이 때 바로 prediction step 이 필요하게 된다.</p>

<hr />
<h2 id="예고">예고</h2>

<ul>
  <li>다음 편에서는 칼만필터를 완성해보겠습니다 <sup id="fnref:book" role="doc-noteref"><a href="#fn:book" class="footnote">2</a></sup>.</li>
</ul>

<hr />
<h2 id="ps">P.S.</h2>

<p>mean update 식을 다시 보면,</p>

\[\begin{align}
\textbf{m}_{t} &amp;= \textbf{m}_{t-1} - \textbf{K}_t \textbf{H}_t \textbf{m}_{t-1} + \textbf{K}_t z_t \\
    &amp;= \textbf{m}_{t-1} + \textbf{K}_t \left( z_t - \textbf{H}_t \textbf{m}_{t-1} \right)
\end{align}\]

<p>$\textbf{H}_t \textbf{m}_{t-1}$ 가 보이는데,<br />
이는 앞선 포스팅에서 소개한 <a href="/blog/2021/03/09/bayesfiltering-1.html#zmodel"> measurement model </a> 이다. 즉, $\textbf{H}_t \textbf{m}_{t-1} = \hat{z_{t}}$<br />
(hat 은 예측값이라는 의미)</p>

<p>따라서 위의 식을 의미상 다음과 같이 다시 적을 수 있는데:</p>

\[\begin{align}
\textbf{m}_{t} &amp;= \textbf{m}_{t-1} - \textbf{K}_t \textbf{H}_t \textbf{m}_{t-1} + \textbf{K}_t z_t \\
    &amp;= \textbf{m}_{t-1} + \textbf{K}_t \left( z_t - \hat{z_{t}} \right)
\end{align}\]

<p>여기서 어떤 냄새를 맡을 수 있다.</p>

<p>hat 은 예측값이라는 의미이기 때문에, $z_t - \hat{z_{t}}$ 의 의미는 예측값과 실측값의 차이라는 것을 알 수 있다.</p>

<p><a href="#kf"> 위의 slide </a> 에서 5번째 줄을 보면<br />
updated 될 mean 값은: 초벌로 먼저 predicted 한 mean (hyphen이 그어져있는 게 predicted 란 의미) 에 어떤 값을 보상해주는 것으로 해석할 수 있다.</p>

<p>그리고 그 보상의 정도는 $\textbf{K}$ 에 실측값과 예측값의 차이가 곱해진 정도이다.</p>

<p>이 때 그 보상의 정도를 조절하는 $\textbf{K}$ 가 바로 칼만필터에서 kalman gain 이라 불리는 것이다.</p>

<blockquote>
  <p>다음 편에서 계속 …</p>
</blockquote>

<hr />
<h3 id="주석">주석</h3>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:woodbury" role="doc-endnote">
      <p>Woodbury formula, Woodbury identity 등으로도 알려져있다. <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf"> 출처 matrix cookbook </a> <a href="#fnref:woodbury" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:book" role="doc-endnote">
      <p>이번편의 유도방식은 다음 책의 chapter 3.2 를 참고했습니다: Särkkä, Simo. Bayesian filtering and smoothing. No. 3. Cambridge University Press, 2013. <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.461.4042&amp;rep=rep1&amp;type=pdf"> PDF link <a></a></a> <a href="#fnref:book" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

</div>

<br>
<hr class="solid" style="margin-top:+30px">
<div id="related" style="margin-top:-20px">
  <h3>Other posts</h3>
  <ul class="posts" style="margin-top:-10px">
    
    <li>
      <!--  -->
      <!-- <span>27 Apr 2021 &raquo;</span>  -->
      <span style="color:gray; font-size:0.85em;"> [Visual SLAM] &raquo;</span>
      <a href="/blog/2021/04/27/msckf-history.html" style="font-size:0.85em;">Filter-based VIO [1편] — MSCKF 계열 history 정리</a>
      <!--  -->
    </li>
    
    <li>
      <!--  -->
      <!-- <span>16 Mar 2021 &raquo;</span>  -->
      <span style="color:gray; font-size:0.85em;"> [Least Square Opt.] &raquo;</span>
      <a href="/blog/2021/03/16/leastsquare-1.html" style="font-size:0.85em;">Iterative Optimization [1편] — Nonlinear ICP 구현을 통해 알아보는 Gauss-newton Optimization</a>
      <!--  -->
    </li>
    
    <li>
      <!--  -->
      <!-- <span>09 Mar 2021 &raquo;</span>  -->
      <span style="color:gray; font-size:0.85em;"> [Filter-based SLAM] &raquo;</span>
      <a href="/blog/2021/03/09/bayesfiltering-1.html" style="font-size:0.85em;">Bayesian Filtering [1편] — posterior 의 mean, covariance 구하기</a>
      <!--  -->
    </li>
    
    <li>
      <!--  -->
      <!-- <span>04 Mar 2021 &raquo;</span>  -->
      <span style="color:gray; font-size:0.85em;"> [Factor graph SLAM] &raquo;</span>
      <a href="/blog/2021/03/04/slambackend-3.html" style="font-size:0.85em;">SLAM back-end [2++편] — Householder QR 분해 구현해보기</a>
      <!--  -->
    </li>
    
    <li>
      <!--  -->
      <!-- <span>04 Mar 2021 &raquo;</span>  -->
      <span style="color:gray; font-size:0.85em;"> [Factor graph SLAM] &raquo;</span>
      <a href="/blog/2021/03/04/slambackend-2.html" style="font-size:0.85em;">SLAM back-end [2편] — Ax=b 대신 Ry=d 를 풀자</a>
      <!--  -->
    </li>
    
    <li>
      <!--  -->
      <!-- <span>04 Mar 2021 &raquo;</span>  -->
      <span style="color:gray; font-size:0.85em;"> [Factor graph SLAM] &raquo;</span>
      <a href="/blog/2021/03/04/slambackend-1.html" style="font-size:0.85em;">SLAM back-end [1편] — SLAM은 Ax=b 를 푸는 것이다</a>
      <!--  -->
    </li>
    
    <li>
      <!--  -->
      <!-- <span>02 Mar 2021 &raquo;</span>  -->
      <span style="color:gray; font-size:0.85em;"> [SLAM 잡담] &raquo;</span>
      <a href="/blog/2021/03/02/slam-root.html" style="font-size:0.85em;">SLAM의 뿌리를 찾아서</a>
      <!--  -->
    </li>
    
    <li>
      <!--  -->
      <!-- <span>02 Mar 2021 &raquo;</span>  -->
      <span style="color:gray; font-size:0.85em;"> [SLAM 잡담] &raquo;</span>
      <a href="/blog/2021/03/02/segmap-build.html" style="font-size:0.85em;">SegMap 빌드하기</a>
      <!--  -->
    </li>
    
    <li>
      <!--  -->
      <!-- <span>01 Mar 2021 &raquo;</span>  -->
      <span style="color:gray; font-size:0.85em;"> [Life] &raquo;</span>
      <a href="/blog/2021/03/01/blog-start.html" style="font-size:0.85em;">블로그 시작!</a>
      <!--  -->
    </li>
    
    <li>
      <!--  -->
    </li>
    
  </ul>
</div>


      <div class="footer">
        <div class="disclaimer" style="margin-top:+5px">

  <a title="home" style="float:right" href="/">
    Home <i class="fa fa-home"></i> 
  </a>

  <a title="top" style="float:left" href="#top">
    Top <i class="fa fa-arrow-up"></i> 
  </a>

  <br>

  <!-- 
  <p>
    The postings on this site are my own and don't necessarily represent my 
    employer’s positions, strategies or opinions.
  </p>
   -->
  
  <!-- ©  -->
  <p>
    <a href="http://bit.ly/gk_profile"> Giseop Kim </a>, 2021 &mdash; built with <a href="http://jekyllrb.com/">Jekyll</a> using <a href="https://github.com/swanson/lagom">Lagom theme</a>
  </p>
</div>
      </div>

    </div>
  </div>

<!-- 
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-xxxx-x']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
 -->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4VNTPFW7FW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-4VNTPFW7FW');
</script>

</body>
</html>

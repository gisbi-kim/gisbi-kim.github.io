<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>The blog of Giseop Kim</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000"/>
 <updated>2022-02-06T15:40:26+09:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Giseop Kim</name>
   <email>paulgkim@kaist.ac.kr</email>
 </author>

 
 <entry>
   <title>SLAM Back-end 공부자료 5개 추천</title>
   <link href="http://localhost:4000/blog/2021/10/03/slam-textbooks.html"/>
   <updated>2021-10-03T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/10/03/slam-textbooks</id>
   <content type="html">&lt;h2 id=&quot;slam-문제란&quot;&gt;SLAM 문제란?&lt;/h2&gt;
&lt;p&gt;SLAM은 robot(의 시간에 따른 궤적)과 주변환경(environment)의&lt;br /&gt;
물리적(및 의미론적) 특성값(state)들을 예측하는 작업이라 요약할 수 있다.&lt;/p&gt;

&lt;p&gt;따라서 수학적으로는 하나의 vector 에 모든 state element 들을 담은 다음에&lt;br /&gt;
최적화 (cost 최소화)과정을 통해 최적의 state 값을 예측하게 된다.&lt;/p&gt;

&lt;p&gt;2000년대 초반까지 널리 쓰이던 (물론 지금도 널리 쓰이는) filtering 기반 방법과 비교&lt;sup id=&quot;fnref:ps1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ps1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;하여&lt;br /&gt;
이렇게 minimization 을 통해 robot 의 궤적과 주변환경의 특성 (예: 위치, 방향)&lt;br /&gt;
들을 예측하게 되는 계열을 편의상 modern SLAM이라고 부르고 싶다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;소개하는-자료의-대상-독자&quot;&gt;소개하는 자료의 대상 독자&lt;/h2&gt;
&lt;p&gt;그렇다면 이러한 optimization 기반 SLAM을 이해하기 위해서는&lt;br /&gt;
어떤 자료로 공부를 하면 좋을까?&lt;/p&gt;

&lt;p&gt;이 글에서는 다음과 같은 분들을 위한 자료들을 소개하고자 한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SLAM open source들 (ORB-SLAM, LeGO-LOAM 등)을 몇 개 실행해보고,&lt;br /&gt;
SLAM이 무엇인지에 대해서도 대강 느낀 바가 있지만,&lt;br /&gt;
논문을 읽어보면 어려운 수식들에 막히는 상태&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;경험상 논문은 지면의 제약이 크기 때문에, (ICRA, IROS 이상 급에서는)&lt;br /&gt;
수학적 background에 대해 독자들이 대체로 이해하고 있다고 내용을 전개하는 편이 많아서&lt;br /&gt;
논문을 통해서 SLAM의 기본 단에 대해 이해하기는 어려웠었다.&lt;/p&gt;

&lt;p&gt;흔히 하는 실수로써&lt;br /&gt;
‘SLAM 수식에서는 linera algebra 나 optimization 지식이 뭔가 많이 요구되는 것 같아 보여서&lt;br /&gt;
관련 수학분야 교과서 (수백쪽) 를 함께 공부해야겠다’는 접근으로 공부를 하게되면&lt;br /&gt;
실질적으로 그 책들의 모든 내용들이 SLAM에 필요한 것이 아니기 때문에 쉽게 지치기 쉽다.&lt;/p&gt;

&lt;p&gt;또한 우리는 보통 SLAM의 theory 를 세계최초로 개발하고 싶다기 보다는,&lt;br /&gt;
현재 SLAM의 역사와 최신 SLAM의 이론적 배경에 대해 ‘빠르게’ 이해하여&lt;br /&gt;
코드를 읽을 때 각 부분이 어떤 부분에 해당하는지 빠르게 grasp 하기 위함이기 때문에&lt;br /&gt;
실질적으로 SLAM에 필요한 기본기만을 빠르게 다질 수 있다면 좋을 것이다.&lt;/p&gt;

&lt;p&gt;이런 상황에서 논문을 읽기 전에 수학적 기본기를 다지기 위해&lt;br /&gt;
약간은 교과서에 가까운 (70~100쪽 규모), 도움이 되었던 자료들을 소개하고자 한다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;modern-slam-문제를-위해-이해해야-하는-요소&quot;&gt;(Modern) SLAM 문제를 위해 이해해야 하는 요소&lt;/h2&gt;

&lt;p&gt;먼저 아래 자료들을 소개하기 전에, 왜 아래 자료들을 선정하였는지에 대해 먼저 말하고자 한다.&lt;/p&gt;

&lt;p&gt;SLAM 을 이해하기 위해서는 아래 내용에 대해 이해하는 것이 요구된다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Rotation Parametrization
    &lt;ul&gt;
      &lt;li&gt;다른 학문들의 최적화문제와 달리 SLAM을 SLAM답게 만드는 것은 우리가 알고자 하는 state 가 사는 공간의 특수성에 있다. 제일 minimal 하게는 우리가 알고싶은 state는 보통 position (ex: x, y, z) 과 orientation (혹은 rotation) 이다.&lt;/li&gt;
      &lt;li&gt;이 때, vector 공간에 존재하는 요소에 대해 최적화하는 것은 항상 쉽다. position 은 vector space에 존재한다. 하지만 rotation 은 vector space 에 존재하지 않기 때문에 항상 문제가 된다.&lt;/li&gt;
      &lt;li&gt;그렇다면 이 rotation 에 대해 최적화를 하기 위해서 우리는 어떻게 해야할까? 에 대해 알아야 한다.&lt;/li&gt;
      &lt;li&gt;이를 위해 rotation 을 우리가 최적화 하기 적합한 형태로 표현할 수 있는데 이 과정을 parametrization (매개변수화) 이라고 한다. 그리고 그 종류에는 크게 3가지 표현법이 존재한다: angle-axis (rotation vector, rotvec 등으로도 불림), quaternion, SO(3) 이다. (Euler angles 라는 parametrization으로 값 3개를 통해 SO(3) 를 표현할 수도 있다)&lt;/li&gt;
      &lt;li&gt;각 parametrization의 장단점은 무엇일까? 우리는 (최적화를 위해) 어떤 parametrization 을 채택해야 할까? 이것에 대해 잘 설명하는 자료를 소개하고자 한다.&lt;/li&gt;
      &lt;li&gt;$\rightarrow$ 해당 자료 (* 위주로 먼저 보는 것을 추천):&lt;br /&gt;
 $  \ \ \ \ $ &lt;a href=&quot;#m1&quot;&gt;*1. Quaternion kinematics for the error-state Kalman filter &lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Least-Square Optimization
    &lt;ul&gt;
      &lt;li&gt;당연히 최적화를 위해서는 least square optimization 에 대해 알아야 한다.&lt;/li&gt;
      &lt;li&gt;하지만 기존 대학수업에서는 convex optimization 수업 등이 많은데, 이런 책을 공부한다고 해서 바로 SLAM의 최적화에 대해 이해하기는 쉽지 않을 수도 있는 것이 (도움은 물론 되겠지만), SLAM 에서 예측하고자 하는 것들에는 앞서 이야기했든 rotation 등 nonlinear 한 애들이 많이 포함되어 있으므로 전역적으로 (global) 최적인 해(solution)를 구하기는 쉽지 않다.&lt;/li&gt;
      &lt;li&gt;따라서 SLAM에서 optimization 이라고 표현하면 거의 항상 iterative update 를 의미한다. 즉, 현재 initial guess 가 있고, 그 state vector 의 최적 delta (변화량) 값을 예측하는 문제로 formulation 하는 것이다.&lt;/li&gt;
      &lt;li&gt;이 과정에서 measurement function 에 대한 Jacobian, Gauss-Newton, LM-method 등의 용어가 나온다. 이에 대한 이론적 전개와, 피부로 느낄 수 있는 자료를 소개하고자 한다.&lt;/li&gt;
      &lt;li&gt;$\rightarrow$ 해당 자료 (* 위주로 먼저 보는 것을 추천):&lt;br /&gt;
 $  \ \ \ \ $  &lt;a href=&quot;#m21&quot;&gt;*2-1). From Least-Squares to ICP (ICRA 2016 SLAM tutorial)&lt;/a&gt; &lt;br /&gt;
 $  \ \ \ \ $  &lt;a href=&quot;#m3&quot;&gt;*3. Course on SLAM&lt;/a&gt;&lt;br /&gt;
 $  \ \ \ \ $  &lt;a href=&quot;#m4&quot;&gt;4. Factor Graphs for Robot Perception&lt;/a&gt;&lt;br /&gt;
 $  \ \ \ \ $  &lt;a href=&quot;#m5&quot;&gt;5. Bundle Adjustment – A Modern Synthesis&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Exploiting Sparsity
    &lt;ul&gt;
      &lt;li&gt;마지막으로 SLAM 을 가장 SLAM 답게 만들어 주는 개념은 sparsity 이다.&lt;/li&gt;
      &lt;li&gt;보통 least square optimization 이라고 하면 $Ax=b$ 를 푸는 것이고 (엄밀하게는 iterative update 를 하므로 $A \Delta x=b$ 라고 적어야 맞겠다), 이를 보통 $A$의 pseudo-inverse 를 이용해서 풀게 된다. 즉  $(A^{T} A) \Delta x = A^T b$ 를 풀게 되는데, 이 식을 normal equation 이라고 부른다. 간단하게는 최적해를 $\Delta x^{*} = (A^T A)^{-1}A^T b$ 라고 closed form으로 계산할 수 있겠다.&lt;/li&gt;
      &lt;li&gt;하지만 SLAM에서는 최적화 하고 싶은 landmark 의 수가 수천, 수십, 수백만개가 될 수도 있고, 각 포인트 하나당 최소 xyz 3개의 값을 요구한다 (rotation 이 포함될 경우 그 이상이 될 수도 있다). 그러면 우리가 풀고자 하는 시스템의 규모 (즉, $A^{T} A$ 행렬의 shape) 는 수백만 x 수백만 의 크기가 될 것이다.&lt;/li&gt;
      &lt;li&gt;그러면 이를 일단 메모리에 올리는 것이 어려울 수 있다. 그리고 이것의 inverse 를 어떻게 효율적으로 계산할 수 있을까?&lt;/li&gt;
      &lt;li&gt;그런데 &lt;a href=&quot;/blog/2021/03/04/slambackend-1.html#Axb&quot;&gt; SLAM back-end [1편] &lt;/a&gt; 에서 그림으로 소개했듯이, SLAM의 state element 들 사이에는 대체로 sparse 관계만이 존재하기 때문에 (locality 라고 부른다) 결과적으로 $A$ matrix 가 sparse 함을 알 수 있다. 이를 이용하면 효율적인 iterative update 알고리즘을 고안할 수 있지 않을까? 에 대해 고민한(+정립된) 자료들을 소개하고자 한다.&lt;/li&gt;
      &lt;li&gt;$\rightarrow$ 해당 자료 (* 위주로 먼저 보는 것을 추천):&lt;br /&gt;
 $  \ \ \ \ $ &lt;a href=&quot;#m3&quot;&gt;3. Course on SLAM&lt;/a&gt;&lt;br /&gt;
 $  \ \ \ \ $ &lt;a href=&quot;#m4&quot;&gt;*4. Factor Graphs for Robot Perception&lt;/a&gt;&lt;br /&gt;
 $  \ \ \ \ $ &lt;a href=&quot;#m22&quot;&gt;*2-2). Graph-Based SLAM and Sparsity (ICRA 2016 SLAM tutorial)&lt;/a&gt;&lt;br /&gt;
 $  \ \ \ \ $ &lt;a href=&quot;#m5&quot;&gt;5. Bundle Adjustment – A Modern Synthesis&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;개인적으로는 위에서 소개한 순서인 1, 2-1, 3, 4, 2-2, 5 의 순서로 이해하는 편이 좋았다.
    &lt;ul&gt;
      &lt;li&gt;하지만 어떤 것을 먼저 읽든 2회독 정도 하면서 모든 내용들이 한데 얽히는 지점을 느끼는 것이 중요하다고 생각된다.
        &lt;ul&gt;
          &lt;li&gt;실제로 modern SLAM 논문을 읽다보면 이들 내용이 모두 요구될 때가 많다. (ex: 2016 TRO, On-Manifold Preintegration for Real-Time Visual-Inertial Odometry)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;slam-기본기를-위한-자료들&quot;&gt;SLAM 기본기를 위한 자료들&lt;/h2&gt;

&lt;!-- --- --&gt;
&lt;p id=&quot;m1&quot;&gt; &lt;/p&gt;
&lt;h3 id=&quot;1-quaternion-kinematics-for-the-error-state-kalman-filter&quot;&gt;1. Quaternion kinematics for the error-state Kalman filter&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/materials/study/soal17eskf.pdf&quot; target=&quot;_blank&quot;&gt; Download&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Author: Joan Solà (2017)&lt;/li&gt;
  &lt;li&gt;요약:
    &lt;ul&gt;
      &lt;li&gt;error-state Kalman filter 를 위한 minimal 한 rotation 표현법인 angle-axis (rotation vector 라고도 부름) 방식의 parametrization 을 소개하고, 이것과 실제 manifold 상에서의 SO(3) 표현법과의 관계 등에 대해 잘 다룬다.&lt;/li&gt;
      &lt;li&gt;rotation parametrization 공부를 위해서는 챕터4까지만 읽어도 무방할 것으로 생각된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;선정이유:
    &lt;ul&gt;
      &lt;li&gt;지금까지 본 rotation parametrization 논문 중 가장 친절함.
        &lt;ul&gt;
          &lt;li&gt;인터넷에 존재하는 다른 자료들을 더 안보고 이 것만 2-3번 반복해서 읽는 것이 더 나을 정도.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;실제 SLAM에서는 rotation 의 미소변화에 대한 미분값인 Jacobian 이 필요한데, Jacobian 에 대해서도 빠짐없이 다루어 줌.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- --- --&gt;
&lt;h3 id=&quot;2-icra-2016-slam-tutorial&quot;&gt;2. ICRA 2016 SLAM tutorial&lt;/h3&gt;
&lt;p id=&quot;m21&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1) From Least-Squares to ICP
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;/materials/study/icra16tuto/icra16_slam_tutorial_grisetti.pdf&quot; target=&quot;_blank&quot;&gt; Download&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Author: Giorgio Grisetti (2016)&lt;/li&gt;
      &lt;li&gt;요약 및 선정이유:
        &lt;ul&gt;
          &lt;li&gt;LS optimization 이 쓰이는 SLAM application 중 가장 간단한 ICP (3 dimension state) 에 대해 Gause-netwon 및 LM update 에 대한 실제 octave (matlab) code 를 보여줌으로써 실제로 이론이 코드로 어떻게 구현되는지 감을 잡기 좋다.
            &lt;ul&gt;
              &lt;li&gt;이 파이프라인 중, iterative optimization 은 J (measurement error Jacobian) 와 e (residual vector) 를 구하는 것이 핵심임을 체감할 수 있음.&lt;/li&gt;
              &lt;li&gt;또한, 100쪽씩 되는 분량의 글이 아닌, 수십쪽의 슬라이드 이기 때문에 빠르게 실습해보고 감을 잡을 수 있어 좋다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;ps:
        &lt;ul&gt;
          &lt;li&gt;이 실습 이후 Grisetti 님의 최신 논문 &lt;a href=&quot;https://arxiv.org/abs/2002.11051&quot; target=&quot;_blank&quot;&gt; Least Squares Optimization: from Theory to Practice&lt;/a&gt; 을 같이 읽으면 좋다. ICP외에도 projective error, BA 등 SLAM에서 주로 쓰이는 application 들에 대해서도 J, e 가 어떻게 생겼는지 소개해줌.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;p id=&quot;m22&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;2) Graph-Based SLAM and Sparsity
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;/materials/study/icra16tuto/icra16_slam_tutorial_stachniss.pdf&quot; target=&quot;_blank&quot;&gt; Download&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Author: Cyrill Stachniss (2016)&lt;/li&gt;
      &lt;li&gt;요약 및 선정이유:
        &lt;ul&gt;
          &lt;li&gt;앞서 2-1) Grisetti 교수님의 자료에서는 small example (ICP) 만이 다루어졌는데, 만약에 state vector 가 매우크다면 Jacobian matrix 를 어떻게 운용해야 하는가? 에 대해 생각해볼 수 있음. 많은 그림을 함께 담고 있어 이해하기 좋음.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- --- --&gt;
&lt;p id=&quot;m3&quot;&gt; &lt;/p&gt;
&lt;h3 id=&quot;3-course-on-slam&quot;&gt;3. Course on SLAM&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/materials/study/soal17courseslam.pdf&quot; target=&quot;_blank&quot;&gt; Download&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Author: Joan Solà (2017)&lt;/li&gt;
  &lt;li&gt;요약 및 선정이유:
    &lt;ul&gt;
      &lt;li&gt;사실 SLAM에 대해 논하기 위해서는 가장 먼저는 coordinate transform&lt;sup id=&quot;fnref:ps2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ps2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;에 대해 이야기해야 한다. 그리고 motion model, observation (measurement) model, 이런 용어들&lt;sup id=&quot;fnref:ps3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ps3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;에 대해서도 먼저 소개해야 할 것이다. 공대 기계과 학부 2-3학년 수업이라면 아마 이런것들을 기본적으로 다 다룰 것이라 생각한다. 그래서인지 SLAM tutorial 들을 보면 이런 기초에 대해서는 다루고 있지 않지만 Sola 님의 이 자료에서는 그런것들도 빠르게 다루고 넘어가주는 것 (챕터 2-3)이 친절한 부분이다.&lt;/li&gt;
      &lt;li&gt;챕터 4에서는 Grisetti 님이 소개한 iterative optimization 에 대해 친절하게 다루어준다.
        &lt;ul&gt;
          &lt;li&gt;단, 위의 Sola 님의 자료 Quaternion kinematics … 를 읽고 이 Course on SLAM 를 읽는 것을 추천함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;심지어 챕터 5에서는 이 다음에 소개할 4. Factor Graphs for Robot Perception 의 내용도 짧게 요약하고 있다.
        &lt;ul&gt;
          &lt;li&gt;Factor Graphs for Robot Perception 를 읽기 전에 빠르게 보고가는 것도 좋을 것 같고, 아니면 4. Factor Graphs for Robot Perception (도 되게 친절하고 천천히 설명을 이어나가고 있으므로) 을 먼저 읽고 recap 차원에서 3. Course on SLAM 의 챕터 5로 돌아오는 것도 좋을 듯하다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Modern Graph-based SLAM에 대해 설명하고자 하면, 이 자료 하나만 보면 될 정도로 core 들이 compact 하지만 detail하게 빠짐없이 잘 수록되어 있다고 할 수 있을 듯하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- --- --&gt;
&lt;p id=&quot;m4&quot;&gt; &lt;/p&gt;
&lt;h3 id=&quot;4-factor-graphs-for-robot-perception&quot;&gt;4. Factor Graphs for Robot Perception&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/materials/study/Dellaert17.pdf&quot; target=&quot;_blank&quot;&gt; Download&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Author: Frank Dellaert and Michael Kaess (2017)&lt;/li&gt;
  &lt;li&gt;요약 및 선정이유:
    &lt;ul&gt;
      &lt;li&gt;최근 SLAM 시스템을 직접 구현하기 위해서 대부분의 프로젝트들은 solver 로 Ceres 혹은 GTSAM을 채택하고 있다. 즉, 지금까지 이론적으로 iterative optimization 을 공부했다면, 그것을 우리가 실제로 구현하는 것은 아니고 그 solver 는 Ceres 혹은 GTSAM을 사용하는 것이 일반적이다. Frank Dellaert and Michael Kaess 는 GTSAM을 구현한 저자들로써 저자들이 어떤 철학을 가지고 efficient solver 들을 발전시켜왔는지 역사까지 함께 엿볼 수 있는 자료이다.&lt;/li&gt;
      &lt;li&gt;Square root SAM (2006), iSAM (2008), iSAM2 (2012) 로 이어지는 sparse least-square optimization 을 푸는 방법 발전 순서를 볼 수 있으며 이에 대해서는 다른 자료에서는 거의 언급되지 않았으므로 이 자료의 가치가 있다.&lt;/li&gt;
      &lt;li&gt;특히 근본이 되는 Square root SAM 논문 내용에 대해 아주 길고 친절하게 설명이 되어 있으며 (~챕터4, ~67쪽), QR decomposition, fill-in, variable ordering 등 solver 의 computational 효율을 증진하기 위한 주요개념들이 많이 나오므로 꼭 봐두는 것이 좋다. 이 개념은 아래 5번 자료에서 소개할 Bundle adjustment 등에서도 중요한 일반적인 sparse linear algebra 개념이기 때문에 알아두면 좋다.&lt;/li&gt;
      &lt;li&gt;또한 이 교과서에서는 이론 외에도 왜 square root matrix 로 풀어야만 하는가? 그것의 의미는 무엇인가? 이런거에 대해서도 (저자직강으로) 인사이트를 제공해주고 있는 점이 좋다 (&lt;a href=&quot;/materials/study/icra16tuto/icra16_slam_tutorial_kaess.pdf&quot; target=&quot;_blank&quot;&gt;12 page 참고 - Efficient Incremental Smoothing, 2016 ICRA tutorial, by Michael Kaess&lt;/a&gt;). 즉, Measurement Jacobian matrix 는 사실 factor graph 와 동치이고, information matrix 는 Markov Random Field 이며, Square Root Inf. Matrix는 Bayes Tree 이다, 라는 것.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- --- --&gt;
&lt;p id=&quot;m5&quot;&gt; &lt;/p&gt;
&lt;h3 id=&quot;5-bundle-adjustment--a-modern-synthesis&quot;&gt;5. Bundle Adjustment – A Modern Synthesis&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/materials/study/Triggs99.pdf&quot; target=&quot;_blank&quot;&gt; Download&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Author: Bill Triggs et al. (1999)&lt;/li&gt;
  &lt;li&gt;요약 및 선정이유:
    &lt;ul&gt;
      &lt;li&gt;그러면 지금까지 소개한 내용들이 SLAM으로 인해 2000년대 말 이후에 접어들며 발전된 것이냐? 하면 그게 아니라 이론적인 토대는 이미 다 있었다고 생각이 된다. 특히 photogrametry 나 computer vision 사람들이 풀던 SfM (structure from motion) 이라고도 불리는 bundle adjustment 라는 문제가 있다. SLAM은 종종 Sequential SfM 이라 불리기도 할 정도로 optimization 관점에서 근본을 많이 공유하고 있다.&lt;/li&gt;
      &lt;li&gt;하지만 앞에서 소개한 자료들이 좀 더 이론적인 배경을 쌓기에는 좋다고 생각되어 먼저 소개하게 되었다. 이런 이론적 토대를 가지고 bundle adjustment 라는 문제를 풀 때 구체적으로 어떤 요소들을 더 고민해야 하는지에 대해 깊게 들어가기 위해 이 자료를 추천한다&lt;sup id=&quot;fnref:ps4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ps4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;slam-기본기를-위한-자료들-1&quot;&gt;SLAM 기본기를 위한 자료들++&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;지금까지 optimization 기반의 SLAM 을 위한 이론적 토대를 다지기 위한 자료들을 소개했다.
    &lt;ul&gt;
      &lt;li&gt;하지만 visual-inertial (2007 ICRA, MSCKF), lidar-inertial (2021 RA-L, FAST-LIO) 등 특정 센서 조합에 대해서는 filtering 기반 방법도 여전히 정확하고 가볍게 쓸 수 있어 좋은 듯하다. 
이 방법들은 error state 의 최적값을 예측하여, 현재 state 에 더해주는 식으로 현재 state 를 추정 (filtering) 하게 된다. Sola님의 1. Quaternion kinematics for the error-state Kalman filter 자료 뒷부분에 대해서 이 개념에 대해 다루기는 했지만 약간은 설명이 부족했다고 느껴진다.&lt;/li&gt;
      &lt;li&gt;그래서 Aided Navigation: GPS with High Rate Sensors 라는 책이 상당히 구체적으로 다루고 있는 듯한데 (대충만 훑어봄), 이 책에 대해 공부해보는 것도 좋을 것 같다.
        &lt;ul&gt;
          &lt;li&gt;FAST-LIO 그룹에서 공개한 &lt;a href=&quot;/materials/study/ikfom21.pdf&quot; target=&quot;_blank&quot;&gt;IKFoM (Kalman Filters on Differentiable Manifolds)&lt;/a&gt; 논문에도 Related work 이 잘 되어 있어서 함께 보고자 한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;앞서 소개한 5개의 자료들은 SLAM back-end 의 수학적 기초에 대해 다루었다고 할 수 있다.
    &lt;ul&gt;
      &lt;li&gt;하지만 SLAM은 특정 센서가 센서데이터를 처리하고 요약하여, 해당 센서에 맞는 measurement model 을 정의하는 것부터 시작된다. 이 부분을 front-end 라고 하고 SLAM 의 전 과정을 이해하기 위해서는 빼놓을 수 없다.&lt;/li&gt;
      &lt;li&gt;특히, camera 가 SLAM을 위한 가장 널리 쓰이는 센서이기 때문에 visual SLAM을 위한 front-end 에 대해 실습을 통해 공부하는 것은 중요하다.&lt;/li&gt;
      &lt;li&gt;이에 대해서 최근(2021년 6월)에  &lt;a href=&quot;https://github.com/gaoxiang12/slambook-en/ &quot; target=&quot;_blank&quot;&gt;slambook 이라는 중국어 교재의 영어번역&lt;/a&gt; &lt;a href=&quot;/materials/study/slambookEng21.pdf&quot; target=&quot;_blank&quot;&gt; (Download) &lt;/a&gt;이 끝났다고 한다 &lt;a href=&quot;https://www.springer.com/gp/book/9789811649387&quot; target=&quot;_blank&quot;&gt; (Springer 를 통해 책이 출간되기도 했다!) &lt;/a&gt;. Eigen, opencv 등 실제 C++ 코드를 함께 소개하고 있어 practically 좋은 듯하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;주석&quot;&gt;주석&lt;/h3&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:ps1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;물론 Kamlan filter 의 경우, 예측값과 실측값 차이의 variance 를 최소화 하는 방향으로 업데이트 하기 때문에 (Kalman filter original 논문의 유도 참고) 그 유도 과정에서 least square optimization 이 쓰인 것이라고 이야기할수도 있다. 따라서 filtering 이냐 혹은 modern SLAM의 smoothing (전체 궤적의 state들을 혹은 많은 수의 landmark들을 함께 업데이트) 이냐 를 구분 짓는 차이는 가장 최근의 state 만을 추적할 것인지 past state history 들도 함께 최적화 할 것인지 차이로 이해할 수 있다. 물론 이 둘이 함께 쓰일 수도 있다. https://github.com/gisbi-kim/FAST_LIO_SLAM 참고 (가장 최근의 pose 를 추정하기 위해서는 filtering 을 활용하여 initial pose-graph 를 construct하고, 전반적인 trajectory smoothing 을 위해서는 pose-graph optimization을 활용함). &lt;a href=&quot;#fnref:ps1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ps2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.mech.sharif.ir/c/document_library/get_file?uuid=5a4bb247-1430-4e46-942c-d692dead831f&amp;amp;groupId=14040&quot; target=&quot;_blank&quot;&gt;Introduction to Robotics (J. Craig, 2005)&lt;/a&gt; 를 추천 (챕터2만 봐도 된다). &lt;a href=&quot;https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=266139534&quot; target=&quot;_blank&quot;&gt;로보틱스 입문&lt;/a&gt; 이라는 이름으로 한국어 번역도 되어 있어 좋다 &lt;a href=&quot;#fnref:ps2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ps3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;그 유명한 probabilistic robotics 책을 추천. &lt;a href=&quot;#fnref:ps3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ps4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;이론적 토대는 이미 1999년에 논문이 나올 정도로 구축되어 있었지만, 최근에 Apple 이 &lt;a href=&quot;https://developer.apple.com/kr/augmented-reality/object-capture/ &quot; target=&quot;_blank&quot;&gt;Object Capture&lt;/a&gt; 라는 API를 공개한 것이 흥미롭다. &lt;a href=&quot;https://youtu.be/IXMCAvocxXc &quot; target=&quot;_blank&quot;&gt;Polycam&lt;/a&gt; 등 상용앱에 bundle adjustment 기능이 들어오기 시작하는 등, 이제는 누구나 높은 accuracy 의 structure from motion을 직접할 수 있는 시대가 되고 있는 듯하다. &lt;a href=&quot;#fnref:ps4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>ICRA 2021 Radar in Robotics Workshop 요약</title>
   <link href="http://localhost:4000/blog/2021/05/31/icra21-radar-ws.html"/>
   <updated>2021-05-31T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/05/31/icra21-radar-ws</id>
   <content type="html">&lt;h1 id=&quot;icra-2021-radar-in-robotics-workshop&quot;&gt;ICRA 2021 Radar in Robotics Workshop&lt;/h1&gt;

&lt;p&gt;ICRA 21 에서 Radar ws &lt;a href=&quot;https://sites.google.com/view/radar-robotics/home&quot;&gt;(웹사이트)&lt;/a&gt; 가 5월 31일에 열렸다.&lt;br /&gt;
영국 헤리엇 왓 대학교의 Sen Wang 님과 김아영 교수님이 공동 조직하였다.&lt;br /&gt;
라이브로 진행되어서 좋았다!&lt;/p&gt;

&lt;figure id=&quot;open&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-31-icra21-radar-ws/open.png&quot; style=&quot;width:80%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; 시작! &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;구성은&lt;br /&gt;
총 4명의 연사들의 키노트,&lt;br /&gt;
3개의 데이터셋 소개 발표,&lt;br /&gt;
여러개의 워크샵 논문 5분발표,&lt;br /&gt;
그리고 제일 마지막의 패널 디스커션 시간으로 구성되었다.&lt;/p&gt;

&lt;p&gt;피크 때 80명 정도까지 들어온 것 같은데&lt;br /&gt;
평소 학회 워크샵에 한방이 가득차면 100명 정도인 것을 생각하면&lt;br /&gt;
원격 라이브임에도 사람들이 많이 찾아와준 것 같다.&lt;/p&gt;

&lt;p&gt;워크샵 논문들과 비디오는 나중에 다 올라온다고 하는데&lt;br /&gt;
일단 먼저 정리해본다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;keynote-1-from-benz&quot;&gt;Keynote 1 from Benz&lt;/h2&gt;

&lt;p&gt;첫번째 연사는 벤츠의 Dr. Jürgen Dickmann 였다.&lt;/p&gt;

&lt;p&gt;벤츠가 레이더를 많이 사용하고 있다는 사실을 알 수 있었고&lt;br /&gt;
또 Next Radar Challenge 에 대해 많이 이야기해주셨다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;즉, next generation 의 radar 는 어때야 하는가?
    &lt;ul&gt;
      &lt;li&gt;High resolution, Imaging radar 이런 단어들이 보였다.&lt;/li&gt;
      &lt;li&gt;high resolution radar 를 달성하기 위해서 serial radars (have roughly 200 channels) 라는 말이 나와서 뭔말인지 몰라서 연구실형에게 물어보니 대략 이런 이야기라고 한다.
        &lt;ul&gt;
          &lt;li&gt;milliEgo&lt;sup id=&quot;fnref:ps1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ps1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 라는 논문에서 예시 사진을 하나 들고와보았다. 아래와 같이 연구자들은 TI 사의 mmwave radar (여기서는 TI AWR1843) 를 많이 쓰고 있다. 이 레이더 센서에 보면 위에 배선들이 있는데 이게 안테나라고 한다. 이게 몇개 없어서 현재의 mmwave radar 의 &lt;a href=&quot;#sparse&quot;&gt; point cloud 가 sparse 하다는 것&lt;/a&gt;. 이걸 일렬로 쭉 이을 수 있으면 더 덴스한 데이터를 얻을 수 있을 것이라고 한다.&lt;/li&gt;
          &lt;li&gt;이 이상의 자세한 설명은 잘 몰라서 생략… &lt;a href=&quot;https://www.ti.com/sensors/mmwave-radar/overview.html&quot;&gt;TI 공홈에 문서들&lt;/a&gt;이 좀 있어서 이걸 읽어봐야겠다…
            &lt;figure id=&quot;tiradar&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-31-icra21-radar-ws/tiradar-full.png&quot; style=&quot;width:100%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; Example: TI Radar &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
            &lt;!-- &lt;br&gt; --&gt;
            &lt;figure id=&quot;sparse&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-31-icra21-radar-ws/tiradar-sparse.png&quot; style=&quot;width:90%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; Example: TI Radar's sparse point cloud &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
            &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;위의 그림처럼 Radar point cloud 는 lidar 에 비해 sparse 하지만 doppler information 을 준다는 것과 foggy 환경에서도 동작할 수 있다는 것이 장점이다.
    &lt;ul&gt;
      &lt;li&gt;현재 연구자들이 쓰는 radar 는 두 종류로 나뉘는 듯하다.
        &lt;ul&gt;
          &lt;li&gt;1) TI radar (sparse but 3D point and doppler information is avaiable)&lt;/li&gt;
          &lt;li&gt;2) Navtech radar (dense and long-range but 2D)
            &lt;ul&gt;
              &lt;li&gt;에 대해서는 뒤에서 더 소개한다.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;그리고 &lt;a href=&quot;#Scenes&quot;&gt; Radar Scenes Dataset &lt;/a&gt; 홍보를 하심.
    &lt;ul&gt;
      &lt;li&gt;흔히 automotive 용 mmwave radar 가 sparse point cloud (and noisy) 를 제공한다고 알고있었는데, 아래 그림을 보면 거의 LiDAR 급임. 회사에서는 연구레벨보다 한참 좋은 것들이 이미 다 개발되어 있겠구나를 느꼈고, 그럼에도 데이터라도 이렇게 공개해줘서 되게 기뻤다.&lt;/li&gt;
      &lt;li&gt;게다가 radar point cloud 라벨링도 되어 있다고 함. 써봐야 겠다.&lt;/li&gt;
    &lt;/ul&gt;
    &lt;figure id=&quot;Scenes&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-31-icra21-radar-ws/scenes.png&quot; style=&quot;width:100%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; Radar Scenes Dataset &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;keynote-2-from-eth-zurich&quot;&gt;Keynote 2 from ETH Zurich&lt;/h2&gt;

&lt;p&gt;두번째로는 ETH Zurich의 Dr. Dengxin Dai 님이 발표를 해주셨다.&lt;/p&gt;

&lt;p&gt;Depth completion by using radar data 이라던가&lt;br /&gt;
radar 로부터 sound estimation (joint learning?) 등의 computer vision 토픽다운 연구들을 소개해주셨다.&lt;/p&gt;

&lt;p&gt;하지만 나는 state estimation 에 주로 관심이 있어서&lt;br /&gt;
여기에 대한 설명은 생략 …&lt;/p&gt;

&lt;p&gt;그나저나 CVPR 21에서 Vision for All Seasons WS 가 열린다고 한다. radar sensor 기반 perception의 지향점이 all-weather autonomoy 라는 큰 목표를 위한 것이니 만큼, 관심이 간다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;keynote-3-prof-martin-d-adams&quot;&gt;Keynote 3 Prof. Martin D. Adams&lt;/h2&gt;

&lt;p&gt;마틴 교수님은 Radar SLAM에서 이미 유명한 교과서 (아래 왼쪽 하늘색 책)의 저자이시다.&lt;/p&gt;

&lt;figure id=&quot;books&quot;&gt;
    &lt;img src=&quot;/figs/2021-05-31-icra21-radar-ws/books.png&quot; style=&quot;width:90%&quot; /&gt;
    &lt;figcaption&gt; 
          &lt;center&gt; Radar Textbooks &lt;/center&gt;
    &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;SLAM 사람들이 카메라도 했다가 lidar 도 했다가&lt;br /&gt;
radar 도 이제 좋아보여서 와 하고 몰려오는 것과 달리&lt;/p&gt;

&lt;p&gt;정말 radar for robot 외길인생을 걸으신 듯한 …&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;다양한 프로젝트 사진들&lt;/p&gt;

    &lt;figure id=&quot;martin&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-31-icra21-radar-ws/martin.png&quot; style=&quot;width:90%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; Prof. Martin D. Adams slide &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;암튼 톡의 내용은 다음과 같았다.&lt;/p&gt;

&lt;p&gt;(현재까지의) radar 는 sensor 특성상 false alarm 이 많을 수밖에 없는데,&lt;br /&gt;
이는 landmark 매칭 기반의 SLAM (e.g., FastSLAM) 을 취약하게 한다.&lt;/p&gt;

&lt;p&gt;물론 false alarm을 앞단에서 잘 제거해 줄 수도 있겠지만,&lt;br /&gt;
이런게 있을 때도 어떻게 수학적으로 강건한 machinery 를 제공할 수 있을까?&lt;/p&gt;

&lt;p&gt;이를 위해 마틴 교수님은 이게 state 가 Vector 기반으로 표현하지 않고,&lt;br /&gt;
(random finite) set 으로 표현해서 해결했다고 한다 (위 그림의 두번째 책).
RB-PHD SLAM&lt;/p&gt;

&lt;p&gt;정말 멋있다는 생각을 하며 …&lt;br /&gt;
나중에 시간이 되면 읽어봐야겠다는 생각을 하고 일단 넘어감.&lt;/p&gt;

&lt;p&gt;talk 의 내용과 별개로 워크샵 전반의 분위기를 잘 잡아주신 것 같다.&lt;br /&gt;
인자한 대가 느낌. 덕분에 워크샵이 매우 훈훈한 분위기였다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;keynote-4-prof-tim-barfoot&quot;&gt;Keynote 4 Prof. Tim Barfoot&lt;/h2&gt;

&lt;p&gt;바로 그 책 그 교과서 state estimation for robotics 의 저자이신 바풋 교수님의 톡이었다.&lt;/p&gt;

&lt;p&gt;아무래도 앞의 분들이 HW에 좀 더 가깝거나, 혹은 CV에 좀 더 가까운 이야기를 했다면,&lt;br /&gt;
바풋 교수님은 robotics 관점에서 중요한 포인트들을 잘 짚어주셨다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There are many different types of radar sensors in robotics … 라며 이야기를 시작하셨는데
    &lt;ul&gt;
      &lt;li&gt;아마 현재 연구들이 제목에는 같은 radar 로 찍혀 나오지만&lt;/li&gt;
      &lt;li&gt;사실 내용을 보면 TI radar 를 사용하는 계열과 Navtech radar 를 사용하는 논문들의 연구방법론이 좀 다른 경향을 알 수 있다.&lt;/li&gt;
      &lt;li&gt;Doppler 정보를 이용할 수 있느냐가 가장 큰 차이로 보인다.&lt;/li&gt;
      &lt;li&gt;바풋교수님의 톡은 Navtech radar 에 관한 것이었다.
        &lt;ul&gt;
          &lt;li&gt;크게 세 가지 내용을 소개하심.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Boreas 라는 dataset 을 준비하고 계신듯하다.
    &lt;ul&gt;
      &lt;li&gt;Navtech radar 기반이며 20개가 넘는 시퀀스가 있다고 한다.&lt;/li&gt;
      &lt;li&gt;6 month with seasonal variation, including 2 snow storms&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Motion compensation 에 대해 이야기 하셨다.
    &lt;ul&gt;
      &lt;li&gt;현재 Navtech radar 는 4Hz 로써 sensor 가 mechanically spinning 하는 동안 로봇이 움직이기 때문에 모션보정을 해주어야 한다.&lt;/li&gt;
      &lt;li&gt;그런데 기존의 cen2018&lt;sup id=&quot;fnref:cen18&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:cen18&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, cen2019 논문들은 안하고 걍 하고 있었나봄&lt;/li&gt;
      &lt;li&gt;cen의 방법과 같은 Sparse (or feature-based) radar odometry 계열에서는&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;강건한 feature matching 을 위해 RANSAC이 들어가는데,&lt;/li&gt;
      &lt;li&gt;그냥 RANSAC보다 motion compensated RANSAC 걸 제안하고, 성능이 더 오름을 이야기 하심.
        &lt;ul&gt;
          &lt;li&gt;사실 motion compensated RANSAC 에 대해서는 2013 RSS 에서 이미 lidar version 에 대해 이야기한적이 있다고 한다. 그거의 radar version 인 셈인데, 아무튼 이후의 radar for state estimation 연구들은 이 논문의 motion compenstaion을 적용하지 않을 수 없을 듯. 
    - 이 내용은 &lt;a href=&quot;https://arxiv.org/abs/2011.03512&quot;&gt;2021 ICRA/RAL 논문&lt;/a&gt; 과 &lt;a href=&quot;https://github.com/keenan-burnett/yeti_radar_odometry&quot;&gt;코드&lt;/a&gt; 도 공개가 되어있다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Do We Need to Compensate for Motion Distortion and Doppler Effects in Spinning Radar Navigation?, ICRA/RAL 2021&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unsupervised feature learning
    &lt;ul&gt;
      &lt;li&gt;local feature matching 기반의 odometry (sparse, indirect 등으로도 불리는) 는 센서를 불문하고 결국 feature 의 질이 결과를 좌우한다.
        &lt;ul&gt;
          &lt;li&gt;그래서 ORB-SLAM 이후로 딥 시대가 오면서 superpoint/superglue 가 localization 은 짱을 먹고 … 이런 흐름이 visual sensor 에서 증명이 되었듯이&lt;/li&gt;
          &lt;li&gt;radar odometry 를 위해서도 feature matching 할 때 그 feature 를 learning 할 수 없을까 이런 생각을 할 수 있다.&lt;/li&gt;
          &lt;li&gt;그렇다면 어떻게 학습해야 하는가?&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;HERO (Hybrid Estimate Radar Odometry) 라는 방법을 제안했다고 한다. 이번 RSS 2021이라고 함.
        &lt;ul&gt;
          &lt;li&gt;기존의 conventional 한 factor-graph optimization 과 deep feature learning 을 잘 결합한 것으로 보이는데, 암튼 읽어봐야겠다!&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2105.14152.pdf&quot;&gt;논문&lt;/a&gt;과 &lt;a href=&quot;https://github.com/utiasASRL/hero_radar_odometry&quot;&gt;코드&lt;/a&gt; 도 바로 공개되었다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;figure id=&quot;hero&quot;&gt;
    &lt;img src=&quot;/figs/2021-05-31-icra21-radar-ws/hero.png&quot; style=&quot;width:90%&quot; /&gt;
    &lt;figcaption&gt; 
          &lt;center&gt; Hybrid Estimate Radar Odometry &lt;/center&gt;
    &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;마지막으로 lesson 들을 이야기 해주신 것도 좋았다.
    &lt;figure id=&quot;lessons&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-31-icra21-radar-ws/lessons.png&quot; style=&quot;width:75%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; Lessons &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;spotlight-talks-workshop-papers&quot;&gt;Spotlight Talks: Workshop papers&lt;/h2&gt;

&lt;p&gt;온라인이라서 별로 없을 줄 알았는데&lt;br /&gt;
생각보다 많은 사람들이 자신들의 논문을 홍보하기 위해 워크샵에 5분 논문 소개 시간을 가졌다.&lt;br /&gt;
내가 석사신입이였다면 radar slam 이 지금시작하기에 괜찮은 토픽이라는 생각도 들었다.&lt;/p&gt;

&lt;p&gt;간단하게 키워드만 요약해본다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Oxford
    &lt;ul&gt;
      &lt;li&gt;Radar place recognition 을 위해 Self supervised로 contrastive learning 어케 해야하는가 …&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;KAIST
    &lt;ul&gt;
      &lt;li&gt;Radar enhanced lidar mapping&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;저장대
    &lt;ul&gt;
      &lt;li&gt;RaLL (TITS)
        &lt;ul&gt;
          &lt;li&gt;코드도 있다&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;스웨덴 CFEAR
    &lt;ul&gt;
      &lt;li&gt;레이더 오도메트리&lt;/li&gt;
      &lt;li&gt;top k strongest 만 보고 point-to-line registration 하니까 잘됨.
        &lt;ul&gt;
          &lt;li&gt;scan당 200 points 정도 (cen은 900-1000개 쯤 feature point 가 존재했었음)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;속도도 당연히 빨라짐. 80Hz.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Univ of Colorado (ARPG: Autonomous Robotics and Perception Group)
    &lt;ul&gt;
      &lt;li&gt;AWR radar sensor&lt;/li&gt;
      &lt;li&gt;Radar inertial state estimation&lt;/li&gt;
      &lt;li&gt;(그리고 이거였는지는 모르겠는데) sparse radar 로 무려 3D occupancy mapping 을 수행하는 연구도 인상적이었다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CMU-GPR dataset
    &lt;ul&gt;
      &lt;li&gt;ground-penetration radar 라는 데이터셋을 공개&lt;/li&gt;
      &lt;li&gt;15 traj 포함&lt;/li&gt;
      &lt;li&gt;indoor localization 에 좋았던듯&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RadarScenes dataset
    &lt;ul&gt;
      &lt;li&gt;77GHz radar sensor&lt;/li&gt;
      &lt;li&gt;Res: 0.15m / 0.5-2 deg&lt;/li&gt;
      &lt;li&gt;radar-scenes.com&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이중에서는 특히 CFEAR 라는 방법이 재밌었다.&lt;br /&gt;
얘들 발표의 그림을 몇개 첨부해본다.&lt;/p&gt;

&lt;figure id=&quot;ws1-1&quot;&gt;
    &lt;img src=&quot;/figs/2021-05-31-icra21-radar-ws/ws1-1.png&quot; style=&quot;width:90%&quot; /&gt;
    &lt;figcaption&gt; 
          &lt;center&gt; Radar Odometry comparison &lt;/center&gt;
    &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure id=&quot;ws1-3&quot;&gt;
    &lt;img src=&quot;/figs/2021-05-31-icra21-radar-ws/ws1-3.png&quot; style=&quot;width:90%&quot; /&gt;
    &lt;figcaption&gt; 
          &lt;center&gt; A scan with top k intensities &lt;/center&gt;
    &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure id=&quot;ws1-2&quot;&gt;
    &lt;img src=&quot;/figs/2021-05-31-icra21-radar-ws/ws1-2.png&quot; style=&quot;width:90%&quot; /&gt;
    &lt;figcaption&gt; 
          &lt;center&gt; CFEAR pipeline &lt;/center&gt;
    &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;dataset-session&quot;&gt;Dataset session&lt;/h2&gt;

&lt;p&gt;나는 dataset session 에서 발표를 가졌다. 세 데이터셋 모두 Navtech radar 에 관한 것이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;나름 초창기에 출시된 데이터셋인 MulRan, Oxford Radar RobotCar, RADIATE dataset 이렇게 세 팀이 10분동안 각각 데이터셋을 홍보하는 시간을 가졌다.
    &lt;ol&gt;
      &lt;li&gt;Oxford Radar RobotCar 는 Oxford RobotCar의 명성을 잇는 느낌이고 (route 도 같음)&lt;/li&gt;
      &lt;li&gt;RADIATE 은 objet lable 과 snow같은 극한상황에 대한 시퀀스가 포함되어있음을 위주로 어필했다.&lt;/li&gt;
      &lt;li&gt;나는 MulRan 에서 multiple cities, multiple loops 를 강조하며 place recognition 과 SLAM연구에 더 적합함을 어필했다.
        &lt;ul&gt;
          &lt;li&gt;그리고 MulRan 에서 함께 제안한 radar scan context 를 이용해서 기존 radar odometry 의 drift 를 해소할 수 있는 &lt;a href=&quot;https://github.com/gisbi-kim/navtech-radar-slam&quot;&gt;project (navtech-radar-slam)&lt;/a&gt; 도 소개하였다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
    &lt;figure id=&quot;example1&quot;&gt;
    &lt;img src=&quot;/figs/2021-05-31-icra21-radar-ws/example1.png&quot; style=&quot;width:90%&quot; /&gt;
    &lt;figcaption&gt; 
          &lt;center&gt; Radar SLAM on MulRan dataset - KAIST 03 &lt;/center&gt;
    &lt;/figcaption&gt;
  &lt;/figure&gt;
    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
    &lt;figure id=&quot;example2&quot;&gt;
    &lt;img src=&quot;/figs/2021-05-31-icra21-radar-ws/example2.png&quot; style=&quot;width:90%&quot; /&gt;
    &lt;figcaption&gt; 
          &lt;center&gt; Radar SLAM on MulRan dataset - Riverside 02 &lt;/center&gt;
    &lt;/figcaption&gt;
  &lt;/figure&gt;
    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;바풋교수님네의 dataset 도 곧 나올거같고, Oxford 애들도 여기저기 오지를 다니는 등 추가시퀀스를 더 공개할 것 같다. 앞으로 연구하기 더 재밌는 판이 만들어질 것으로 기대된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;panel-discussion&quot;&gt;Panel discussion&lt;/h2&gt;

&lt;p&gt;몇 가지 기억나는 이야기들&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;마틴 교수님?:
    &lt;ul&gt;
      &lt;li&gt;radar 는 (vision 쪽과는 좀 다른 거 같은게) HW 하는 사람들 signal processing 사람들 robotics 사람들 다 모여서 협업해야한다.
        &lt;ul&gt;
          &lt;li&gt;(roboticist 입장에서) 처음에는 data 가 좀 dirty 하다고 느낄 수 있겠지만 알고보면 재밌다&lt;/li&gt;
          &lt;li&gt;radar 는 가치있는 정보를 많이 주는 센서임을 알게된다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;바풋 교수님:
    &lt;ul&gt;
      &lt;li&gt;radar data 를 위한 (합의된) interface 가 필요하다.
        &lt;ul&gt;
          &lt;li&gt;예를 들어 TI radar는 sparse point cloud 를 주고 Navtech radar 는 image format 을 준다.&lt;/li&gt;
          &lt;li&gt;radar 는 intensity ray인가? image 인가? 사실 lidar 도 생각해보면 원래는 intensity ray 인데 top 1 (or up to 2nd) point 만을 추려서 3D point cloud 로 사용자에게 주는 것이다. 그렇게 치면 radar 도 point cloud 처럼 사용해도 무방한가? 이렇게 다들 쓰자고 약속할까? 어떻게 해야할까?&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;그리고 더 많은 공개데이터셋들이 아직도 더 필요하고, KITTI와 같이 benchmark를 구성하는 것이 중요할 것이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;the-end&quot;&gt;The end&lt;/h2&gt;
&lt;p&gt;그렇게 한국시각으로 새벽 1시에 워크샵이 끝이 났다. 재밌었따.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;radar 를 이용해서 perception and state estimation 을 하기 이전에, Radar 라는 센서를 아직 잘 모르겠다. 공부를 하자!
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/ZHOUYI1023/awesome-radar-perception&quot;&gt;여기 (awesome-radar-perception)&lt;/a&gt; 에 좋은 자료들을 잘 모아둔 것 같다.&lt;/li&gt;
      &lt;li&gt;연구실형은 &lt;a href=&quot;https://www.ti.com/ko-kr/sensors/mmwave-radar/automotive/technical-documents.html&quot;&gt;TI 공식홈피의 tech docu&lt;/a&gt; 들을 추천함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;주석&quot;&gt;주석&lt;/h3&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:ps1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Lu, Chris Xiaoxuan, et al. “milliEgo: single-chip mmWave radar aided egomotion estimation via deep sensor fusion.” Proceedings of the 18th Conference on Embedded Networked Sensor Systems. 2020. &lt;a href=&quot;#fnref:ps1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:cen18&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Cen, Sarah H., and Paul Newman. “Precise ego-motion estimation with millimeter-wave radar under diverse and challenging conditions.” 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018. &lt;a href=&quot;#fnref:cen18&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Radar Odometry Results on MulRan dataset</title>
   <link href="http://localhost:4000/blog/2021/05/30/yeti-radar-odom-mulran1.html"/>
   <updated>2021-05-30T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/05/30/yeti-radar-odom-mulran1</id>
   <content type="html">&lt;h1 id=&quot;radar&quot;&gt;Radar&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Here, the radar means Navtech radar data.
    &lt;ul&gt;
      &lt;li&gt;For details, see https://oxford-robotics-institute.github.io/radar-robotcar-dataset/&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Radar is robust to occlusions than LiDAR in urban sites.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;yeti-radar-odometry&quot;&gt;Yeti Radar Odometry&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Paper
    &lt;ul&gt;
      &lt;li&gt;Do We Need to Compensate for Motion Distortion and Doppler Effects in Spinning Radar Navigation? (ICRA 2021)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Code
    &lt;ul&gt;
      &lt;li&gt;https://github.com/keenan-burnett/yeti_radar_odometry&lt;/li&gt;
      &lt;li&gt;Features
        &lt;ul&gt;
          &lt;li&gt;implemented cen2018, cen2019 method with motion compensation&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Requirements
        &lt;ul&gt;
          &lt;li&gt;need ray-wise timestamps&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;tutorial-running-yeti-on-mulran-dataset&quot;&gt;Tutorial: running Yeti on MulRan dataset&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;see this video
    &lt;ul&gt;
      &lt;li&gt;link (TBA)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;results-on-mulran-dataset&quot;&gt;Results on MulRan dataset&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;MulRan dataset
    &lt;ul&gt;
      &lt;li&gt;MulRan: Multimodal Range Dataset for Urban Place Recognition (ICRA 2019)&lt;/li&gt;
      &lt;li&gt;https://sites.google.com/view/mulran-pr/dataset&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Methods
    &lt;ul&gt;
      &lt;li&gt;Cen2018: Precise ego-motion estimation with millimeter-wave radar under diverse and challenging conditions (ICRA 2018)&lt;/li&gt;
      &lt;li&gt;Cen2019: Radar-only ego-motion estimation in difficult settings via graph matching (ICRA 2019)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Result captures &lt;sup id=&quot;fnref:ps1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ps1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:ps2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ps2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
    &lt;ul&gt;
      &lt;li&gt;Left: Cen2018, Right: Cen2019&lt;/li&gt;
      &lt;li&gt;The trajectory color means start-blue and red-end.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure id=&quot;KAIST02&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-30-yeti-radar-odom-mulran1/KAIST02.png&quot; style=&quot;width:110%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; KAIST 02 &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure id=&quot;KAIST03&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-30-yeti-radar-odom-mulran1/KAIST03.png&quot; style=&quot;width:110%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; KAIST 03 &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure id=&quot;DCC01&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-30-yeti-radar-odom-mulran1/DCC01.png&quot; style=&quot;width:110%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; DCC 01 &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure id=&quot;DCC02&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-30-yeti-radar-odom-mulran1/DCC02.png&quot; style=&quot;width:110%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; DCC 02 &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure id=&quot;DCC03&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-30-yeti-radar-odom-mulran1/DCC03.png&quot; style=&quot;width:110%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; DCC 03 &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure id=&quot;Riverside01&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-30-yeti-radar-odom-mulran1/Riverside01.png&quot; style=&quot;width:110%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; Riverside 01 &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure id=&quot;Riverside02&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-30-yeti-radar-odom-mulran1/Riverside02.png&quot; style=&quot;width:110%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; Riverside 02 &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure id=&quot;Riverside03&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-30-yeti-radar-odom-mulran1/Riverside03.png&quot; style=&quot;width:110%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; Riverside 03 &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure id=&quot;Sejong02&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-30-yeti-radar-odom-mulran1/Sejong02.png&quot; style=&quot;width:110%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; Sejong 02 &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure id=&quot;Sejong03&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-30-yeti-radar-odom-mulran1/Sejong03.png&quot; style=&quot;width:110%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; Sejong 03 &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;comments&quot;&gt;Comments&lt;/h3&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:ps1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;ps. for the visualiation code, see https://github.com/gisbi-kim/yeti_odom_drawer &lt;a href=&quot;#fnref:ps1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ps2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;ps2. KAIST 01 and Sejong 01 sequences do not provide ray-wise timestamps, so the results are omitted. &lt;a href=&quot;#fnref:ps2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Scan Context-based LiDAR Pose-graph SLAM [구현]</title>
   <link href="http://localhost:4000/blog/2021/05/17/sclidarslam.html"/>
   <updated>2021-05-17T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/05/17/sclidarslam</id>
   <content type="html">&lt;h1 id=&quot;lidar-slam&quot;&gt;LiDAR SLAM&lt;/h1&gt;

&lt;p&gt;SLAM 이란&lt;/p&gt;

\[\begin{align*}
    \text{SLAM} = \text{odometry} + \text{loop closing} 
\end{align*}\]

&lt;p&gt;이라고 할 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;odometry 는 locally accruate 한 consecutive motion 을 예측하고&lt;/li&gt;
  &lt;li&gt;loop closing 은 odometry에서 누적된 error 를 smoothing 한다.
    &lt;ul&gt;
      &lt;li&gt;loop detection 과 pose-graph optimization 으로 구성된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;sc-based-lidar-slam&quot;&gt;SC-based LiDAR SLAM&lt;/h2&gt;

&lt;p&gt;예전에 LiDAR SLAM을 구성하기 위해&lt;br /&gt;
&lt;a href=&quot;https://github.com/irapkaist/scancontext&quot;&gt;Scan Context&lt;/a&gt; 기반의 loop detection 을 lidar odometry 에 통합한 적이 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;https://github.com/irapkaist/SC-LeGO-LOAM&lt;/li&gt;
  &lt;li&gt;https://github.com/gisbi-kim/SC-LIO-SAM&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;처음에는 LeGO-LOAM 저자의 pose-graph optimziation 구현 (skeleton) 이 좋아서 거기에 integrate 했었는데,&lt;br /&gt;
사용하다보니 다른 front-end 에 물렸으면 싶을 때가 종종 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ex. LeGO-LOAM은 range image 상에서 feature 를 찾기 때문에, roll pitch motion 이 왕왕 큰 hand-held 에서 A-LOAM보다 경험적으로 잘 안된다던지 그런 현상들을 좀 겪는다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;sc-based-pose-graph-slam&quot;&gt;SC-based Pose-graph SLAM&lt;/h2&gt;

&lt;p&gt;그래서 loop detection 및 pose-graph optimization 부분만&lt;br /&gt;
별도의 ros node 로 구성해보았다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/gisbi-kim/SC-A-LOAM/blob/main/src/laserPosegraphOptimization.cpp&quot;&gt;laserPosegraphOptimization.cpp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;사용자는 임의의 lidar odometry 를 실행하고,&lt;br /&gt;
그 임의의 lidar odometry 프로그램은 time t에서의 odometry 와 point cloud topic 을 publish 시켜주기만 하면된다.&lt;/p&gt;

&lt;p&gt;그럼 위의 laserPosegraphOptimization.cpp node 는 그 odometry (locally errornous한) 와 point cloud topic 을 subscribe 해서&lt;br /&gt;
내부적으로 loop 를 찾고 pose graph optimization 을 수행한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;결과적으로는 보정된 odometry (및 path) 와 error 가 해소된 global map 을 publish 한다. Rviz 로 이를 확인할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;laserPosegraphOptimization.cpp 모듈을 SC-PGO라고 부른다면,&lt;br /&gt;
LiDAR SLAM의 전체 파이프라인은 아래와 같다.&lt;/p&gt;
&lt;figure id=&quot;scfastlio&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-17-sclidarslam/anypipe.png&quot; style=&quot;width:90%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; &lt;a href=&quot;https://github.com/gisbi-kim/SC-A-LOAM&quot;&gt; Code Link &lt;/a&gt; &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 리포지토리에서는 존재하는 open source 중 가장 간단한 lidar odometry인&lt;br /&gt;
A-LOAM과 결합하여 바로 사용할 수 있도록 하였다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;코드: &lt;a href=&quot;https://github.com/gisbi-kim/SC-A-LOAM&quot;&gt;SC-A-LOAM (github)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;예시:&lt;/li&gt;
&lt;/ul&gt;
&lt;figure id=&quot;scfastlio&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-17-sclidarslam/riverside01.png&quot; style=&quot;width:90%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; &lt;a href=&quot;https://youtu.be/FwAVX5TVm04?t=303&quot;&gt; Youtube Link (대전 갑천)&lt;/a&gt; &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;generic-and-modular&quot;&gt;Generic and Modular&lt;/h2&gt;

&lt;p&gt;하지만 SC-PGO (laserPosegraphOptimization.cpp) 는&lt;br /&gt;
lidar odometry algorithm에 완전히 독립적인 모듈이기 때문에&lt;br /&gt;
어떤 lidar odometry front-end 와도 결합될 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;따라서 SC-based Generic (modular) Pose-graph SLAM 라고 부를 수 있겠다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;예시로, FAST-LIO라는 ESKF 기반 최신 LIO 와도 물려보았다 (아래 예시 비디오).&lt;/p&gt;
&lt;figure id=&quot;scfastlio&quot;&gt;
  &lt;img src=&quot;/figs/2021-05-17-sclidarslam/scfastlio.png&quot; style=&quot;width:85%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; &lt;a href=&quot;https://youtu.be/Fw9S6D6HozA?t=1708&quot;&gt; Youtube Link (including step-by-step tutorial) &lt;/a&gt; &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;
(당연한 이야기지만) odometry only 일 때 누적된 에러를 “쉽게” 보정할 수 있다 – 사용자는 odometry 와 loop closing 의 결합에 대해 고민할 필요없이, 단순히 두 node 를 각각 실행해주는 것으로 LiDAR SLAM을 완성할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ps. 이 예제에서 알 수 있듯이 Scan Context 는 livox lidar (전방 70도만 봄) 에서도 잘 작동함을 알 수 있다.&lt;/li&gt;
  &lt;li&gt;ps 2. 또한 odometry topic 은 특성 sensor 에 dependent하지 않기 때문에 꼭 lidar odometry가 아니라 vo, wheel odometry 등 다양한 source 와도 쉽게 결합할 수 있을 것으로 생각된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- ---  --&gt;
&lt;!-- ### 주석 --&gt;
</content>
 </entry>
 
 <entry>
   <title>Filter-based VIO [1편] — MSCKF 계열 history 정리</title>
   <link href="http://localhost:4000/blog/2021/04/27/msckf-history.html"/>
   <updated>2021-04-27T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/04/27/msckf-history</id>
   <content type="html">&lt;p&gt;&lt;span style=&quot;color:gray&quot;&gt; NOTE: 아래 내용들은 김연조 et al 의 “속도증분벡터를 활용한 ORB-SLAM 및 관성항법 결합 알고리즘 연구 (2019)” 논문의 관련연구 섹션을 참고하였습니다. &lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;MSCKF 계열은 Tightly-coupled Filter-based&lt;sup id=&quot;fnref:note1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:note1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 라고 할 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;특징
    &lt;ol&gt;
      &lt;li&gt;많은 특징점 또는 이미지 패치를 필요로 하지 않는다.&lt;/li&gt;
      &lt;li&gt;Optimization-based VIO보다 적은연산량을 필요로 한다.&lt;/li&gt;
      &lt;li&gt;따라서 가벼운 onboard platform에서 선호된다.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;msckf-2007&quot;&gt;MSCKF (2007)&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;요약&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;특징점&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;대신&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;지난&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;시점의&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;카메라&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;포즈를&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;상태변수에&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;포함&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;특징점의 확률분포를 가우시안 분포로 근사할 필요가 없기 때문에, 더욱 고정밀의 포즈 추정이 가능&lt;/li&gt;
  &lt;li&gt;논문&lt;sup id=&quot;fnref:note2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:note2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
    &lt;ul&gt;
      &lt;li&gt;07 ICRA A. I. Mourikis and S. I. Roumeliotis, “A multi-state constraint Kalman filter for vision-aided inertial navigation”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;msckf의-후속-2012-2013&quot;&gt;MSCKF의 후속 (2012, 2013)&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;요약&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;camera와&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IMU&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;사이&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;캘리브레이션&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;파라미터&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;역시&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;상태변수에&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;포함&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;캘리브레이션 파라미터와 요(yaw) 각에 대한 가관측성 (Observability) 분석도 수행함.
    &lt;ul&gt;
      &lt;li&gt;필터의 일관성 (consistency)을 향상&lt;/li&gt;
      &lt;li&gt;결과적으로 추정된 상태변수의 불확실성(uncertainty)과 실제 불확실성을 일치시키려 함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;논문
    &lt;ul&gt;
      &lt;li&gt;note: MSCKF 1저자인 Mourikis가 교신저자&lt;/li&gt;
      &lt;li&gt;12 ICRA (학회버전) M. Li and A. I. Mourikis, “Improving the accuracy of EKF-based visual-inertial odometry”&lt;/li&gt;
      &lt;li&gt;13 IJRR (저널버전) M. Li and A. I. Mourikis, “High-precision, consistent EKF-based visual-inertial odometry”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;msckf의-후속2-2015-&quot;&gt;MSCKF의 후속2 (2015-)&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;요약&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;특징점&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geometric&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;대신&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;패치의&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;photometric&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;를&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;사용&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;특징점을 사용하지 않기 때문에, 모션블러 또는 텍스쳐가 적은 환경에 강인하다는 특징을 갖는다.&lt;/li&gt;
  &lt;li&gt;논문
    &lt;ul&gt;
      &lt;li&gt;15 IROS P. Tanskanen, T. Naegeli, M. Pollefeys, and O. Hilliges, “Semi-direct EKF-based monocular visual-inertial odometry”&lt;/li&gt;
      &lt;li&gt;15 IROS M. Bloesch, S. Omari, M. Hutter, and R. Siegwart, “Robust visual inertial odometry using a direct EKF-based approach”&lt;/li&gt;
      &lt;li&gt;17 IJRR M. Bloesch, M. Burri, S. Omari, M. Hutter, and R. Siegwart, “Iterated extended Kalman filter based visual-inertial odometry using direct photometric feedback”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;msckf의-후속3-2018&quot;&gt;MSCKF의 후속3 (2018)&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;요약&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;steroe&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;version&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;논문
    &lt;ul&gt;
      &lt;li&gt;18 RAL Robust stereo visual inertial odometry for fast autonomous flight&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;주석&quot;&gt;주석&lt;/h3&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:note1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;이외에 Loosely coupled filter-based 도 있는데, loosely coupled 는 일반적으로 VO 알고리즘의 해를 칼만 필터의 측정치(measurement)로 사용하여 결합한다. &lt;br /&gt; 따라서 특징점 수에 상관없이 항상 같은 수준의 계산 복잡도를 가진다 (관측된 특징점을 상태변수에 포함시키지 않기 때문). &lt;br /&gt; 이러한 약결합 방식은 강결합 방식에 비해 필터 구조가 간단하기 때문에, 다른 센서를 융합함으로써 성능을 향상시키기 용이하다 &lt;br /&gt; 논문: &lt;br /&gt; — 11 ICRA Real-time metric state estimation for modular vision-inertial systems (스케일 팩터를 상태변수에 추가함), &lt;br /&gt; — 13 ICRA Stereo vision and IMU based real-time ego-motion and depth image computation on a handheld device (스테레오 버전), &lt;br /&gt; — 13 IROS A robust and modular multi-sensor fusion approach applied to MAV navigation and 14 ICRA Multi-Sensor Fusion for Robust Autonomous Flight in Indoor and Outdoor Environments with a Rotorcraft MAV (IMU, laser scanner, stereo cameras, pressure altimeter, magnetometer, and a GPS 등 다양한 다른 센서를 융합한 버전) &lt;a href=&quot;#fnref:note1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:note2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;이 논문을 이해하기 위해서는 IMU model과 quaternion에 대한 기초지식이 요구된다. &lt;br /&gt; — 이에 대해서는 Trawny, Nikolas, and Stergios I. Roumeliotis. “Indirect Kalman filter for 3D attitude estimation. (2005) 를 추천한다 (note: MSCKF논문의 교신저자인 Roumeliotis가 여기서도 교신저자이다).  &lt;br /&gt; — 또는 최근 문서인 Joan Sola의 Quaternion kinematics for the error-state Kalman filter (2017) 도 되게 잘 설명하고 있다. &lt;br /&gt; — ps. 일반적으로 항법 추정문제에서 칼만 필터는 추정 변수의 “오차”를 상태변수로 사용한다 - 이런 세팅을 indirect 라고 부른다. 그래서 위 두 테크리포트의 제목에 indirect 라든가 error-state 라든가 하는 단어가 포함되어 있는 것을 볼 수 있다. &lt;a href=&quot;#fnref:note2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Iterative Optimization [1편] — Nonlinear ICP 구현을 통해 알아보는 Gauss-newton Optimization</title>
   <link href="http://localhost:4000/blog/2021/03/16/leastsquare-1.html"/>
   <updated>2021-03-16T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/03/16/leastsquare-1</id>
   <content type="html">&lt;p&gt;TBA&lt;/p&gt;

&lt;!-- ---
### 주석 --&gt;
</content>
 </entry>
 
 <entry>
   <title>Bayesian Filtering [2편] — Recursive estimation 의 시작 (칼만필터 유도)</title>
   <link href="http://localhost:4000/blog/2021/03/09/bayesfiltering-2.html"/>
   <updated>2021-03-09T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/03/09/bayesfiltering-2</id>
   <content type="html">&lt;h1 id=&quot;개요&quot;&gt;개요&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;/blog/2021/03/09/bayesfiltering-1.html#proofdone&quot;&gt; 앞선 포스팅 &lt;/a&gt;에서 우리는 posterior 의 mean 과 covariance 가&lt;br /&gt;
prior 와 likelihood 로부터 closed form 으로 유도됨을 살펴보았다.&lt;/p&gt;

&lt;p&gt;근데 지난포스팅에서도 몇번 이야기 했듯이:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Bayesian analysis 의 묘미는 현재 턴의 posterior 가 다음 턴의 prior 로 쓰이는 것&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:gray&quot;&gt; [Filter-based SLAM] &lt;/span&gt; 시리즈의 &lt;a href=&quot;/blog/2021/03/09/bayesfiltering-1.html&quot;&gt; 이전 편 &lt;/a&gt; 을 마무리했던 그 식 을 다시 들고와보자.&lt;/p&gt;

\[\begin{equation}
  p(\textbf{x} | \textbf{z}) = \mathcal{N} \left( \text{mean: } \left(\textbf{P}_0^{-1} + \frac{1}{\sigma^2}\textbf{H}^{T}\textbf{H} \right)^{-1} \left( \textbf{P}_0^{-1} \textbf{m}_0 + \frac{1}{\sigma^2}\textbf{H}^{T}\textbf{z} \right), \\
  \ \text{covariance: } \left(\textbf{P}_0^{-1} + \frac{1}{\sigma^2}\textbf{H}^{T}\textbf{H} \right)^{-1} \right) \tag{eq.1}\label{eq:1}
\end{equation}\]

&lt;p&gt;이 식은 batch estimation 이었다고 할 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;즉 measurement ($\textbf{z} = \textbf{z}_{1:k}$) 들이 한번에 모두 주어져있고,&lt;/li&gt;
  &lt;li&gt;우리가 알고싶은 state $\textbf{x}$ 에 대한 초기 mean $\textbf{m}_0$, covariance $\textbf{P}_0$ 가 initial value 로써 주어지는 상황이었다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이번 포스팅에서는 이것을 recursive version 으로 바꾸어 볼 것이다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;recursive-estimation&quot;&gt;Recursive estimation&lt;/h1&gt;

&lt;p&gt;사실 여기서부터는 쉽다.&lt;br /&gt;
“현재 턴의 posterior 가 다음 턴의 prior 로 쓰인다”는 묘미를 기억하자.&lt;/p&gt;

&lt;p&gt;식 \eqref{eq:1} 에서의 posterior의 mean 과 covariance 가&lt;br /&gt;
각각 $\textbf{m}_{t-1}$, $\textbf{P}_{t-1}$ 였다고 하자.&lt;/p&gt;

&lt;p&gt;그리고 지금은 time$=t$ 가 되어서,&lt;br /&gt;
새로운 measurement $z_t$ 가 하나 들어온 상황이다.&lt;br /&gt;
(꼭 딱 하나일 필요는 없지만 편의상 하나라고 해보자)&lt;/p&gt;

&lt;p&gt;그러면 위의 식 \eqref{eq:1} 에서,&lt;br /&gt;
time$=t-1$의 posterior 가 다음 턴 time$=t$의 prior 로 쓰이기 때문에,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\textbf{P}_0$ 자리에 $\textbf{P}_{t-1}$ 라고 말만 다시 바꿔달면 된다.&lt;/li&gt;
  &lt;li&gt;$\textbf{m}_0$ 자리에 $\textbf{m}_{t-1}$ 라고 말만 다시 바꿔달면 된다.&lt;/li&gt;
  &lt;li&gt;measurement $\textbf{z}$ 자리에 $z_{t}$ 라고 말만 다시 쓰면 된다.&lt;/li&gt;
  &lt;li&gt;그리고 measurement 의 jacobian 인 $\textbf{H}$는 현재 턴에 새로 들어온 measurement 의 것이므로 위의 식에서 $\textbf{H}$ 를 $\textbf{H}_{t}$ 라고 써주면 말이 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그러면 식은 다음과 같다:&lt;/p&gt;

\[\begin{align}
  p(\textbf{x}_{t} | z_t) = \mathcal{N} \left(  \text{mean: } \left(\textbf{P}_{t-1}^{-1} + \frac{1}{\sigma^2}\textbf{H}_{t}^{T}\textbf{H}_{t} \right)^{-1} \left( \textbf{P}_{t-1}^{-1} \textbf{m}_{t-1} + \frac{1}{\sigma^2}\textbf{H}_{t}^{T}z_t \right), \\
   \text{covariance: } \left(\textbf{P}_{t-1}^{-1} + \frac{1}{\sigma^2}\textbf{H}_{t}^{T}\textbf{H}_{t} \right)^{-1} \right) \tag{eq.2}\label{eq:2}
\end{align}\]

&lt;p&gt;완성!&lt;/p&gt;

&lt;p&gt;참 쉽죠?&lt;/p&gt;

&lt;p&gt;우리는 이제 관측 데이터가 순차적으로 하나씩 들어오더라도,&lt;/p&gt;

&lt;p&gt;예전 예측을 반영해서 최적해를 update 해나갈 수 있게 되었다!&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;recursive-estimation-continued&quot;&gt;Recursive estimation: continued&lt;/h1&gt;

&lt;p&gt;식을 조금만 더 수리해보자.&lt;/p&gt;

&lt;p&gt;재밌는게 나올 수 있다.&lt;/p&gt;

&lt;p&gt;위의 recursive 식에서 posterior (즉, update 된) 의 covariance 가&lt;br /&gt;
$\left(\textbf{P}_{t-1}^{-1} + \frac{1}{\sigma^2}\textbf{H}_{t}^{T}\textbf{H}_{t} \right)^{-1}$ 였다.&lt;/p&gt;

&lt;p&gt;지금 measurement 의 noise 를 scalar 라고 가정해서 식이 저런건데&lt;br /&gt;
사실은 이렇게 써져있는 거라고 머릿속에서 식의 생김새를 생각해주자&lt;/p&gt;

\[\begin{equation}
    \textbf{P}_{t} = \left(\textbf{P}_{t-1}^{-1} + \textbf{H}_{t}^{T}(\sigma^{-2}\textbf{I})\textbf{H}_{t} \right)^{-1}
    \tag{eq.3}\label{eq:3}
\end{equation}\]

&lt;p&gt;&lt;span style=&quot;color:gray&quot;&gt; (추후에 저 $\sigma^{-2}\textbf{I}$ 자리에 대신 임의의 noise matrix $\textbf{R}$ 같은걸로 교체해주기만 하면된다) &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;근데 선형대수 꿀팁 중에 matrix inversion lemma 라는 것이 있다 &lt;sup id=&quot;fnref:woodbury&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:woodbury&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;대략 이런걸 해준다.&lt;/p&gt;

\[\begin{equation}
  (\textbf{A} + \textbf{C} \textbf{B} \textbf{C}^{T})^{-1} = \textbf{A}^{-1} - \textbf{A}^{-1}\textbf{C} ( \textbf{C}^{T}\textbf{A}^{-1} \textbf{C} + \textbf{B}^{-1})^{-1} \textbf{C}^{T} \textbf{A}^{-1}
  \tag{eq.4}\label{eq:4}
\end{equation}\]

&lt;p&gt;근데 우리의 covariance \eqref{eq:3} 가 딱 보니 \eqref{eq:4} 의 좌변의 생김새와 완전히 같지 않은가?&lt;/p&gt;

&lt;p&gt;이렇게 치환해서&lt;/p&gt;

\[\begin{equation}
    \textbf{A} = \textbf{P}_{t-1}^{-1} \\ 
    \textbf{B} = \sigma^{-2}\textbf{I} \\ 
    \textbf{C} = \textbf{H}_{t}^{T}
\end{equation}\]

&lt;p&gt;식을 쭉 전개하면&lt;/p&gt;

\[\begin{align}
    \textbf{P}_{t} &amp;amp;= \left(\textbf{P}_{t-1}^{-1} + \textbf{H}_{t}^{T}(\sigma^{-2}\textbf{I})\textbf{H}_{t} \right)^{-1} \\
    &amp;amp;= \textbf{P}_{t-1} - \textbf{P}_{t-1}\textbf{H}_{t}^{T} \left(  \textbf{H}_{t} \textbf{P}_{t-1} \textbf{H}_{t}^{T} + \sigma^{2}  \right)^{-1} \textbf{H}_{t}\textbf{P}_{t-1} \tag{eq.5}\label{eq:5}
\end{align}\]

&lt;p&gt;\eqref{eq:5} 가 된다!&lt;/p&gt;

&lt;p&gt;와~~~&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;recursive-estimation-kalman-filter-등장&quot;&gt;Recursive estimation: Kalman Filter 등장&lt;/h1&gt;

&lt;p&gt;그래서 어쩌라고? 싶겠지만&lt;/p&gt;

&lt;p&gt;한 번 더 식을 다듬어 보자!&lt;/p&gt;

\[\begin{align}
    \textbf{P}_{t} &amp;amp;= \left(\textbf{P}_{t-1}^{-1} + \textbf{H}_{t}^{T}(\sigma^{-2}\textbf{I})\textbf{H}_{t} \right)^{-1} \\
    &amp;amp;= \textbf{P}_{t-1} - \underbrace{ \textbf{P}_{t-1}\textbf{H}_{t}^{T} \left( \underbrace{ \textbf{H}_{t} \textbf{P}_{t-1} \textbf{H}_{t}^{T} + \sigma^{2} }_{\textbf{S}_t} \right)^{-1} }_{\textbf{K}_t} \textbf{H}_{t}\textbf{P}_{t-1}  \\
    &amp;amp;= \textbf{P}_{t-1} - \textbf{K}_t \textbf{S}_t  \textbf{K}_t^{T} \  \text{ // or } \\ 
    &amp;amp;= \textbf{P}_{t-1} - \textbf{K}_t \textbf{H}_t \textbf{P}_{t-1} \  \text{ // seems more frequently used form} \\ 
    &amp;amp;= ( \textbf{I} - \textbf{K}_t \textbf{H}_t)  \textbf{P}_{t-1} 
    \tag{eq.6}\label{eq:6}
\end{align}\]

&lt;p&gt;covariance 에 대한 update 식이 조금 더 깔쌈하게 만들어진 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;근데 이거 어디서 좀 본 모양 같은데…?&lt;/p&gt;

&lt;p&gt;학부 기계전기 등 로보틱스 관련수업에서 무조건 배우는 Kalman Filter 아닌가 …?&lt;/p&gt;

&lt;p&gt;호옴 …&lt;/p&gt;

&lt;p&gt;mean 에 대한 update 식도 좀 더 다듬어 보자.&lt;/p&gt;

\[\begin{align}
\textbf{m}_{t} &amp;amp;= \textbf{P}_t \left( \textbf{P}_{t-1}^{-1}\textbf{m}_{t-1} + \frac{1}{\sigma^2}\textbf{H}_{t}^{T}z_t \right) \\ 
    &amp;amp;= ( \textbf{I} - \textbf{K}_t \textbf{H}_t)  \textbf{P}_{t-1}  \left( \textbf{P}_{t-1}^{-1}\textbf{m}_{t-1} + \frac{1}{\sigma^2}\textbf{H}_{t}^{T}z_t \right) \\ 
    &amp;amp;= ( \textbf{I} - \textbf{K}_t \textbf{H}_t)  \left( \textbf{P}_{t-1}\textbf{P}_{t-1}^{-1}\textbf{m}_{t-1} + \textbf{P}_{t-1}\frac{1}{\sigma^2}\textbf{H}_{t}^{T}z_t \right) \\ 
    &amp;amp;= ( \textbf{I} - \textbf{K}_t \textbf{H}_t)  \left( \textbf{m}_{t-1} + \frac{1}{\sigma^2}\textbf{P}_{t-1}\textbf{H}_{t}^{T} z_t \right) \\
    &amp;amp;= \textbf{m}_{t-1} - \textbf{K}_t \textbf{H}_t \textbf{m}_{t-1} + \frac{1}{\sigma^{2}}( \textbf{I} - \textbf{K}_t \textbf{H}_t) (\textbf{P}_{t-1}\textbf{H}_{t}^{T}) z_t
    \tag{eq.7}\label{eq:7}
\end{align}\]

&lt;p&gt;근데 재밌는게&lt;/p&gt;

\[\begin{align}
    \frac{1}{\sigma^{2}}( \textbf{I} - \textbf{K}_t \textbf{H}_t) (\textbf{P}_{t-1}\textbf{H}_{t}^{T}) = \textbf{K}_t
    \tag{eq.8}\label{eq:8}
\end{align}\]

&lt;p&gt;이다!&lt;br /&gt;
양변에 곱하고 빼고 요리조리하면 direct 증명이 쉽게 가능하지만 굳이 또 적어보자면:&lt;/p&gt;

\[\begin{align}
    \frac{1}{\sigma^{2}}( \textbf{I} - \textbf{K}_t \textbf{H}_t) (\textbf{P}_{t-1}\textbf{H}_{t}^{T}) &amp;amp;= \textbf{K}_t \\
    ( \textbf{I} - \textbf{K}_t \textbf{H}_t) (\textbf{P}_{t-1}\textbf{H}_{t}^{T}) &amp;amp;= \sigma^{2}\textbf{K}_t \\ 
    \textbf{P}_{t-1}\textbf{H}_{t}^{T} - \textbf{K}_t \textbf{H}_t \textbf{P}_{t-1}\textbf{H}_{t}^{T} &amp;amp;= \sigma^{2}\textbf{K}_t \\ 
    \textbf{P}_{t-1}\textbf{H}_{t}^{T} &amp;amp;= \textbf{K}_t \textbf{H}_t \textbf{P}_{t-1}\textbf{H}_{t}^{T} + \sigma^{2}\textbf{K}_t \\ 
    \textbf{P}_{t-1}\textbf{H}_{t}^{T} &amp;amp;= \textbf{K}_t \left( \textbf{H}_t \textbf{P}_{t-1}\textbf{H}_{t}^{T} + \sigma^{2} \right) \\ 
    \textbf{P}_{t-1}\textbf{H}_{t}^{T} &amp;amp;= \textbf{K}_t \textbf{S}_t \\ 
    \textbf{P}_{t-1}\textbf{H}_{t}^{T}\textbf{S}_t^{-1} &amp;amp;= \textbf{K}_t    
\end{align}\]

&lt;p&gt;마지막 줄은 $\textbf{K}_t$의 정의였으므로 자명하고 따라서 성립한다.&lt;/p&gt;

&lt;p&gt;암튼 그래서 \eqref{eq:8} 을 \eqref{eq:7} 에 대입하면,&lt;/p&gt;

\[\begin{align}
\textbf{m}_{t} &amp;amp;= \textbf{m}_{t-1} - \textbf{K}_t \textbf{H}_t \textbf{m}_{t-1} + \textbf{K}_t z_t \\
    &amp;amp;= \textbf{m}_{t-1} + \textbf{K}_t \left( z_t - \textbf{H}_t \textbf{m}_{t-1} \right)
    \tag{eq.9}\label{eq:9} 
\end{align}\]

&lt;p&gt;오잉 근데 이거도 어디서 좀 많이 본 거 같은데 …?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/pdf/slam04-ekf.pdf&quot;&gt; Cyrill 교수님의 강의자료 &lt;/a&gt;를 보면&lt;/p&gt;

&lt;figure id=&quot;kf&quot;&gt;
  &lt;img src=&quot;/figs/2021-03-12-bayesfiltering-2/kf.png&quot; style=&quot;width:90%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; &lt;a href=&quot;#kf&quot;&gt; Figure: KF equation &lt;/a&gt; &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;4, 5, 6 번째 줄이 우리가 방금 유도한 $\textbf{K}_t$, $\textbf{m}_t$, $\textbf{P}_t$ 와 생김새가 같음을 알 수 있다!&lt;/p&gt;

&lt;p&gt;그렇다면 &lt;a href=&quot;#kf&quot;&gt; 위의 slide &lt;/a&gt; 에서 2, 3번째 줄은 뭘까?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;바로 Kalman filter 에서 prediction step 이라 불리는 부분이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그리고 우리가 오늘 유도한 것은&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Kalman filter 의 update step 에 해당하는 부분이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, 우리는&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2021/03/09/bayesfiltering-1.html#proofdone&quot;&gt; 지난 포스팅 &lt;/a&gt; 에서 batch 로 bayesian update 를 수행했고,&lt;/li&gt;
  &lt;li&gt;이것이 자연스럽게 recursive 한 식이 됨을 이번 포스팅 앞 부분에서 소개했고,&lt;/li&gt;
  &lt;li&gt;그것의 모양을 요래조래 하다보니 Kalman filter 의 update step 와 동일하다는 사실까지 오게되었다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, 오늘 우리는 그 유명한 Kalman filter 의 special 버전 (i.e., update step만 있는) 을 유도했다.&lt;/p&gt;

&lt;p&gt;근데 update step만 있다는 것은 무슨 의미일까?&lt;/p&gt;

&lt;p&gt;바로 우리가 예측하고자 하는 state $\textbf{x}$ 값이 들어오는 measurement 에 관계없이 “고정” 된 상태라는 뜻이다.&lt;/p&gt;

&lt;p&gt;예를 들어서 line fitting 할 때 line 의 계수 같은 게 될 수 있을 것이다.&lt;/p&gt;

&lt;p&gt;하지만 그 $\textbf{x}$ 가 “움직이는” robot 의 위치처럼, 계속 바뀌는 값이라면?&lt;/p&gt;

&lt;p&gt;이 때 바로 prediction step 이 필요하게 된다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;예고&quot;&gt;예고&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;다음 편에서는 칼만필터를 완성해보겠습니다 &lt;sup id=&quot;fnref:book&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:book&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;ps&quot;&gt;P.S.&lt;/h2&gt;

&lt;p&gt;mean update 식을 다시 보면,&lt;/p&gt;

\[\begin{align}
\textbf{m}_{t} &amp;amp;= \textbf{m}_{t-1} - \textbf{K}_t \textbf{H}_t \textbf{m}_{t-1} + \textbf{K}_t z_t \\
    &amp;amp;= \textbf{m}_{t-1} + \textbf{K}_t \left( z_t - \textbf{H}_t \textbf{m}_{t-1} \right)
\end{align}\]

&lt;p&gt;$\textbf{H}_t \textbf{m}_{t-1}$ 가 보이는데,&lt;br /&gt;
이는 앞선 포스팅에서 소개한 &lt;a href=&quot;/blog/2021/03/09/bayesfiltering-1.html#zmodel&quot;&gt; measurement model &lt;/a&gt; 이다. 즉, $\textbf{H}_t \textbf{m}_{t-1} = \hat{z_{t}}$&lt;br /&gt;
(hat 은 예측값이라는 의미)&lt;/p&gt;

&lt;p&gt;따라서 위의 식을 의미상 다음과 같이 다시 적을 수 있는데:&lt;/p&gt;

\[\begin{align}
\textbf{m}_{t} &amp;amp;= \textbf{m}_{t-1} - \textbf{K}_t \textbf{H}_t \textbf{m}_{t-1} + \textbf{K}_t z_t \\
    &amp;amp;= \textbf{m}_{t-1} + \textbf{K}_t \left( z_t - \hat{z_{t}} \right)
\end{align}\]

&lt;p&gt;여기서 어떤 냄새를 맡을 수 있다.&lt;/p&gt;

&lt;p&gt;hat 은 예측값이라는 의미이기 때문에, $z_t - \hat{z_{t}}$ 의 의미는 예측값과 실측값의 차이라는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#kf&quot;&gt; 위의 slide &lt;/a&gt; 에서 5번째 줄을 보면&lt;br /&gt;
updated 될 mean 값은: 초벌로 먼저 predicted 한 mean (hyphen이 그어져있는 게 predicted 란 의미) 에 어떤 값을 보상해주는 것으로 해석할 수 있다.&lt;/p&gt;

&lt;p&gt;그리고 그 보상의 정도는 $\textbf{K}$ 에 실측값과 예측값의 차이가 곱해진 정도이다.&lt;/p&gt;

&lt;p&gt;이 때 그 보상의 정도를 조절하는 $\textbf{K}$ 가 바로 칼만필터에서 kalman gain 이라 불리는 것이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;다음 편에서 계속 …&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;주석&quot;&gt;주석&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:woodbury&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Woodbury formula, Woodbury identity 등으로도 알려져있다. &lt;a href=&quot;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt; 출처 matrix cookbook &lt;/a&gt; &lt;a href=&quot;#fnref:woodbury&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:book&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;이번편의 유도방식은 다음 책의 chapter 3.2 를 참고했습니다: Särkkä, Simo. Bayesian filtering and smoothing. No. 3. Cambridge University Press, 2013. &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.461.4042&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt; PDF link &lt;a&gt;&lt;/a&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:book&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Bayesian Filtering [1편] — posterior 의 mean, covariance 구하기</title>
   <link href="http://localhost:4000/blog/2021/03/09/bayesfiltering-1.html"/>
   <updated>2021-03-09T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/03/09/bayesfiltering-1</id>
   <content type="html">&lt;h1 id=&quot;개요&quot;&gt;개요&lt;/h1&gt;

&lt;p&gt;SLAM은 filter-based 와 optimization-based 로 편의상 나눌 수 있다.&lt;br /&gt;
(물론 두 방법이 robot 의 state 를 추정하는데 상보적으로 동시에 쓰일수도 있다.)&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:gray&quot;&gt; [Factor graph SLAM] &lt;/span&gt; 에서는&lt;br /&gt;
optimization-based 기반의 SLAM에 대해서 소개하고 있다.&lt;/p&gt;

&lt;p&gt;한편,  &lt;span style=&quot;color:gray&quot;&gt; [Filter-based SLAM] &lt;/span&gt; 시리즈에서는&lt;br /&gt;
filter-based SLAM 에 대해서 소개하려고 하는데,&lt;/p&gt;

&lt;p&gt;이 포스트 (와 몇 편 더) 에서는, 그전에&lt;br /&gt;
filter-based SLAM의 근간 (mathmatical machinary)이 되는&lt;br /&gt;
Bayesian analysis 자체에 대해 먼저 소개하려고 한다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;bayesisan-analysis-란&quot;&gt;Bayesisan Analysis 란?&lt;/h1&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;p&gt;Bayesisan Analysis 이 무엇인지 설명하기 전에,&lt;br /&gt;
Bayesisan Analysis 이 왜 필요한지, 예시 상황으로부터 시작해보자.&lt;/p&gt;

&lt;p&gt;우리는 어떤 한 시점에서 로봇의 위치를 알고싶다.&lt;br /&gt;
이 경우 위치를 $\textbf{x}_{i} = (x, y)$ 의 두 값으로 표현할 수 있다. (heading 은 편의상 생략)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;상황 1:
    &lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
    &lt;p&gt;로봇은 위치를 추정하기 위해서 GPS를 사용했다. &lt;br /&gt;
GPS는 로봇의 위치가 $(1,2)$ 라고 알려주었다 (measurement, $\textbf{z}_1$).&lt;/p&gt;

    &lt;p&gt;그래서 로봇은 자기의 위치를 $(1,2)$ 이라고 생각하고 여러 미션을 수행하려고 한다.&lt;/p&gt;

    &lt;p&gt;근데 GPS는 로봇의 위치가 다시 $(1.5, 3)$ 라고 알려주었다 (measurement 2, $\textbf{z}_2$).&lt;br /&gt;
그래서 로봇은 자기의 위치를 다시 $(1.5, 3)$ 으로 바꿨다.&lt;br /&gt;
근데 GPS는 로봇의 위치가 다시 $(2.5, 1)$ 라고 알려주었다 (measurement 3, $\textbf{z}_3$).&lt;br /&gt;
그래서 로봇은 자기의 위치를 다시 $(2.5, 1)$ 으로 바꿨다.&lt;/p&gt;

    &lt;p&gt;… 무한반복 …&lt;br /&gt;
로봇은 확신없이 자기 위치를 결국 추정하지 못하고 결국 세월이 다갔다 …&lt;/p&gt;

    &lt;p&gt;이렇게, 매번 개별 measurement만의 가능도 (likelihood) 를 최대화 해주는 방식으로&lt;br /&gt;
위치를 추정하는 게 좋은 방법일까?&lt;br /&gt;
이러면 그때그때 들어오는 measurement 에 biased 된 예측이 내려질 것이다&lt;sup id=&quot;fnref:likelihood&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:likelihood&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

  &lt;/li&gt;
&lt;/ul&gt;

&lt;p id=&quot;situ2&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;상황 2:
    &lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
    &lt;p&gt;상황1과 똑같이,&lt;br /&gt;
로봇은 위치를 추정하기 위해서 GPS를 사용했다. &lt;br /&gt;
GPS는 로봇의 위치가 $(1,2)$ 라고 알려주었다 (measurement, $\textbf{z}_1$).&lt;/p&gt;

    &lt;p&gt;근데 이 로봇은 자기의 현재 위치가 $(0.5, 2)$ 라고 믿고 있는 상태이다 (prior).&lt;br /&gt;
이 값은 현재 시점에서 측정 (measurement) 한 것은 아니지만 로봇이 알고있는 어떤 믿음이다.&lt;br /&gt;
로봇은 이 경우에도 상황1에서와 같이 현재 측정된 정보만을 최대화 하도록&lt;br /&gt;
자기의 위치를 추정해야할까?&lt;/p&gt;

    &lt;p&gt;흠 근데 그건 좀 unreasonable 해 보인다.&lt;br /&gt;
prior 와 measurement 두 정보를 모두 고려하는 게 좋을 것 같다.&lt;/p&gt;

    &lt;p&gt;근데 만약 prior 와 measurement 두 정보를 모두 고려한다 하더라도,&lt;br /&gt;
수학적으로 어떻게 융합하여야 하는가?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이에 대답하기 전에 먼저 아주 간단한 수학을 recap하고 넘어가자.&lt;/p&gt;

&lt;h2 id=&quot;bayes-rule&quot;&gt;Bayes Rule&lt;/h2&gt;

&lt;p&gt;Bayesisan Analysis 의 기본 틀은 Bayes Rule 이다.&lt;br /&gt;
대학교 확률과 통계 수업을 들으면&lt;br /&gt;
거의 첫주에 배우는 아주 기초적이고, 제일 먼저 나오는 내용이라고 할 수 있다.&lt;br /&gt;
다들 알다시피 Bayes Rule 을 쓰면 다음과 같다.&lt;/p&gt;

\[\begin{align*}
  p(A \ | \ B) &amp;amp;\propto p(A) \cdot p( B \  | \ A) \\ 
\end{align*}\]

&lt;p&gt;그런데 robotics (or 여타 estimation 문제) 에서 실제로 중요한 것은&lt;br /&gt;
위에서 $A$와 $B$의 위치가 바뀌는게 베이즈어쩌구구나~ 음~ 외웠어~ 가 아니라&lt;/p&gt;

&lt;p&gt;$A$와 $B$ 자리에 들어가는 것이,&lt;br /&gt;
&lt;u&gt;실 세계에서 어떤 물리적 의미를 지니는지&lt;/u&gt;를 이해하는 것이 중요하다.&lt;/p&gt;

&lt;h2 id=&quot;bayesisan-analysis-란-1&quot;&gt;Bayesisan Analysis 란?&lt;/h2&gt;

&lt;p&gt;Bayesian analysis 는 우리가 알고싶은 parameter (들, variables as vector)의&lt;br /&gt;
posterior probability 를 maximize 하는&lt;br /&gt;
optimal parameter (그래서 maximum a posteriori, MAP) 를 찾는 방법을 말한다.&lt;/p&gt;

&lt;p&gt;먼 말인지 차근차근 살펴보자.&lt;/p&gt;

&lt;p&gt;우리는 시스템의 어떤 상태에 대해 알고 싶다&lt;br /&gt;
(예: 로봇의 위치, 로봇의 calibration parameter 등 어떤 것이든.)&lt;/p&gt;

&lt;p&gt;이것을 모두 한데 우겨넣고 vector 로 만든 것을 $\textbf{x}$ 라고 해보자 (state).&lt;/p&gt;

&lt;p&gt;우리는 $\textbf{x}$ 를 예측하기 위한 단서들을 가지고 있다 (measurements, $\textbf{z}_{1:k}$).&lt;br /&gt;
로봇은 센서를 가지고 있기 때문에 다양한 측정을 할 수 있다.&lt;br /&gt;
(예: laser 센서로 알려진 랜드마크까지의 거리를 재기, GPS로 위치를 바로 얻기 등)&lt;br /&gt;
robotics 에서 measurement 는 문자로 적을 때는 $\textbf{z}$ 로 많이 적는 편이다.&lt;/p&gt;

&lt;p&gt;따라서 우리가 관심있는 SLAM (i.e., state estimation) 이란 문제는,&lt;br /&gt;
로봇이 가진 센서를 이용해서 얻은 다양한 측정값이 주어졌을 때 (given),&lt;br /&gt;
$\textbf{x}$ 를 추론하는 문제이다 (inference).&lt;/p&gt;

&lt;p&gt;이를 수학적으로 표현하면 $p ( \textbf{x} \ | \ \textbf{z}_{1:k} )$ 로 쓸 수 있다 &lt;sup id=&quot;fnref:eqposterior&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:eqposterior&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;따라서 SLAM (state estimation)이란 무엇인가?&lt;br /&gt;
수학적으로:&lt;br /&gt;
$\textbf{z}$가 given 일 때&lt;br /&gt;
$\textbf{x}$의 확률이 최대가 되는 $\textbf{x}$ 값을 구하는 것이다.&lt;/p&gt;

&lt;p&gt;그런데, Practically, 공학적으로는 어지간하면&lt;br /&gt;
우리는 관심있는 random variable 들 ($\textbf{x}$ 등)이&lt;br /&gt;
모두 Gaussian 이라고 가정한다.&lt;br /&gt;
Gaussian 분포는 여러가지 유용한 성질이 있기 때문이다 (후술함). $+\alpha$&lt;sup id=&quot;fnref:alpha&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:alpha&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;암튼 우리가 최대화하려는 이 확률 $p ( \textbf{x} \ | \ \textbf{z}_{1:k} )$ 은&lt;br /&gt;
measurement 가 측정이 완료되어서, 주어져야 계산할 수 있기 때문에&lt;br /&gt;
사후확률 (posterior) 이라고 부른다.&lt;/p&gt;

&lt;p&gt;그런데 이 posterior는 Bayes rule 에 의해,&lt;br /&gt;
다음과 같이 두개의 텀으로 분리가 되고, 각각을 prior 와 likelihood 라고 부른다.&lt;/p&gt;

\[\begin{align*}
  p(\textbf{x} \ | \ \textbf{z}_{1:k}) &amp;amp;\propto p(\textbf{x}) \cdot p(\textbf{z}_{1:k} \ | \ \textbf{x}) \\ 
  \text{posterior} &amp;amp;\propto \text{prior} \cdot \text{likelihood}
\end{align*}\]

&lt;p&gt;$\propto$ 로 쓴 이유: 보통 분모는 normalization 용도 (확률의 정의를 지켜주기 위해서 sum 을 1로 만들어주는 역할)로 생각되기 때문에, 통상적으로 practically 생략하는 편이 많다. 어짜피 우리가 관심이 있는 것은 최대확률값이 아니라, 최대확률이 될 때의 $\textbf{x}$값이기 때문이다 (상수의 곱에 무관하다).&lt;/p&gt;

&lt;p&gt;이 때 일반적으로 우리는 prior 와 likelihood 가 Gaussian 을 따른다고 가정한다.&lt;br /&gt;
그러면 Gaussian 의 좋은(!) 성질 덕분에 &lt;u&gt;posterior 도 Gaussian&lt;/u&gt;이 된다.&lt;/p&gt;

&lt;p&gt;여기서 그럼 다시 물어야 할 것이,&lt;br /&gt;
posterior가 Gaussian인 것이 (state estimation 관점에서) 왜 중요할까?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;일단 Gaussian은 optimal argmax 값을 얻기가 편하다 (median이 mean 과 같음) &lt;sup id=&quot;fnref:additional&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:additional&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
  &lt;li&gt;그리고 Gaussian인 경우, 자연스럽게 recursive 한 estimation 이 가능해진다. 이게 뭔말이냐면, 현재 턴의 posterior 가 다음턴의 prior 로 쓰인다고 생각해보자. 앞서, 우리는 prior 가 Gaussian이라고 가정했었다. 그럼 posterior 도 Gaussian이 되고, 다음턴의 prior 도 다시 Gaussian이 되고, 그 턴의 posterior 도 다시 Gaussian이 되고, … 무한 반복.&lt;sup id=&quot;fnref:conju&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:conju&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;
    &lt;ul&gt;
      &lt;li&gt;즉, 현재 시점에서 얻은 사전 정보를 다음턴에 자연스럽게 활용할 수 있다. 심지어 가우시안의 유용한 성질들 덕분에 그 recursive update 조차도 &lt;u&gt;closed form&lt;/u&gt; 으로 딱 떨어진다 (곧 증명해본다).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;또한, Gaussian은 mean (optimal) 과 더불어 covariance (uncertainty) 도 얻을 수 있다.
    &lt;ul&gt;
      &lt;li&gt;예를 들어 앞서 본 &lt;a href=&quot;#situ2&quot;&gt;예시상황2&lt;/a&gt; 에서 prior 와 likelihood 를 어떻게 융합해야하는지에 관한 문제가 있었다. 이 비율의 balancing 이 prior 와 likelihood 각각의 uncertainty 들에 의해 조절된다 (곧 증명해본다)!&lt;/li&gt;
      &lt;li&gt;robotics 는 거의 uncertainty 에 관한 학문이라고 할 수 있을만큼 이 uncertainty 는 다양하게 쓰이고 중요하다 &lt;sup id=&quot;fnref:cov&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:cov&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;암튼-정리해보자면&quot;&gt;암튼 정리해보자면&lt;/h2&gt;
&lt;p&gt;우리가 state estimation 에서 Bayesian filtering 이라고 부르는 것은 다음과 같은 상황을 의미한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;로봇은 자신의 상태 (state) $\textbf{x}$의 최적값을 알고 싶다 &lt;sup id=&quot;fnref:xdetail&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:xdetail&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.
    &lt;ul&gt;
      &lt;li&gt;어떤 물리적 (위치, 속도 등) 혹은 비물리적 (임의의 시스템 파라미터) 요소라도 state 가 될 수 있다. 가장 일반적으로는 pose (e.g., SE(3)).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;로봇은 센서측정데이터 (measurement) $\textbf{z}_{1:k}$ 를 가지고 있다.
    &lt;ul&gt;
      &lt;li&gt;어떤 센서라도 가능하다. 가장 일반적인 것은 Camera, LiDAR, IMU, GPS, Radar, Wheel encoder 등.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;우리는 $ p(\textbf{x} \ | \ \textbf{z}_{1:k}) $ 를 최대화 하고 싶다 &lt;sup id=&quot;fnref:eqposterior:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:eqposterior&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.
    &lt;ul&gt;
      &lt;li&gt;$==$ 우리는 posterior probability 의 mean (== 확률을 최대로 하는 $\textbf{x}$) 과 covariance 를 알고 싶다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;우리는 posterior 를 직접 최대화하는 것이 아니라, 대신 우회적으로 prior 와 likelihood 의 곱을 최대화 한다.&lt;/li&gt;
  &lt;li&gt;prior 와 likelihood의 분포로 Gaussian 을 사용한다.&lt;/li&gt;
  &lt;li&gt;covariance는 pior 와 likelihood 를 융합할 때 기여도를 고려하는 역할을 한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p id=&quot;introproof&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;posterior의 mean 과 covariance는 prior 와 likelihood 의 mean 과 covariance 를 이용해서 (closed-form으로!) 얻어진다 (이제 증명해 봄)&lt;/li&gt;
  &lt;li&gt;posterior 는 다음 턴의 prior 로써 기능 &lt;sup id=&quot;fnref:priorrole&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:priorrole&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; 한다.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;/&quot;&gt;다음 편 (Bayesian Filtering 이야기 (2편)) &lt;/a&gt;에서 더 자세히 다룰 예정.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;posterior-의-mean-covariance-구하기&quot;&gt;Posterior 의 mean, covariance 구하기&lt;/h1&gt;

&lt;p&gt;이번 포스팅에서 원래 하려고했던 걸 이제야 (…) 소개한다.&lt;/p&gt;

&lt;p&gt;앞서 우리는, 우리가 알고싶어하는 값 $\textbf{x}$ 의 posterior probability 의&lt;br /&gt;
mean 과 covariance 를 알고싶어한다고 소개했다.&lt;/p&gt;

&lt;p&gt;이것이 사실 Filter-based SLAM의 수학적 틀 &lt;sup id=&quot;fnref:filterslamadd&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:filterslamadd&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; 이기도 하다!&lt;/p&gt;

&lt;p&gt;왜 mean 이 궁금한 것인가에 대해 다시 한번 recap을 하자면,&lt;br /&gt;
Gaussian일 경우, 그 값이 posterior probability 를 최대화해주는 값이기 때문이다.&lt;br /&gt;
확률이 최대란 말은, 확률이 최대가 되는 그 때의 $\textbf{x}$ 가 말 그대로 가장 “probable” 하다는 뜻이 된다.&lt;/p&gt;

&lt;p&gt;또한, posterior probability 의 mean 과 covariance 는&lt;br /&gt;
prior 와 likelihood 의 mean 과 covariance 들을 이용해서 얻어진다고 &lt;a href=&quot;#introproof&quot;&gt; 소개 &lt;/a&gt; 했었다.&lt;/p&gt;

&lt;p&gt;여기서 먼저 두괄식으로 &lt;a href=&quot;#gaussianprop4&quot;&gt; 공식&lt;/a&gt; 만 소개하자면 다음과 같다.&lt;/p&gt;

\[\begin{align*}
\Sigma_{\text{posterior}} &amp;amp;= ( \Sigma_{\text{prior}}^{-1} + \Sigma_{\text{likelihood}}^{-1})^{-1} \\ 
\mu_{\text{posterior}} &amp;amp;= \Sigma_{\text{posterior}}(\Sigma_{\text{prior}}^{-1} \mu_{\text{prior}} + \Sigma_{\text{likelihood}}^{-1} \mu_{\text{likelihood}})
\end{align*}\]

&lt;p&gt;암튼 그래서 우리는 posterior 의 mean 과 cov를 구하기 위한 building block으로써&lt;br /&gt;
먼저 prior 와 likelihood 의 mean 과 covariance 에 대해 알아보아야 한다.&lt;br /&gt;
(위의 식에 대입해서 풀어나가면 된다)&lt;/p&gt;

&lt;p&gt;prior 에는 random variable 이 $\textbf{x}$ 하나밖에 없기 때문에,&lt;br /&gt;
간단하게 그냥 mean 과 covariance 를 말해줄 수 있다.&lt;br /&gt;
$\textbf{x}$ 의 mean 을 $\textbf{m}_0$, covariance 를 $\textbf{P}_0$ 라고 하면 된다.&lt;br /&gt;
subscriptor 로 $0$를 쓴 이유는 별 이유는 없고 그냥 시작 시점이다~ 최초의 시점이다~ 그런의미이다.&lt;/p&gt;

&lt;p&gt;그럼 likelihood 의 mean 과 covariance 는 무엇일까? 알아보자.&lt;/p&gt;

&lt;h2 id=&quot;measurement-model&quot;&gt;Measurement Model&lt;/h2&gt;
&lt;p id=&quot;zmodel&quot;&gt; &lt;/p&gt;

&lt;p&gt;likelihood는 $p(\textbf{z}_{1:k} | \ \textbf{x})$ 이거였다.&lt;/p&gt;

&lt;p&gt;여기서는 총 $k$ 개의 측정 값을 모두 이용한다는 상황인데,&lt;br /&gt;
제일 간단하게 하나의 측정 $\textbf{z}_{i}$만을 생각해보자.&lt;/p&gt;

&lt;p&gt;이 때, 각 측정이 모두 독립적이라면,&lt;br /&gt;
다음과 같이 likelihood는 개별 measurement 들의 확률의 총 곱이된다.&lt;/p&gt;
&lt;center&gt; $p(\textbf{z}_{1:k} | \ \textbf{x}) = \prod_{i = 1}^{k} p(\textbf{z}_{i} | \ \textbf{x})$ &lt;/center&gt;

&lt;p&gt;그럼 다시 질문은&lt;br /&gt;
a single measurement ($p(\textbf{z}_{i} | \ \textbf{x})$) 의 mean 과 covariance 는 무엇일까? 라는 질문이 된다.&lt;/p&gt;

&lt;p&gt;식에서 알수있듯이, $p(\textbf{z}_{i} | \ \textbf{x})$ 에서는&lt;br /&gt;
$\textbf{x}$가 알려진 값 (given) 역할이며&lt;br /&gt;
$\textbf{z}_i$ 는 알고싶은 (모르는) variable 이다.&lt;/p&gt;

&lt;p&gt;따라서 $\textbf{x}$ 가 input (given) 이고 $\textbf{z}_i$ 가 output 인 어떤 함수를 생각해볼 수 있다.&lt;/p&gt;

&lt;p&gt;보통 그 함수를 measurement model 이라고 부르고 $h$ 로 많이 적는다.&lt;br /&gt;
식으로 적으면 $\textbf{z}_i = {h}_{i}(\textbf{x})$ 가 된다.&lt;/p&gt;

&lt;p&gt;예를 들어 1차원 선상에서 움직이는 로봇을 생각해보자.&lt;br /&gt;
어떤 알려진 landmark 가 $2$ 의 위치에 있었다고 하자 ($2$의 위치에 있다는 사실도 로봇에게 알려져있다&lt;sup id=&quot;fnref:map&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:map&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;).&lt;/p&gt;

&lt;p&gt;이 때 로봇은 거리센서를 가지고 있어서 그 landmark 까지의 거리를 잴 수 있다.&lt;br /&gt;
그럼 그 거리측량값 (measurement) $\textbf{z}_i$ 는, 로봇의 위치에 의존적일 것이다.&lt;/p&gt;

&lt;p&gt;예를 들어, 여기서는 &lt;br /&gt;
$\hat{\textbf{z}_i} = h_i(\textbf{x}) = | \textbf{x} - 2 |$ 가 될 것이다.&lt;/p&gt;

&lt;p&gt;이렇듯 우리는 “실측” 하기 전부터,&lt;br /&gt;
현재 state 를 이용해서, 그 실측값이 어떤 값일지를 미리 “예측” 해볼 수 있다. &lt;br /&gt;
이것이 바로 measurement model 이다.&lt;br /&gt;
($\hat{}$ 은 예측 (estimated) 값 이라는 의미이다)&lt;/p&gt;

&lt;p&gt;즉, $h_i(\cdot)$는 $\hat{\textbf{z}_{}}$ 와 $\textbf{x}$ 를 mapping 해주는 함수 라고 말할 수 있다.&lt;/p&gt;

&lt;p&gt;그런데 이 때, 일반적으로,&lt;br /&gt;
공학적으로 계산상의 이점을 얻기 위해서는 (Linear algebra 의 다양한 툴들을 활용하기 위해서는),&lt;br /&gt;
그 mapping 함수가 linear 할 필요가 있다 (== matrix 곱으로 표현될 수 있어야 한다).&lt;/p&gt;

&lt;p&gt;위의 예시에서는 measurement model 이 $| \textbf{x} - 2 |$ 와 같이 간단한 함수였지만&lt;br /&gt;
다양한 센서를 사용하다보면 exponential, cosine, 등이 다 섞이는 nonlinear 한 함수가 될 때도 있다.&lt;/p&gt;

&lt;p&gt;이 경우, linear 하지 않기 때문에 (matrix 곱으로 나타낼 수 없기 때문에),&lt;br /&gt;
우리는 (일반적으로) nonlinear (라고 가정하는) 인 measurement model 을 미분해서 matrix 로 만들어준다.&lt;/p&gt;

&lt;p&gt;이것이 흔히들 “자코비안” 이라고 부르는 것이며,&lt;br /&gt;
자세한 수학적 근거는 대학미적분학 정도를 들었다면 다 알것이므로 생략한다.&lt;br /&gt;
혹은 여기를 참고: &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_calculus#Vector-by-vector&quot;&gt; vector-by-vector 미분 (위키) &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;$h(\cdot)$ 을 미분해서 선형화 시킨 것 (i.e., $h(\cdot)$의 자코비안) 을&lt;br /&gt;
보통 $H$ 와 같이 대문자로 많이 적는다.&lt;/p&gt;

&lt;p&gt;이 경우 linear 하므로, $h(\textbf{x})$ 에서처럼 괄호 안에 argument 방식으로 굳이 적어줄 필요가 없어진다.&lt;br /&gt;
즉, $\hat{\textbf{z}_{}} = H_i\textbf{x}$ 와 같이 간단히 매트릭스 곱으로 적을 수 있다.&lt;/p&gt;

&lt;p&gt;근데 이것은 실측 값이 아니라 예측값 (말 그대로 “모델”)임을 잊지 말아야 한다!&lt;/p&gt;

&lt;p&gt;실측값은 센서 오차 등으로 $H_i\textbf{x}$ 와는 조금 다를 수 있다.&lt;br /&gt;
$H_i\textbf{x}$ 에 여기에 또 Gaussian noise 가 더해진 것이 실측값 $\textbf{z}$ 라고 모델링 해볼 수 있다. &lt;br /&gt;
이 noise 는 말 그대로 noise 이기 때문에 mean 이 $0$ 이고 covariance 가 대충 작은 값 얼마라고 할 수 있다 &lt;sup id=&quot;fnref:userparam&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:userparam&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;문제를 단순화 하기 위해서&lt;br /&gt;
$\textbf{z}_i$ 가 1-dim vector 이고&lt;br /&gt;
따라서 a single measurement 의 covariance 가 단순히 어떤 스칼라 값 $\sigma^{2}$ 라고 하자 (이후 포스팅에서 일반화해봄).&lt;/p&gt;

&lt;!-- 위의 말들을 다음과 같이 쓸 수 있다. 
&lt;p id=&quot;zxs&quot;&gt; &lt;/p&gt;
$$
\begin{align*}
  \textbf{z}_i &amp;= \textbf{H}_{i}{\textbf{x}_{i}} + \textbf{s}_i ,\\
\end{align*}
$$

여기서 $\textbf{x}_i$는 Gaussian이고 $\bf{\sigma}$ 도 Gaussian이기 때문에 $\textbf{z}_i$ 도 Gaussian이다.  
(여기서 $\textbf{s}_i = \sigma^2$ 인데 $\sigma$ 는 scalar value 자체를 의미하고 $\textbf{s}_i$ 는 (Gaussian) random variable 을 의미하기 때문에 약간 다른 알파벳으로 적어보았다) --&gt;

&lt;p&gt;그럼 noise 의 mean 은 0이었기 때문에&lt;br /&gt;
실제 measurement $\textbf{z}_{i}$ 는&lt;br /&gt;
mean이 $H\textbf{x}$ 이고, variance 가 $\sigma^{2}$ 인 Gaussian 을 따른다고 말할 수 있다.&lt;/p&gt;

&lt;p&gt;이를 $p(\textbf{z}_{i} | \ \textbf{x}) = \mathcal{N}(H_i\textbf{x},\sigma^{2}) $ 라고 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;거의 다왔다.&lt;/p&gt;

&lt;p&gt;이제 prior 와 likelihood 의 mean 과 covariance 들이 모두 준비되었다.&lt;br /&gt;
이거를 어떻게 잘 융합하면 posterior 의 mean 과 covariance 가 나오는지 이제 유도해보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;gaussian-의-성질&quot;&gt;Gaussian 의 성질&lt;/h2&gt;

&lt;p&gt;Gaussian 분포는 이런 성질(장점) 들을 가지고 있다. (&lt;a href=&quot;https://www.cs.ubc.ca/~schmidtm/Courses/540-W19/L14.pdf&quot; target=&quot;_blank&quot;&gt; 출처 &lt;/a&gt;)&lt;/p&gt;
&lt;p style=&quot;margin-top:-20px&quot;&gt; &lt;/p&gt;
&lt;figure id=&quot;gaussianprop&quot;&gt;
  &lt;img src=&quot;/figs/2021-03-09-bayesfiltering-1/prop.png&quot; alt=&quot;img1&quot; style=&quot;width:100%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; &lt;a href=&quot;#gaussianprop&quot; class=&quot;img1&quot;&gt; Figure: Closedness of Multivariate Gaussian &lt;/a&gt; &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;여기서 네번째 성질을 이용하려고 한다.&lt;/p&gt;

&lt;p&gt;다음과 같다.&lt;/p&gt;

&lt;!-- &lt;p style=&quot;margin-top:-20px&quot;&gt; &lt;/p&gt; --&gt;
&lt;figure id=&quot;gaussianprop4&quot;&gt;
  &lt;img src=&quot;/figs/2021-03-09-bayesfiltering-1/prop4.png&quot; alt=&quot;img1&quot; style=&quot;width:100%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; &lt;a href=&quot;#gaussianprop4&quot; class=&quot;img2&quot;&gt; Figure: Product of Gaussian Densities &lt;/a&gt; &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;저기 중앙의 수식에 적힌대로 따라 하면 된다.&lt;br /&gt;
차근 차근 해보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;posterior-의-mean-과-covariance-유도&quot;&gt;Posterior 의 mean 과 covariance 유도&lt;/h2&gt;

&lt;p&gt;이해하기 쉽도록 &lt;a href=&quot;#gaussianprop4&quot;&gt; 위의 그림에 있는 수식 &lt;/a&gt; 과 같은 notation 을 사용하자.&lt;/p&gt;

&lt;p&gt;번호 $1$ 의 위치에 prior, 번호 $2$ 번의 위치에 likelihood 를 넣어보자.&lt;br /&gt;
그러면 일단 prior 에 대해서는 쉽다&lt;sup id=&quot;fnref:reason&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:reason&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

\[\begin{align*}
  \Sigma_{1} &amp;amp;= \textbf{P}_0 ,\\
  \mu_{1} &amp;amp;= \textbf{m}_0 
\end{align*}\]

&lt;p&gt;그럼, likelihood에 대한, $\Sigma_{2}$ 와 $\mu_{2}$는 무엇인가?&lt;/p&gt;

&lt;p&gt;일단, likelihood 가 $\prod_{i = 1}^{k} p(\textbf{z}_{i} | \ \textbf{x})$ 처럼 여러개의 항들의 곱이었기 때문에 
위의 공식을 이용하려면 하나의 항으로 만들어 주어야 한다.&lt;/p&gt;

&lt;p&gt;이를 위해서 $\textbf{z}_i$ 들을 쌓아서 하나의 긴 vector $\textbf{z}$ 를 만들어 주자.&lt;/p&gt;

&lt;p&gt;그러면 $\textbf{z}$는 mean 이 $\textbf{H}\textbf{x}$ 이고 covariance 가 diag($\sigma^2$) $=\sigma^2 \textbf{I}$인&lt;sup id=&quot;fnref:diagreason&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:diagreason&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt; Gaussian을 따르게 된다.&lt;/p&gt;

&lt;p&gt;이 때 $\textbf{H}$ 는 $\textbf{H}_i$ 를 세로로 쌓은 (stack) matrix 이다.&lt;br /&gt;
$\textbf{I}$는 identity matrix 이며 shape 은 $\textbf{H}$의 세로(row)길이 by 세로길이 가 된다.&lt;/p&gt;

&lt;p&gt;이를 $p(\textbf{z} | \textbf{x}) = \mathcal{N}(H\textbf{x},\sigma^{2}\textbf{I}) $ 라고 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;그럼 이제 $\Sigma_{2}$ 와 $\mu_{2}$ 로써,  $\textbf{H}\textbf{x}$ 와 $\sigma^{2}\textbf{I}$를 쓰면 되나?&lt;/p&gt;

&lt;p&gt;안된다!&lt;/p&gt;

&lt;p&gt;$\textbf{H}\textbf{x}$ 와 $\sigma^{2}$ 는 “$\textbf{z}$” 의 mean/cov 이기 때문이다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#gaussianprop4&quot;&gt; 위의 그림에 있는 수식 &lt;/a&gt; 을 이용하려면&lt;br /&gt;
$\Sigma_{2}$ 와 $\mu_{2}$ 는 “$\textbf{x}$” 의 mean/cov 여야 한다.&lt;br /&gt;
(prior 도 $\textbf{x}$ 에 대한 것이고, 우리가 관심있는 posterior 도 $\textbf{x}$에 대한 것이기 때문이다.)&lt;/p&gt;

&lt;p&gt;우리는 $\textbf{z}$ 의 mean/cov 를 알고 있는데,&lt;br /&gt;
이를 이용해서 $\textbf{x}$ 의 mean/cov 를 알 수 있다.&lt;/p&gt;

&lt;p&gt;Gaussian 의 exponential term 을 요리조리 재구성하면 그것을 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;먼저 Gaussian 분포의 실제 수식 생김새를 보면, 이렇게 생겼다.&lt;/p&gt;

&lt;figure id=&quot;gaussian1&quot;&gt;
  &lt;img src=&quot;/figs/2021-03-09-bayesfiltering-1/gaussian1.png&quot; style=&quot;width:90%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; &lt;a href=&quot;#gaussian1&quot;&gt; Figure: Gaussian Density &lt;/a&gt; &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;여기서 exponential 안쪽의 각 항에 위치하는 애들이 mean 과 covariance 의 의미를 가진다.&lt;/p&gt;

&lt;figure id=&quot;gaussian2&quot;&gt;
  &lt;img src=&quot;/figs/2021-03-09-bayesfiltering-1/gaussian2.png&quot; style=&quot;width:90%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; &lt;a href=&quot;#gaussian2&quot;&gt; Figure: Gaussian Density (visually explained) &lt;/a&gt; &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;즉, 회색 dotted box 안의 값만 중요하다.&lt;br /&gt;
(아까부터 계속 말했듯 exp 바깥의 term 들은 상수로써 확률의 합을 1로 만들어 주기 위함이고, 확률을 최대화 하는 argmin (i.e., mean) 에는 영향을 주지 않는다)&lt;/p&gt;

&lt;p&gt;까만 박스 위치에 놓이는 것이 우리가 관심있는 random variable 이다.&lt;br /&gt;
파란 박스 위치에 놓이는 것이 그 random variable 의 mean 이다.&lt;br /&gt;
빨간 박스 위치에 놓이는 것이 그 random variable 의 covariance 이다.&lt;/p&gt;

&lt;p&gt;그럼 이 때, 지금 우리의 경우에서는: 즉, $p(\textbf{z} | \textbf{x})$ = $\mathcal{N}(H\textbf{x},\sigma^{2}\textbf{I}) $ 일 때,&lt;br /&gt;
$p(\textbf{z} | \textbf{x})$ 의 exponential term 은 이렇게 생겼을 것이다 (-1/2 도 편의상 생략).&lt;/p&gt;

\[\begin{align*}
( \textbf{z} - \textbf{H}\textbf{x} )^{T} (\sigma^2 \textbf{I})^{-1} ( \textbf{z} - \textbf{H}\textbf{x} )
\end{align*}\]

&lt;p&gt;좀 전에 말했듯 까만 박스 위치에 지금은 $\textbf{z}$ 가 놓여있다.&lt;br /&gt;
근데 우리의 관심사는 $\textbf{x}$ 의 mean과 covariance 이다.&lt;/p&gt;

&lt;p&gt;따라서 위의 수식의 모양새를 수정해서 (하지만 수식 자체가 바뀌면 안된다 (동치여야 한다))&lt;/p&gt;

&lt;p&gt;지금 $\textbf{z}$ 가 있는 위치에 $\textbf{x}$가 놓이도록 수정해보자.&lt;/p&gt;

&lt;p&gt;$(\textbf{H}\textbf{H}^{T})(\textbf{H}\textbf{H}^{T})^{-1}$ 가 $\textbf{I}$ 와 같다는 사실 (자명하다) 을 이용해보자&lt;/p&gt;

&lt;!-- (증명)[^corproof].  --&gt;

&lt;!-- [^corproof]: 증명: $\textbf{H}(\textbf{H}^{T}\textbf{H})^{-1}\textbf{H}^{T}$ 의 왼쪽에 $\textbf{H}^{T}$ 를 곱해보자. 그러면 $\textbf{H}^{T}\textbf{H}(\textbf{H}^{T}\textbf{H})^{-1}\textbf{H}^{T} = (\textbf{H}^{T}\textbf{H})(\textbf{H}^{T}\textbf{H})^{-1}\textbf{H}^{T} = \textbf{H}^{T}$ 가 된다. 한 편, $\textbf{I}$ 의 왼쪽에 $\textbf{H}^{T}$ 를 곱한 결과도 $\textbf{H}^{T}$ 가 된다. 따라서 $\textbf{H}(\textbf{H}^{T}\textbf{H})^{-1}\textbf{H}^{T}$ 는 $\textbf{I}$ 이다. --&gt;

\[\begin{align*}
  &amp;amp; (  \textbf{z} - \textbf{H}\textbf{x} )^{T} (\sigma^2 \textbf{I})^{-1} ( \textbf{z} - \textbf{H}\textbf{x} ) \\ 
= \ &amp;amp;  (  \textbf{I}\textbf{z} - \textbf{H}\textbf{x} )^{T} (\sigma^2 \textbf{I})^{-1} ( \textbf{I}\textbf{z} - \textbf{H}\textbf{x} ) \\ 
= \ &amp;amp;  \left(  (\textbf{H}\textbf{H}^{T})(\textbf{H}\textbf{H}^{T})^{-1}\textbf{z} - \textbf{H}\textbf{x} \right)^{T} (\sigma^2 \textbf{I})^{-1} \left( (\textbf{H}\textbf{H}^{T})(\textbf{H}\textbf{H}^{T})^{-1}\textbf{z} - \textbf{H}\textbf{x} \right) \\ 
= \ &amp;amp;  \left( \textbf{H}(\textbf{H}^{T})(\textbf{H}\textbf{H}^{T})^{-1}\textbf{z} - \textbf{H}\textbf{x} \right)^{T} (\sigma^2 \textbf{I})^{-1} \left( \textbf{H}(\textbf{H}^{T})(\textbf{H}\textbf{H}^{T})^{-1}\textbf{z} - \textbf{H}\textbf{x} \right) \\ 
= \ &amp;amp;  \left( \textbf{H} \left( \textbf{H}^{T}(\textbf{H}\textbf{H}^{T})^{-1}\textbf{z} - \textbf{x} \right) \right)^{T} (\sigma^2 \textbf{I})^{-1} \left( \textbf{H} \left( \textbf{H}^{T}(\textbf{H}\textbf{H}^{T})^{-1}\textbf{z} - \textbf{x} \right) \right)\\ 
= \ &amp;amp;  \left(\left( \textbf{H}^{T}(\textbf{H}\textbf{H}^{T})^{-1}\textbf{z} - \textbf{x} \right)^{T} \textbf{H}^{T} \right)(\sigma^2 \textbf{I})^{-1} \left( \textbf{H} \left( \textbf{H}^{T}(\textbf{H}\textbf{H}^{T})^{-1}\textbf{z} - \textbf{x} \right) \right) \\ 
= \ &amp;amp;  \left( \textbf{H}^{T}(\textbf{H}\textbf{H}^{T})^{-1}\textbf{z} - \textbf{x} \right)^{T} \left( \textbf{H}^{T} (\sigma^2 \textbf{I})^{-1}  \textbf{H} \right) \left( \textbf{H}^{T}(\textbf{H}\textbf{H}^{T})^{-1}\textbf{z} - \textbf{x} \right) \\ 
= \ &amp;amp; \left( \textbf{x} - \textbf{H}^{T}(\textbf{H}\textbf{H}^{T})^{-1}\textbf{z} \right)^{T} \left( \frac{\textbf{H}^{T}\textbf{H}}{\sigma^2} \right) \left( \textbf{x} - \textbf{H}^{T}(\textbf{H}\textbf{H}^{T})^{-1}\textbf{z} \right) \\ 
= \ &amp;amp; \left( \textbf{x} - \mu_{x} \right)^{T} \left( \Sigma_{x}^{-1} \right) \left( \textbf{x} - \mu_{x} \right)
\end{align*}\]

&lt;p&gt;&lt;span style=&quot;color:gray&quot;&gt; 해설&lt;sup id=&quot;fnref:eqexplain&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:eqexplain&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;따라서 우리는 이로부터,&lt;br /&gt;
$\textbf{H}\textbf{x}$ 와 $\sigma^{2}$ 가 각각 $\textbf{z}$ 의 mean 과 covariance 일 때, &lt;br /&gt;
$\textbf{x}$ 의 mean은 $\textbf{H}^{T}(\textbf{H}\textbf{H}^{T})^{-1}\textbf{z}$ 이고, covariance 는 $\left( \frac{\textbf{H}^{T}\textbf{H}}{\sigma^2} \right) ^{-1}$ 임을 알 수 있다.&lt;br /&gt;
(NOTE: 여기서 $\textbf{z}$ 는 로봇이 센서를 이용해서 실측한 값이다)&lt;/p&gt;

&lt;p&gt;따라서 &lt;a href=&quot;#gaussianprop4&quot;&gt; 위의 그림에 있는 수식 &lt;/a&gt; 의 $\Sigma_{2}$ 와 $\mu_{2}$ 는 다음과 같다.&lt;/p&gt;

\[\begin{align*}
  \Sigma_{2} &amp;amp;= \left( \frac{1}{\sigma^2}\textbf{H}^{T}\textbf{H} \right) ^{-1} ,\\
  \mu_{2} &amp;amp;= \textbf{H}^{T}(\textbf{H}\textbf{H}^{T})^{-1}\textbf{z} 
\end{align*}\]

&lt;p&gt;&lt;br /&gt;
따라서 Gaussian product 의 공식대로 하면&lt;br /&gt;
posterior 의 mean 과 covariannce 는 다음과 같다.&lt;/p&gt;

&lt;p&gt;먼저 posterior 의 covariance $\textbf{P}_{\text{pos}}$는 다음과 같다. (슬라이드 그림에서 $\Sigma$ 로 되어있어서 지금까지 $\Sigma$ 로 이야기했는데, covariance 를 일컫는 notation 을 prior 에서 그랬듯이 $\textbf{P}$ 로 다시 통일하도록 하자)&lt;/p&gt;

\[\begin{align*}
  \textbf{P}_{\text{pos}} &amp;amp;= \left( \Sigma_1{}^{-1} + \Sigma_2{}^{-1} \right)^{-1} \\ 
             &amp;amp;= \left(\textbf{P}_0^{-1} + \frac{1}{\sigma^2}\textbf{H}^{T}\textbf{H} \right)^{-1}
\end{align*}\]

&lt;p&gt;그리고 posterior 의 mean $\textbf{m}_{\text{pos}}$ 는 다음과 같다.&lt;/p&gt;

\[\begin{align*}
  \textbf{m}_{\text{pos}} &amp;amp;= \textbf{P}_{\text{pos}} \left( \Sigma_1{}^{-1}\mu_{1} + \Sigma_2{}^{-1}\mu_{2} \right) \\ 
    &amp;amp;= \textbf{P}_{\text{pos}} \left( \textbf{P}_0^{-1} \textbf{m}_0 + \left( \frac{1}{\sigma^2}\textbf{H}^{T}\textbf{H} \right) \left( \textbf{H}^{T}(\textbf{H}\textbf{H}^{T})^{-1}\textbf{z} \right) \right) \\ 
    &amp;amp;= \textbf{P}_{\text{pos}} \left( \textbf{P}_0^{-1} \textbf{m}_0 + \left( \frac{1}{\sigma^2}\textbf{H}^{T} \right) \left( \textbf{H} \textbf{H}^{T} \right) (\textbf{H}\textbf{H}^{T})^{-1}\textbf{z} \right) \\ 
    &amp;amp;= \textbf{P}_{\text{pos}} \left( \textbf{P}_0^{-1} \textbf{m}_0 + \frac{1}{\sigma^2}\textbf{H}^{T}\textbf{z} \right) \\ 
    &amp;amp;= \left(\textbf{P}_0^{-1} + \frac{1}{\sigma^2}\textbf{H}^{T}\textbf{H} \right)^{-1} \left( \textbf{P}_0^{-1} \textbf{m}_0 + \frac{1}{\sigma^2}\textbf{H}^{T}\textbf{z} \right)
\end{align*}\]

&lt;p id=&quot;proofdone&quot;&gt; &lt;/p&gt;
&lt;p&gt;증명 끝!&lt;/p&gt;

&lt;p&gt;이를 수식으로 다시 정리하면:&lt;/p&gt;

\[\begin{align*}
  p(\textbf{x} | \textbf{z}) = \mathcal{N} \left(\left(\textbf{P}_0^{-1} + \frac{1}{\sigma^2}\textbf{H}^{T}\textbf{H} \right)^{-1} \left( \textbf{P}_0^{-1} \textbf{m}_0 + \frac{1}{\sigma^2}\textbf{H}^{T}\textbf{z} \right), \ \ \left(\textbf{P}_0^{-1} + \frac{1}{\sigma^2}\textbf{H}^{T}\textbf{H} \right)^{-1} \right) 
\end{align*}\]

&lt;p&gt;가 된다!&lt;/p&gt;

&lt;p&gt;결과만 보면 좀 복잡해보이지만, &lt;a href=&quot;#gaussianprop4&quot;&gt; 이 식이 유도되어온 맥락 &lt;/a&gt;  을 기억한다면&lt;br /&gt;
그리 어렵게 느껴지진 않을 것 같다!&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;요약&quot;&gt;요약&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Bayesian Filtering 의 뿌리에 대해 알아보았습니다.
    &lt;ul&gt;
      &lt;li&gt;posterior 는 measurement 가 주어졌을 때, state 의 확률을 의미한다. 그리고 이것을 최대화하는 과정을 MAP (maximum a posteriori) 라고 부른다.&lt;/li&gt;
      &lt;li&gt;실제로는 posterior 의 mean 과 covariance 를 직접 구하지 않고, prior 와 likelihood 의 mean 과 covariance 를 조합해서 구한다.&lt;/li&gt;
      &lt;li&gt;prior, likelihood, posterior 가 모두 Gaussian이라고 가정할 경우, Gaussian product 에 의해, posterior 의 mean 과 covariance 가 closed form 으로 딱 떨어진다.&lt;/li&gt;
      &lt;li&gt;그리고 이렇게 얻어진 posterior는 다음 턴의 (새로운 measurement 가 하나 더 들어왔다고 생각해보자) prior 로 다시 쓰인다. 즉, recursive estimation 이 가능한 수학적 엔진을 오늘 만들어보았다! (recursive estimation 에 대해서는 다음편에서 더 자세히 이야기함)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Bayesian Filtering 공부에 대해서, 이 책&lt;sup id=&quot;fnref:bayesfilteringbook&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bayesfilteringbook&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;을 추천합니다&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;예고&quot;&gt;예고&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;다음 편에서 오늘 유도한 걸 그대로 이용해서 칼만필터를 유도해보겠습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- --- 
## 생각해보기 --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;주석&quot;&gt;주석&lt;/h3&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:likelihood&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://dtransposed.github.io/blog/Bayesian-Linear-Regression.html&quot; target=&quot;_blank&quot;&gt; 이 블로그 &lt;/a&gt;의 gif 예제를 보면 이해하기 쉽다. &lt;a href=&quot;https://dtransposed.github.io/assets/9/batch_1/Data_Space.gif&quot; target=&quot;_blank&quot;&gt; data point들이 sequentially 들어올 때 &lt;/a&gt;, 그때그때마다의 measurement에만 최대로 fit하는 예측을 내리게 된다면 &lt;a href=&quot;https://dtransposed.github.io/assets/9/batch_1/Likelihood.gif&quot; target=&quot;_blank&quot;&gt; 이렇게 들쭉날쭉한 결과를 얻게 될 것이다. &lt;/a&gt; &lt;a href=&quot;#fnref:likelihood&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:eqposterior&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;중간의 | 는 “given” 으로 읽는다. 그리고 $\textbf{z}$의 subscript 인 $1:k$는 총 k개의 measurement 가 있다고 가정하자, 라는 의미인데 간단히 생략되기도 한다 – 문맥에 따라 때로는 time $= 1$ 부터 $t$ 까지의 측정값, 처럼 time 에 걸쳐 얻은 measurement 로 이해되기도 한다. &lt;a href=&quot;#fnref:eqposterior&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:eqposterior:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:alpha&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Factor graph-based SLAM 에서는 Gaussian noise 를 가정할 경우 Probability의 사뭇 추상적이었던 수식이 Least-square optimization 으로 과 동치가 되고, 문제를 iterative optimization으로 풀 수 있다는 장점이 있기도 하다. &lt;a href=&quot;https://link.springer.com/article/10.1023/A:1008854305733&quot;&gt; Lu and Milios 의 1997년 논문&lt;/a&gt; 이 optimization-based SLAM의 시초로 평가받는 듯하다. (하지만 이에 대해서 입문용으로는 Square Root SAM 논문이 제일 정석적이고 좋은 듯) &lt;a href=&quot;#fnref:alpha&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:additional&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;optimization-based SLAM 관점에서도 좋다. Gaussian 의 Exp 안의 term 이 Mahalanobis L2 norm form 이고 이것을 최소화 하는 것은 확률을 최대화하는 것과 동치가 되기 때문에, 자연스럽게 원래 확률의 언어로 표현되었던 것이 least square optimization 문제로 변환이 된다. &lt;a href=&quot;/&quot;&gt; [Factor graph-based SLAM]» SLAM back-end 이야기 (2편) &lt;/a&gt; 에서도 이 이야기를 좀 더 해보겠다. &lt;a href=&quot;#fnref:additional&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:conju&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;우리는 지금 posterior 가 prior 와 같은 분포 패밀리가 되는 것을 원하고 있었다. Gaussian = Gaussian $\times$ Gaussian 인 조합 외에, conjugate distribution이라고 검색해보면 몇개 더 다양한 분포의 조합이 가능함을 알 수 있다. 하지만 practically, Gaussian 말고 별로 다른 조합은 적어도 SLAM에서는 잘 써본 적이 없다 (== Gaussian에 대해서만 일단 잘 알면 된다). &lt;a href=&quot;#fnref:conju&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:cov&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;또한, covariance 는 uncertainty 의 의미로써 그 자체로도 solely 다양한 application에 활용 될 수 있다. 예를 들어 loop closing 을 수행하기 위해 어느 정도 반경 내의 후보 node 들만 검색하는 등 search space 를 줄여줄 수 있다. &lt;a href=&quot;#fnref:cov&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:xdetail&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;속도, 주변 맵 포인트들의 위치, 자신의 내부 상태 등 다양한 어떤 값들이든 될 수 있다. 예를 들자면, 가장 일반적인 형태로는 robot 자신의 set of poses (i.e., trajectory over time) 과 주변 환경 특징점 (landmark) 들의 3차원 위치 x,y,z 들이 될 수 있다 – &lt;a href=&quot;/blog/2021/03/04/slambackend-1.html#Axb&quot;&gt; 이 그림 &lt;/a&gt; 을 보면 이해하기 쉬울 것이다. &lt;a href=&quot;#fnref:xdetail&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:priorrole&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;robotics에서는 data 가 한번에 주어지지 않고 time 에 따라 sequentially 생성되는 경우가 일반적이다. 따라서 time $=t$ 의 posterior 를 time $=t+1$ 에서의 prior 로 사용하고, 이 때 새로 들어온 measurement 들과 융합하는 사전 정보로써 쓰이게 된다. &lt;br /&gt; – 이런 특성 때문에 Bayesian analaysis (+Gaussian asussmption) 은 recursive estimator 라고 불리기도 한다. 혹은 기존의 믿음 prior 를 likelihood 를 이용해서 수정하는 느낌으로써, 현재 상태를 (더 나은 값으로) 조정하기 때문에 filtering 이라고도 불린다. &lt;a href=&quot;#fnref:priorrole&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:filterslamadd&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;mean 과 covariance format의 경우 update 시 장단점이 있어서, computationally 이점을 얻기 위해 information matrix 를 update 하는 방식으로 푸는 시도들이 그 이후에 발전되어 왔다. Eustice, Ryan M., Hanumant Singh, and John J. Leonard. “Exactly sparse delayed-state filters for view-based SLAM.” IEEE Transactions on Robotics (2006) 등의 논문을 참고. 이에 대해서는 &lt;a href=&quot;http://jinyongjeong.github.io/2017/02/19/lec07_EIF/&quot;&gt; 진용님의 블로그 &lt;/a&gt; 에 잘 정리되어 있으니 참고. &lt;a href=&quot;#fnref:filterslamadd&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:map&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;이것이 SLAM과 더불어 mobile robotics 에서 많이 푸는 문제인 “locazliation in a given map”. &lt;a href=&quot;#fnref:map&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:userparam&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;이 noise 의 covariance가 filtering 이든 optimization 이든 예측에 영향을 주는 user-parameter 가 된다. 따라서 실제로 연구나 실무에서 이 값을 튜닝을 통해 예측 결과의 품질을 사람이 봐가면서 조정하는 식으로 이루어지고 있다. 딥러닝이 여러 loss 들을 조합할 때의 비율 (weight) 을 노가다로 찾는다면, SLAM에서는 각 factor (measurement)들의 covariance 들이 hyper parameter (이지만 노가다로 구해지는) 가 된다 (– 노가다가 귀찮기 때문에 보통 적당한 fixed value 를 사용하긴 하지만). 따라서 어떤 논문에서 자기들이 최고의 결과를 이 데이터셋에서 보여주었다 하더라도 그 최적값을 얻을 수 있었던 파라미터를 노가다를 뛰어서 찾아서 얻은 결과임을 생각해야 한다 – 다른 데이터셋에서도 일관적으로 높은 성능을 보일 것이라는 보장을 가질 수는 없다. 따라서 엄밀하게 보자면 이런 부분에서 자동화가 되고 학습을 통해 최적 noise parameter 들을 찾을 수 있다면 이런게 spatial AI로 갈 수 있는 부분 중 하나일 것이라 생각한다. &lt;a href=&quot;#fnref:userparam&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:reason&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;앞서 말했듯이, prior 에는 random variable 이 $\textbf{x}$ 하나밖에 없기 때문에, 간단하게 그냥 mean 과 covariance 를 말해줄 수 있다. $\textbf{x}$ 의 mean 을 $\textbf{m}_0$, covariance 를 $\textbf{P}_0$ 라고 하면 된다. subscriptor 로 $0$를 쓴 이유는 별 이유는 없고 그냥 시작 시점이다~ 최초의 시점이다~ 그런의미이다. &lt;a href=&quot;#fnref:reason&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:diagreason&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;white noise 는 has a zero correlation with all other values in the series 이기 때문에 diagonal term만 존재한다. &lt;a href=&quot;#fnref:diagreason&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:eqexplain&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;4번째에서 5번째 $=$ 로 넘어갈 때 $(AB)^{T} = B^{T}A^{T}$ 가 쓰였다. &lt;a href=&quot;#fnref:eqexplain&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bayesfilteringbook&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Särkkä, Simo. Bayesian filtering and smoothing. No. 3. Cambridge University Press, 2013. &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.461.4042&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt; PDF link &lt;a&gt;&lt;/a&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:bayesfilteringbook&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>SLAM back-end [2++편] — Householder QR 분해 구현해보기</title>
   <link href="http://localhost:4000/blog/2021/03/04/slambackend-3.html"/>
   <updated>2021-03-04T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/03/04/slambackend-3</id>
   <content type="html">&lt;p&gt;TBA&lt;/p&gt;

&lt;!-- ---
### 주석 --&gt;
</content>
 </entry>
 
 <entry>
   <title>SLAM back-end [2편] — Ax=b 대신 Ry=d 를 풀자</title>
   <link href="http://localhost:4000/blog/2021/03/04/slambackend-2.html"/>
   <updated>2021-03-04T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/03/04/slambackend-2</id>
   <content type="html">&lt;p&gt;TBA&lt;/p&gt;

&lt;!-- ---
### 주석 --&gt;
</content>
 </entry>
 
 <entry>
   <title>SLAM back-end [1편] — SLAM은 Ax=b 를 푸는 것이다</title>
   <link href="http://localhost:4000/blog/2021/03/04/slambackend-1.html"/>
   <updated>2021-03-04T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/03/04/slambackend-1</id>
   <content type="html">&lt;!-- see using latex here: https://mkkim85.github.io/blog-apply-mathjax-to-jekyll-and-github-pages/ --&gt;
&lt;!-- --- --&gt;

&lt;h1 id=&quot;개요&quot;&gt;개요&lt;/h1&gt;
&lt;p&gt;SLAM은 세상의 모든것 &lt;em&gt;[1. 나 (robot)와 2.너 (world)]&lt;/em&gt; 의&lt;br /&gt;
state (e.g., position, orientation, velocity) 를 예측하는 학문이다.&lt;br /&gt;
그래서 state estimation 이라고 불리기도 한다.&lt;/p&gt;

&lt;p&gt;이런 최적 state 를 예측하는 solver 에 관한 연구를&lt;br /&gt;
SLAM back-end 라고 편의상 부르기도 한다.&lt;/p&gt;

&lt;p&gt;그런데 어떻게 예측할까? 어떻게 최적해를 얻을까?&lt;/p&gt;

&lt;p&gt;(SLAM back-end의 마일스톤 논문인) Square Root SAM 논문&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 에서 말했듯,&lt;br /&gt;
SLAM은 수학적으로 $Ax=b$ 를 푸는 문제이다.&lt;/p&gt;

&lt;p&gt;그래서 SLAM back-end 라고 하면 주로&lt;/p&gt;

&lt;p&gt;어떻게 더 $Ax=b$를&lt;br /&gt;
빠르고 (fast), 정확하고 (accurate), 안정적으로 (numerically stable) 풀 수 있는지에 대한 연구라고 생각하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:gray&quot;&gt; (다음 편들에서 그런 알고리즘들에 대해서 자세히 알아보도록 하고) &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;일단 왜 SLAM이 $Ax=b$ 를 푸는 문제인지 알아보자.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;slam-이란&quot;&gt;SLAM 이란&lt;/h1&gt;

&lt;p&gt;어느 마을에 건설로봇 (SCV)이 있었다.&lt;/p&gt;

&lt;p&gt;얘는 자기가 어디서 온 지 모른다.&lt;/p&gt;

&lt;p&gt;그래서 편의상 자기가 현재 Origin (e.g., [0, 0]) 에 있다고 생각한다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/figs/2021-03-04-slambackend-1/robot1.png&quot; alt=&quot;img1&quot; style=&quot;width:100%&quot; /&gt;
  &lt;figcaption&gt; &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;SCV는 길을 가고 있었다. 로봇인 이상 움직이지 않을 수 없으므로.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/figs/2021-03-04-slambackend-1/robot2.png&quot; alt=&quot;img2&quot; style=&quot;width:100%&quot; /&gt;
  &lt;figcaption&gt; &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;얘는 사이사이마다 얼마만큼 이동한지 (relative motion) 를 측정하는 능력 (&lt;em&gt;odometry&lt;/em&gt;) 이 있어서 (&lt;span style=&quot;color:blue&quot;&gt;파랑 화살표&lt;/span&gt;),&lt;/p&gt;

&lt;p&gt;자연스럽게 $t=2, 3, 4, 5$에서의 위치도 계산할 수 있었다 (localization).&lt;/p&gt;

&lt;p&gt;근데 그 능력 (odometry) 은 엄청 정확하지는 않아서 (sensor noise), &lt;br /&gt;
갈수록 자기가 어디에 있는지 덜 신뢰하게 될 수밖에 없었다 (uncertainty was propagated).&lt;/p&gt;

&lt;p&gt;그래서 얘는 주변의 포토캐논들 (landmark) 을 봐가면서 이동하기 시작했다.&lt;br /&gt;
SCV는 사실 laser 를 장착하고 있어서 포토캐논까지의 거리를 직접적으로 잴 수 있었다 (&lt;span style=&quot;color:orange&quot;&gt;주황 화살표&lt;/span&gt;).&lt;/p&gt;

&lt;figure id=&quot;BN&quot;&gt;
  &lt;img src=&quot;/figs/2021-03-04-slambackend-1/robot3.png&quot; alt=&quot;img3&quot; style=&quot;width:105%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; &lt;a href=&quot;#BN&quot; class=&quot;img3&quot;&gt; Figure: Bayes Net &lt;/a&gt; &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;그래서 파랑색 제약 (motion constraints) 도 다 만족시키는 와중에,&lt;br /&gt;
노란색 제약 (landmark constraints) 들도 다 만족시키도록 한다면&lt;br /&gt;
SCV는 자기의 위치를 조금 더 정확히 추정할 수 있을 것이다.&lt;/p&gt;

&lt;p&gt;통상적으로 파랑색 제약을 생성하는 모델을 motion model,&lt;br /&gt;
노란색 제약을 생성하는 모델을 observation model 이라고 부르긴 하는데&lt;br /&gt;
그냥 여기서는 둘 다 measurement (or factor) 라고 부르자.&lt;/p&gt;

&lt;p&gt;이런 관계 (방향성, directed) 그래프를 Bayes Net 이라고 부른다.&lt;br /&gt;
(ps. Bayes Net에 관한 좋은 강의 추천&lt;sup id=&quot;fnref:fgyoutube&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fgyoutube&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;)&lt;/p&gt;

&lt;p&gt;근데 우리는 그림이 필요한게 아니라 (iSAM2에 가면 필요하다. 언젠가 다음편에서 소개함&lt;sup id=&quot;fnref:link&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:link&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;),&lt;br /&gt;
Algebraically 풀 수 있는, 즉 대수적인 툴이 필요하기 때문에&lt;br /&gt;
이 Bayes Net의 measurement 들을 한 데 다 우겨넣으면&lt;/p&gt;

&lt;p&gt;이렇게 matrix 형태로 표현할 수 있다.&lt;/p&gt;

&lt;figure id=&quot;Axb&quot;&gt;
  &lt;img src=&quot;/figs/2021-03-04-slambackend-1/axb.png&quot; alt=&quot;img4&quot; style=&quot;width:95%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; &lt;a href=&quot;#Axb&quot; class=&quot;img3&quot;&gt; Figure: SLAM system &lt;/a&gt; &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;SCV는 구비한 sensor (laser 등)들과  motion model 과 observation model 을 통해 $\textbf{b}$ 를 직접 측정해서 알고 있다.&lt;/p&gt;

&lt;p&gt;따라서 SCV의 정교한 위치(state) 는&lt;br /&gt;
위의 $Ax=b$ 를 풀어서 나오는 최적해일 것이다 (state estimation).&lt;/p&gt;

&lt;p&gt;덤으로 포토캐논들의 위치도 알 수 있게 된다 (mapping) &lt;del&gt;개이득&lt;/del&gt;.&lt;/p&gt;

&lt;p&gt;우리는 여기서 통상적인 사실 몇 가지를 알 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;matrix $A$는 $m \times n$의 형태인데 통상적으로 $m$이 $n$보다 크다는 것이다 ($m &amp;gt; n$). 이런 상황을 보고 overdetermined system 이라고 부른다. &lt;span style=&quot;color:gray&quot;&gt; ps. 심화과정&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; &lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;matrix $A$는 sparse 하다. 왜냐하면 하나의 measurement 는 적은 수의 entity들 사이의 관계만 규정하기 때문이다 &lt;span style=&quot;color:gray&quot;&gt;(여기서는 두개 사이의 관계들만 예시로 나오고 있지만 물론 당연히 둘 이상일 수 있다. ps. 심화과정&lt;sup id=&quot;fnref:ps2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ps2&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;slam이란-도대체-무엇인가&quot;&gt;SLAM이란 도대체 무엇인가?&lt;/h3&gt;

&lt;p&gt;이제는 명확해졌다!&lt;/p&gt;

&lt;p&gt;SCV는 이 연립방정식 $Ax=b$를 풀어야 한다!!!&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;어떻게-axb를-풀까&quot;&gt;어떻게 Ax=b를 풀까?&lt;/h1&gt;

&lt;p&gt;$Ax=b$를 풀 때, 
A의 inverse (혹은 pseudo inverse) 를 곱해서 바로 (deterministic) $x$ 를 쉽게 구할 수 있지~ (normal equation 이라고 부른다)&lt;br /&gt;
라고 말한다면 그것은 중학교 교과서에서만 가능한 연립방정식 예제에 불과하다…&lt;/p&gt;

&lt;p&gt;실제로는 SCV의 odometry와 laser sensor 가&lt;br /&gt;
정확하지 않기 때문에 (noisy measurements) &lt;del&gt;&lt;em&gt;SCV는 싼 유닛이다&lt;/em&gt;&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;해 (optimal solution) 가 deterministic 하게 존재하지 않을 수 있다. &lt;br /&gt;
즉, 완전히 $Ax == b$ 로 완벽히 같을 수는 없다는 말.&lt;/p&gt;

&lt;p&gt;대신 우리는 $ |Ax - b|_{2}^{2} $ 를 최소화하는 해를 찾게 된다 (least square optimization&lt;sup id=&quot;fnref:lsbook&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:lsbook&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;).&lt;br /&gt;
완벽히 같진 않더라도 비슷은 해지자고.&lt;/p&gt;

&lt;p&gt;이 경우 거의 iterative 하게 푸는 것이 국룰이다.&lt;br /&gt;
즉, optimal 한 $x^{*}$ 를 단번에 찾을 수는 없고, 대신,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$x_{0}$으로부터 출발해서 optimal 한 변화량 $\Delta^{*}$ 을 추정한다음에&lt;/li&gt;
  &lt;li&gt;$x_{\text{next}} = x_{\text{prev}} + \Delta^{*} $ 만큼 업데이트 해주는 방식으로 최적해를 향해 나아간다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;그러면 &lt;em&gt;어떻게 $Ax=b$ 를 풀까?&lt;/em&gt; 라는 문제는&lt;/p&gt;

&lt;p&gt;다시, &lt;strong&gt;&lt;em&gt;어떻게 $A\Delta=b$ 를 풀까?&lt;/em&gt;&lt;/strong&gt; 라는 문제가 된다 &lt;sup id=&quot;fnref:ps3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ps3&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;따라서 SLAM이란, $\underset{\Delta}{\mathrm{argmin}} \ ||A\Delta - b ||_{2}^{2}$ 인 $\Delta$를&lt;br /&gt;
어떻게 (효율적으로 w.r.t time and memory) 찾을까? 에 대답하는 문제이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;center&gt; *** 다음 시간에 계속 ... ***  &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;요약&quot;&gt;요약&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;SLAM back-end 입문으로 Factor graphs for robot perception 책 &lt;sup id=&quot;fnref:fgbook&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fgbook&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; 을 추천합니다. SAM, iSAM, iSAM2 세 논문의 내용을 쉬운 언어로 잘 서술하고 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;color:gray&quot;&gt; ps. 심화과정 – 팩트체크: 근데 사실 b는 … &lt;sup id=&quot;fnref:psb&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:psb&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; &lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;예고&quot;&gt;예고&lt;/h2&gt;
&lt;p&gt;다음으로&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;SLAM back-end 이야기 (2편): $Ax=b$ 풀기&lt;br /&gt;
— QR decomposition 이란? + Householder reflection &lt;em&gt;구현&lt;/em&gt; (Matlab 실습)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;편을 써보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:gray&quot;&gt; 그 다음으로 &lt;del&gt;제발&lt;/del&gt; &amp;lt;SLAM back-end 이야기 (2편): $Ax=b$ 풀기 — Householder reflection &lt;em&gt;이론&lt;/em&gt;&amp;gt; 편을 써보겠습니다. &lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;생각해보기&quot;&gt;생각해보기&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;SCV의 &lt;a href=&quot;#Axb&quot;&gt; SLAM system matrix &lt;/a&gt; 에서 지금 matrix 의 column 순서가 포토캐논부터 적혀져있다. 근데 이거를 SCV부터 적으면, 즉 column order 가 달라지면 estimation 할 때 어떤 점들이 달라질까? 해가 달라질까? 더 빨리 풀 수 있을까? 아니면 아무 상관 없을까?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SCV 는 $t=3$에서 포토캐논 a 와 c를 봤다고 생각했다. 하지만 알고보니 b와 d를 본 것이었다면? 즉 data association 의 outlier 가 존재할 때 SLAM 의 최적해는 어떤 영향을 받을까? 그리고 이런 false association 이 존재함에도 불구하고, 어떻게 해를 더 강건하게 예측할 수 있을까?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$||Ax - b ||_{2}^{2}$ 의 argmin 을 찾는 것은 일반적으로 SLAM외에도 다른 computer vision estimation 문제에도 매우 자주 등장하는 상황이다. SLAM에 있어 위 식을 푸는 것은 다른 computer vision 의 estimation 문제와 어떤점에서 특별하게 다를까?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;주석&quot;&gt;주석&lt;/h3&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Dellaert, Frank, and Michael Kaess. “Square Root SAM: Simultaneous localization and mapping via square root information smoothing.” The International Journal of Robotics Research 25.12 (2006): 1181-1203. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:fgyoutube&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Youtube lectures — &lt;a href=&quot;https://youtube.com/playlist?list=PLOJ3GF0x2_eWtGXfZ5Ne1Jul5L-6Q76Sz&quot;&gt;Factor graphs short course (Jan 2020, UAL)&lt;/a&gt; by Prof. Jose Luis Blanco &lt;a href=&quot;#fnref:fgyoutube&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:link&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;link — To be added &lt;a href=&quot;#fnref:link&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;(1) 실제로는 랜드마크 수가 훨씬 더 수백 수천개만큼 많을 수도 있고 (SfM), 없을 수도 있다 (Pose-graph SLAM) &lt;br /&gt; (2) 실제로는 measurement model들이 대부분 non-linear 하기 때문에 1차미분한 Jacobian matrix H가 쓰인다. 더 엄밀하게는 이 H에 covariance matrix (noise matrix) 의 inverse squared 가 곱해진 형태가 A이다. 자세한 것은 이 책&lt;sup id=&quot;fnref:fgbook:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fgbook&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;의 chapter 2 를 참고. &lt;br /&gt; (3) 최근에는 underdetermined system 일 때 SLAM을 어떻게 풀어야할지에 관한 연구도 이루어지고 있다 – 참고: Fourie, Dehann, et al. “Towards Real-Time Non-Gaussian SLAM for Underdetermined Navigation.” (IROS 2020). &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ps2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;여기서 예측하는 대상을 variable, 그 관계에 대해 factor 라고 부른다. factor 는 수학적으로는 n-ary function이다. 자세한 내용은 이 책&lt;sup id=&quot;fnref:fgbook:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fgbook&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; 의 Chapter 1장 (만 읽어도 됨) 참고. &lt;a href=&quot;#fnref:ps2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:lsbook&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;더 알아보고싶다면, 이 논문을 참고. Grisetti, Giorgio, et al. “Least squares optimization: From theory to practice.” Robotics (2020) &lt;a href=&quot;#fnref:lsbook&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ps3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;그래서 SLAM이 왕왕 &lt;strong&gt;iterative&lt;/strong&gt; non-linear least-square optimization 이라고 불리기도 한다 &lt;a href=&quot;#fnref:ps3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:fgbook&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Dellaert, Frank, and Michael Kaess. “Factor graphs for robot perception.” Foundations and Trends in Robotics (2017) &lt;a href=&quot;#fnref:fgbook&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:fgbook:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:fgbook:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:psb&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;#Axb&quot;&gt; SLAM system matrix figure &lt;/a&gt; 에서 $b$가 마치 measurement 값인것 처럼 일단 소개를 했었었다 (쉬운 이해를 위해). 하지만 사실 b는 prediction error vector이다. 즉 어떤 시점 $i$ 에서, measurement model 을 이용해서 예측된 (우리가 그 값일 거라고 기대하는) measurement $h_{i}(X_{i}^{o})$ 와 실제로 얻은 measurement 값 $z_{i}$ 의 차이가 $b$ vector 가 된다. 즉, $|z_{i} - h_{i}(X_{i}^{o})|$ 가 $b$ vector인 것이다. 더 엄밀하게는 prediction error vector에 whitening 까지 된 것이 $b$ vector가 되는 것인데, 자세한 내용은 Factor Graph Book&lt;sup id=&quot;fnref:fgbook:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fgbook&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; 의 챕터 2.3 을 참고. &lt;a href=&quot;#fnref:psb&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>SLAM의 뿌리를 찾아서</title>
   <link href="http://localhost:4000/blog/2021/03/02/slam-root.html"/>
   <updated>2021-03-02T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/03/02/slam-root</id>
   <content type="html">&lt;h3 id=&quot;slam의-뿌리를-찾아서-&quot;&gt;&lt;strong&gt;SLAM의 뿌리&lt;/strong&gt;를 찾아서 ...&lt;/h3&gt;
&lt;p&gt;떠나봅시다. &lt;/p&gt;
&lt;p&gt;NOTE: 이 글은 예전에 미디엄에 올린 &amp;lt;SLAM을 SLAM답게 만드는 건 무엇일까?: 내맘대로 SLAM 정의하기&amp;gt; 의 후속편 같은 느낌으로 적어보았습니다. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;참고: &amp;lt;SLAM을 SLAM답게 만드는 건 무엇일까?: 내맘대로 SLAM 정의하기&amp;gt;&lt;br /&gt;
&lt;a href=&quot;http://bit.ly/define-slam-myself-1&quot;&gt;http://bit.ly/define-slam-myself-1&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;SLAM의 본질이 어디에 있는지 파악하고&lt;br /&gt;
하나씩 끝말잇기 하듯이&lt;br /&gt;
그 다음 뿌리를 향해 나아가봅시다. &lt;/p&gt;
&lt;p&gt;레츠고 ...&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;1&quot;&gt;1.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 &lt;strong&gt;&lt;em&gt;Estimation&lt;/em&gt;&lt;/strong&gt; 이다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;따라서 MLE, MAP 와 같은 이야기가 나온다. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;더 나아가 SLAM문제를 Gaussian noise 기반으로 probabilistic 하게 formulating 하는 건 거의 시초라고도 (이자 SLAM문제의 코어) 할 수 있다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; R. C. Smith and P. Cheeseman. On the representation and estimation of spatial uncertainty. IJRR. 1986. &lt;/li&gt;
&lt;li&gt; Smith, Self, and Cheesemans, Estimating Uncertain Spatial Relationships in Robotics, 1990 &lt;/li&gt;
&lt;li&gt; filtering 으로 풀든 optimization으로 풀든 공통으로 해당되는 중요한 내용이다. &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;2&quot;&gt;2.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 &lt;strong&gt;&lt;em&gt;State&lt;/em&gt;&lt;/strong&gt; estimation 이다. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;여기서 state란 어떤 것이든 될 수 있지만, 통상적으로 우리는 알고싶은 것은 robot 과 world (map) 의 pose (즉, position + rotation) 이다. &lt;ul&gt;
&lt;li&gt; 근데 state estimation 은 1. filtering 기반, 2. optimization 기반으로 나눌 수 있는데, 특히 이 글에서는 optimization기반의 SLAM의 뿌리에 대해 찾아가보자 ... &lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;물론 velocity, bias, semantic class 등 추론하고 싶은 어떤 것도 state가 될수는 있다 (사실 state라는 것은 수학적으로 확률변수-randome variable- 이기 때문에!). &lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;3&quot;&gt;3.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 &lt;strong&gt;&lt;em&gt;Optimization-based&lt;/em&gt;&lt;/strong&gt; state estimation 이다. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;iteratively 해를 구하는 것이 골자. &lt;ul&gt;
&lt;li&gt; 특히 SLAM은 overdetermined system (measurement 수가 variable 수보다 많음) 이기 때문에 closed form으로 해를 구할 수 없어서 iterative optimization으로 답을 구해야 한다.  &lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;그래서 기본적으로 SLAM공부에 있어 (computer vision과 마찬가지로) GN, LM 등이 빠질 수 없다. &lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;4&quot;&gt;4.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 &lt;strong&gt;&lt;em&gt;Nonlinear&lt;/em&gt;&lt;/strong&gt; optimization-based state estimation 이다. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;센서데이터의 measurement model 이 주로 nonlinear 하기 때문&lt;/li&gt;
&lt;li&gt;따라서 linearize, Jacobian 등의 말이 많이 등장한다. &lt;/li&gt;
&lt;li&gt;이 정의와 같은 의미로 Factor graph-based SLAM 이다~ 라는 말도 자주 쓰인다. &lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;5&quot;&gt;5.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 &lt;strong&gt;&lt;em&gt;Sparse&lt;/em&gt;&lt;/strong&gt; nonlinear optimization-based state estimation 이다. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;SLAM에서는 하나의 measurement 에는 적은 수의 variable 만이 관여하기 때문. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; 예를 들어, 아주 많은 수의 node들이 system에 있다고 하더라도 하나의 odometry measurement는 직전 node와 현재 node 만이 관여한다. 따라서 전체 measurement block들을 쌓은 Jacobian matrix 는 매우 sparse 해진다. &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;또한, SLAM에서는 알고싶은 state (variable) 의 개수 보다 measurement 의 수가 훨씬 많기 때문에 더 sparse 해진다. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;따라서 sparsity 를 잘 이용하기 위해서 variable re-ordering 같은 이야기가 나온다. 여기 5번 항목까지가 Square-root SAM 논문의 내용이라고 봐도 무방하다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; Frank Dellaert, and Michael Kaess. &quot;Square Root SAM: Simultaneous localization and mapping via square root information smoothing.&quot;, IJRR 2006&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;6&quot;&gt;6.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 sparse nonlinear optimization-based state estimation &lt;strong&gt;&lt;em&gt;in an incremental setting&lt;/em&gt;&lt;/strong&gt; &amp;nbsp; 이다. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;센서데이터는 순차적으로 들어오기 때문에, 이전 계산 값을 이용해야 시간적으로 효율적이다. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;또한 SLAM은 nonlinear problem 이기 때문에 이전 계산 값을 이용해야 local minima 에 빠지는 것을 방지할 수 있어 성능적으로 효과적이다. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; 여기까지가 isam1 의 내용이다 (08 TRO iSAM: Incremental smoothing and mapping)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;7&quot;&gt;7.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 sparse nonlinear optimization-based state estimation in an incremental setting &lt;strong&gt;&lt;em&gt;by connecting between graphical model and sparse linear algebra perspective&lt;/em&gt;&lt;/strong&gt; &amp;nbsp; 이다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Squared Root SAM및 isam 까지는 실제 solver 는 선형대수적으로 풀었지만, Bayes tree기반의 isam2로 넘어올 수 있었던 것은 matrix 가 실제로 graphically 어떤 의미를 가지는 지를 (Kaess and Dellaert‬님께서) 이해하고 있었기 때문임.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Information matrix 는 그 의미가 사실 MRF이고, ... 이런 것에 대한 이야기들. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; 여기까지가 isam2 의 내용이다 (12 IJRR iSAM2: Incremental Smoothing and Mapping Using the Bayes Tree). 또한 Factor Graphs for Robot Perception 책에서도 잘 서술되어 있다. &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;8&quot;&gt;8.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 sparse nonlinear optimization-based state estimation in an incremental setting by connecting between graphical model and sparse linear algebra perspective &lt;strong&gt;&lt;em&gt;on a manifold space&lt;/em&gt;&lt;/strong&gt; &amp;nbsp; 이다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;SLAM에서 관심있는 state는 거의 pose인데 여기서 rotation 부분이 nonlinear 함. 따라서 해를 update 하는 공간을 manifold 로 삼을 필요가 있음.   &lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;9&quot;&gt;9.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 sparse nonlinear optimization-based state estimation in an incremental setting by connecting between graphical model and sparse linear algebra perspective on a manifold space &lt;strong&gt;&lt;em&gt;by integrating multiple sensors’ data&lt;/em&gt;&lt;/strong&gt; &amp;nbsp; 이다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;front-end에 관한 이야기이다. &lt;/li&gt;
&lt;li&gt;SLAM은 수중, 지하, 지상, 공중 등 다양한 환경에서 로봇이 활동(navigation)하는 것에 모두 관심사가 있음. 그런데 환경의 특성마다 적합한 센서가 다르고 (수중은 Sonar센서 등) 이들을 퓨전해야 할 필요가 있음. &lt;ul&gt;
&lt;li&gt; 따라서 SLAM 엔지니어는 어떤 센서가 본인의 로봇이 활동하고자 하는 환경에 적합한지 이해하고, 선정하고, 융합할 수 있어야 함. &lt;/li&gt;
&lt;li&gt; 특히 카메라 센서 기반의 SLAM을 하는경우 multiple-view geometry를 공부해야 하는데 그 이유가 여기 9번에 해당하는 내용. &lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;10&quot;&gt;10.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 sparse nonlinear optimization-based state estimation in an incremental setting by connecting between graphical model and sparse linear algebra perspective on a manifold space by integrating multiple sensors’ data &lt;strong&gt;&lt;em&gt;with robust data associations&lt;/em&gt;&lt;/strong&gt; &amp;nbsp; 이다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;front-end 에서는 data association (DA) 이 아닌 것이 없다. local feature matching 부터 loop detection 까지, 결국 모든 것이 이거랑 제일 가까운 애가 누구냐? 에 대답하는 문제. &lt;/li&gt;
&lt;li&gt;근데 이게 100% 정확할 수 없기 때문에 두 가지 방향에서의 노력이 요구된다. &lt;ul&gt;
&lt;li&gt;먼저 그럼에도 더 실수없이 잘 할 수 있도록 더 좋은 DA를 해야하고, &lt;/li&gt;
&lt;li&gt;두 번째로 DA에 실수가 있음에도 불구하고, solution이 망가지지 않도록 back-end에서 강건하게 막아줄 수 있어야 한다. MIT의 Luca Carlone 교수님이 이 분야에서 최근 많은 연구를 하고 있다. &lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;11&quot;&gt;11.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 sparse nonlinear optimization-based state estimation in an incremental setting by connecting between graphical model and sparse linear algebra perspective on a manifold space by integrating multiple sensors’ data with robust data associations, &lt;strong&gt;&lt;em&gt;for multiple robots&lt;/em&gt;&lt;/strong&gt; &amp;nbsp; 이다. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;robot mission 이 결국 multi-robot 이 팀을 이루어 미션을 수행하는 경우가 많음. &lt;/li&gt;
&lt;li&gt;멀티 로봇의 경우, 또 communication, coordinate 등 다양한 것들이 고려되어야 함. &lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;12-to-be-added-tba-&quot;&gt;12. To be added (TBA) ...&lt;/h4&gt;
&lt;hr /&gt;

&lt;p&gt;대충 여기까지 와보았습니다.&lt;br /&gt;
깊고도 넓은 SLAM의 세계!&lt;br /&gt;
그럼 20000!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>SegMap 빌드하기</title>
   <link href="http://localhost:4000/blog/2021/03/02/segmap-build.html"/>
   <updated>2021-03-02T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/03/02/segmap-build</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;NOTE: &lt;a href=&quot;https://gisbi.medium.com/segmap-%EB%B9%8C%EB%93%9C%ED%95%98%EA%B8%B0-220d6d9b4ef6&quot;&gt;미디엄 블로그&lt;/a&gt; 이전중입니다&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;목표&quot;&gt;목표&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;segmap 을 빌드해보자.&lt;/li&gt;
  &lt;li&gt;작업환경: Ubuntu18.04, ROS 는 root (not virtual)로 설치한 상황이며 나머지 디펜던시는 모두 virtualenv에서 진행 (아래에 자세히 설명함)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;배경지식&quot;&gt;배경지식&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;원래는 segmatch라는 리포였다가 (17 ICRA 시절) 이름이 segmap (18 RSS 시절) 으로 바뀌었다.&lt;/li&gt;
  &lt;li&gt;segmap 공식 리포 (ethz-asl/segmap) 의 경우 빌드하려면 딥러닝 기반 segmap말고 안딥러닝인 segmatch를 쓰고 싶어도 tensorflow_ros_cpp 를 무적권 빌드해야해서 너무 불키하다.&lt;/li&gt;
  &lt;li&gt;그래서 인터넷에 segmatch 포크본들이 돌아다니고 있어서 해보니까 왠지 안된다.&lt;/li&gt;
  &lt;li&gt;그래서 그냥 공식리포본에서 tensorflow_ros_cpp 를 빌드를 어찌저찌 해내기로 결심하였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;과정&quot;&gt;과정&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;수많은 삽질들이 있었으나 작동하는 사항만을 아래 기술해보자.&lt;/li&gt;
  &lt;li&gt;일단 모든 과정들을 virtual env 에서 하면 된다. (모든 캣킨 빌드들 및 로스실행 포함). tf 를 virtualenv 에서 설치하는 게 편하기 때문.&lt;/li&gt;
  &lt;li&gt;일단 segmap 저자의 리드미에서 하라는 순서로 해도되지만, 내 기준에 먼저 해놓으면 좋은것부터 이제 설명해보자.&lt;/li&gt;
  &lt;li&gt;일단 작업할 virtual environment 를 만들자.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- ```  --&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;virtualenv ~/segmapenv &lt;span class=&quot;c&quot;&gt;# 하면 만들어지고&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/segmapenv/bin/activate &lt;span class=&quot;c&quot;&gt;# 하면 접속됨 (conda activate 같은 것 같다). 그러면 앞에 (env name) 이 뜬다&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# 예시 &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;!-- ```  --&gt;

&lt;ul&gt;
  &lt;li&gt;그럼 ~/segmapenv 에 대충 mkdir home 해서 home 밑에서 작업해보자&lt;/li&gt;
  &lt;li&gt;이제 tensorflow 를 설치해야 한다.&lt;/li&gt;
  &lt;li&gt;간편하게 pip install 하면 안되냐 싶겠지만 segmap 이 쓰고 있는 tensorflow_ros_cpp 라는 모듈이 그러면 안된다.&lt;/li&gt;
  &lt;li&gt;간단히 이유를 설명하자면 tf를 설치하는 방법에는 세가지가 있다. 1. pip, 2. bazel, 3. tensorflow_catkin&lt;/li&gt;
  &lt;li&gt;1은 잘 알테고 (하지만 지금 이걸로 설치하면 마지막에 API가 달라서? segmapper 가 링크가 안된다 — 다른 디펜던시 컴파일 20분(-j32의 경우)~2시간 겨우겨우 기다리고 마지막에 빌드 에러나는 환장하는 상황을 볼 수 있다), 3은 몰라도 되고 (궁금하면 따로 찾아보자)&lt;/li&gt;
  &lt;li&gt;결론은 2번 방법인 bazel 로 설치해야 한다. bazel은 구글이 만든 빌드툴 어쩌구 저쩌구… 그렇다고 한다. 뭐 자세히 알필요는 없고&lt;/li&gt;
  &lt;li&gt;암튼 2로 해야 하는이유는 tensorflow_ros_cpp 의 깃 리포에 가보면 표들이 있는데 https://github.com/tradr-project/tensorflow_ros_cpp&lt;/li&gt;
  &lt;li&gt;Ubuntu 18.04 64bits, Python 2.7.6, ROS Melodic 의 경우 1.8.0 tf version을 사용할 시 bazel cpu 및 gpu 에만 체크표시가 되어있다. 여기 보면 ABI difference problems 라는 말이 나오는데 이게 위에서 말한 20분 기다리고 마지막에 에러날때 볼 수 있는 현상이다. 이 고생을 안하려면 무적권 bazel 로 설치하자.&lt;/li&gt;
  &lt;li&gt;https://github.com/tradr-project/tensorflow_ros_cpp#custom-compilation-of-tensorflow-using-bazel 를 읽어보자.&lt;/li&gt;
  &lt;li&gt;그나저나 일단 tf src를 받아야 한다. https://www.tensorflow.org/install/source?hl=ko 여기 잘 나와있음.&lt;/li&gt;
  &lt;li&gt;그래도 굳이 명령어들을 다시 적어주자면&lt;/li&gt;
  &lt;li&gt;cd ~/sgmapenv/home하고, 여기다가 tensorflow 를 받자&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git clone https://github.com/tensorflow/tensorflow.git&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;segmap저자는 1.8.0 을 쓰라고 하니 (언제적 버전이지만…) 시키는 대로 하자. 해당 브랜치로 변경해주어야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git checkout r1.8&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;직접 빌드하는경우 configuration 을 정해줘야 하므로 ./configure 하자. 근데 계속 엔터를 갈기면 된다. 즉 디폴트 옵션으로 하면됨.&lt;/li&gt;
  &lt;li&gt;우리는 1.8.0 을 빌드할것이므로&lt;/li&gt;
  &lt;li&gt;그리고 cpu 버전으로 일단 빌드하고 있다. 내 목표는 segmap까지도 필요없고 segmatch만 쓰는 것이므로…&lt;/li&gt;
  &lt;li&gt;따라서 다음 명령어를 해주면 된다. 고 나와있다. (근데 일케 하지마시오)&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;bazel build &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;opt //tensorflow/tools/pip_package:build_pip_package &lt;span class=&quot;c&quot;&gt;# 이 때 저 // 까지 모두 포함해서 한줄로 써주면 된다. 헷갈리면 다시 여기를 보자 https://www.tensorflow.org/install/source?hl=ko&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;근데 이대로 하면 안됨. 안된다. 안된다. 안된다. 안된다. 안된다.&lt;/li&gt;
  &lt;li&gt;https://www.tensorflow.org/install/source?hl=ko 여기서 하려고 하는거는 빌드를 직접 내 입맛에 맞는 configuration 으로 해서 결국에는 pip 패키지를 만들려고 하는건데&lt;/li&gt;
  &lt;li&gt;우리의 목표는 아예 pip 패키지 아니기 때문. segmap 리드미에 pip 로 설치하라 되어있는데 안됨&lt;/li&gt;
  &lt;li&gt;그래서 tensorflow_ros_cpp 의 리드미를 잘 읽어야 한다. 진짜루&lt;/li&gt;
  &lt;li&gt;https://github.com/tradr-project/tensorflow_ros_cpp#custom-compilation-of-tensorflow-using-bazel&lt;/li&gt;
  &lt;li&gt;이걸 미리 읽지않고 이틀을 날린 내 자신에게 반성을…&lt;/li&gt;
  &lt;li&gt;여기보면 뭐라 되어있냐면&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Follow the Installing TensorFlow from Sources guide up to “Configure the installation” (including), and build the C++ library with the following command:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bazel build --config=opt --define framework_shared_object=false tensorflow:libtensorflow_cc.so&lt;/code&gt; You don’t need to continue with the guide building or installing the pip package (but you might be interested, because a custom-built tensorflow can provide you with higher performance even in Python).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;그니껜 bazel을 쓰긴 쓸건데 https://www.tensorflow.org/install/source?hl=ko 있는대로 할필요 없다는 뜻임&lt;/li&gt;
  &lt;li&gt;그리고 bazel build 뒤에 붙은 옵션들도 tf site에 있는 것과 다른데 절대 위의 명령어로 해주어야 한다.&lt;/li&gt;
  &lt;li&gt;중요하니까 한번더 복붙&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;bazel build &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;opt &lt;span class=&quot;nt&quot;&gt;--define&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;framework_shared_object&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false &lt;/span&gt;tensorflow:libtensorflow_cc.so&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;근데 이 bazel build 라는 놈이 안될수 있다. 왜냐하면 bazel 이라는게 2018년 부터 발전해와서 API가 엄청 달라진듯 버전마다 (뇌피셜)&lt;/li&gt;
  &lt;li&gt;암튼 여기 잘나와 있다 http://nblog.syszone.co.kr/archives/9751&lt;/li&gt;
  &lt;li&gt;한줄요약 하면 tensorflow-1.8.0 를 bazel로 빌드하려면 Bazel 0.10.0 을 써야됨&lt;/li&gt;
  &lt;li&gt;다양한 bazel 버전들은 bazel 공식 깃 리포에 있으며 0.10.0 버전은 https://github.com/bazelbuild/bazel/releases/tag/0.10.0&lt;/li&gt;
  &lt;li&gt;여기서 아마 bazel-0.10.0-without-jdk-installer-linux-x86_64.sh 이거 아니면 bazel-0.10.0-installer-linux-x86_64.sh 이거를 설치하면 됨&lt;/li&gt;
  &lt;li&gt;나는 앞에걸로 한듯 — 근데 암튼 bazel 디펜던시로 jdk 설치해줘야 하는거같다 https://docs.bazel.build/versions/3.3.0/install.html 여기 보면 sudo apt install openjdk-11-jdk 해주자&lt;/li&gt;
  &lt;li&gt;일단 지금 다시 상기하자면 지금 virtualenv 가 activate 된 home 에서 하고 있음&lt;/li&gt;
  &lt;li&gt;chmod +x bazel-0.10.0-without-jdk-installer-linux-x86_64.sh 하고 ./bazel-0.10.0-without-jdk-installer-linux-x86_64.sh 하면 금방 깔린다.&lt;/li&gt;
  &lt;li&gt;그리고 나서 이제 다시 아까 하려면 걸 다시해주면 (아래 명령어)&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;bazel build &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;opt &lt;span class=&quot;nt&quot;&gt;--define&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;framework_shared_object&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false &lt;/span&gt;tensorflow:libtensorflow_cc.so&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;이거는 거의 1분 안되어서 금방 끝난다. 컴퓨터 사양마다 다를수있음.&lt;/li&gt;
  &lt;li&gt;암튼 이러면 어딘가에 libtensorflow_cc.so 가 생성되어있다. tensorflow_ros_cpp 는 cmakelist.txt 에 보면 이 so 파일을 찾아다가 빌드하는 식이다.&lt;/li&gt;
  &lt;li&gt;ㅇㅋ 그럼 이제는 tensorflow_ros_cpp 를 빌드해야 하니까 catkin ws 밑에 src에 이 리포를 받아주자&lt;/li&gt;
  &lt;li&gt;근데 여기서부터는 segmap저자가 필요한 디펜던시들을 한방에 모조리 git clone 해올수 있도록 wstool 로 잘 해놨으니 tensorflow_ros_cpp 도 받는 김에 다 받아오자.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; YOUR_VIRTUAL_ENV_PATH/home/segmap_ws/src
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;YOUR_VIRTUAL_ENV_PATH/home/segmap_ws/
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;catkin init
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;catkin config &lt;span class=&quot;nt&quot;&gt;--merge-devel&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;catkin config &lt;span class=&quot;nt&quot;&gt;--cmake-args&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-DCMAKE_BUILD_TYPE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Release
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;src
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git clone https://github.com/ethz-asl/segmap.git
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;wstool init
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;wstool merge segmap/dependencies.rosinstall
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;wstool update&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;이제 segmap_ws위치에서 catkin build tensorflow_ros_cpp 를 해서 tensorflow_ros_cpp 만 컴파일 해준다.&lt;/li&gt;
  &lt;li&gt;segmap 저자가 시키는대로 하면 https://github.com/ethz-asl/segmap/wiki/FAQ#q-issues-compiling-tensorflow_ros_cpp&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmappyenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;catkin build tensorflow_ros_cpp &lt;span class=&quot;nt&quot;&gt;--cmake-args&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-DFORCE_TF_PIP_SEARCH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ON&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;라고 해서 뒤에 이상한 옵션들이 더 붙는데 이거는 pip 도 찾긴찾을래? 라는 거 므로 사실 ON으로 하면 안된다. segmap_ws/src/tensorflow_ros_cpp 밑에 CMakeLists.txt 가 있는데 거기서 bazel 말고 다른애들을 찾을 가능성을 다 off 해주자. 즉&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-cmake&quot; data-lang=&quot;cmake&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# variables affecting the search for the tensorflow library&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;FORCE_TF_PIP_SEARCH OFF CACHE BOOL “Whether to search for pip-installed Tensorflow even on systems using C++11 ABI”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;DISABLE_TF_PIP_SEARCH ON CACHE BOOL “Whether to skip search for pip-installed Tensorflow”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;FORCE_TF_BAZEL_SEARCH ON CACHE BOOL “Whether to search for bazel-compiled Tensorflow even if tensorflow was already found”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;DISABLE_TF_BAZEL_SEARCH OFF CACHE BOOL “Whether to skip search for bazel-compiled Tensorflow”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;FORCE_TF_CATKIN_SEARCH OFF CACHE BOOL “Whether to search for tensorflow_catkin even if tensorflow was already found”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;DISABLE_TF_CATKIN_SEARCH ON CACHE BOOL “Whether to skip search for tensorflow_catkin”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;그리고 그 좀 아래에 보면 bazel로 tensorflow_ros_cpp를 빌드 할때 두 가지를 얘가 끌어다가 쓰는 걸 알수있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-cmake&quot; data-lang=&quot;cmake&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# variables affecting bazel search&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;TF_BAZEL_LIBRARY “{CATKIN_DEVEL_PREFIX}/../libtensorflow_cc.so” CACHE STRING “Path to the bazel-compiled Tensorflow C++ library”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;TF_BAZEL_SRC_DIR “{CATKIN_DEVEL_PREFIX}/../tensorflow-include-base” CACHE STRING “Path to the Tensorflow sources directory”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;즉 .so 파일과 tensorflow source 가 필요한건데 tensorflow source 의 경로는 지금 어쩌구…../tensorflow-include-base 라고 되어있는데 귀찮으니까 절대경로로 바꿔주자. 아까 tensorflow git clone 했던 그 디렉토리를 해주면 된다.&lt;/li&gt;
  &lt;li&gt;그럼 이제 아까 bazel build 로 만든 .so 파일이 어딘가 있는데 일단 대충 탐색기에서보면 못찾겠다. 이때 find 명령어를 써주자&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; / 
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;find &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-name&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;libtensorflow_cc.so&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;해주니까 이상한 bazel의 숨김폴더 밑에 존재하는 걸 확인할 수 있었다.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;./home/user/.cache/bazel/_bazel_user/f61ec775ae98149c983e28ce5aff1318/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/libtensorflow_cc.so.runfiles/org_tensorflow/tensorflow/libtensorflow_cc.so
./home/user/.cache/bazel/_bazel_user/f61ec775ae98149c983e28ce5aff1318/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/libtensorflow_cc.so&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo cp&lt;/span&gt; ./home/user/.cache/bazel/_bazel_user/f61ec775ae98149c983e28ce5aff1318/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/libtensorflow_cc.so &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;어딘가붙여넣을경로&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;해서 보기좋은 곳으로 옮겨주자&lt;/li&gt;
  &lt;li&gt;그리고 다시 segmap_ws/src/tensorflow_ros_cpp 의 CMakeLists.txt 로 돌아가서 .so 를 찾는 라인에 경로를 저 내가 복사해놓은 어딘가붙여넣을 경로 로 바꿔주면 된다.&lt;/li&gt;
  &lt;li&gt;근데 기본적으로 {CATKIN_DEVEL_PREFIX}/../libtensorflow_cc.so 라고 되어 있는데 이거는 catkin workspace (devel, build, src있는 그 경로) 경로를 의미한다. 나는 cmakelist 에 저 - - 라인을 바꾸기 귀찮아서 so파일을 아예 workspace (devel build src와 같은 위치) 에 복사해버림.&lt;/li&gt;
  &lt;li&gt;그러고 나서 (아참 catkin_tools 를 미리 설치해야 함 그래야 catkin build 를 쓸수있음)&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;catkin build tensorflow_ros_cpp&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;하니까 한 30초 안걸려서 빌드가 다 됐다.&lt;/li&gt;
  &lt;li&gt;이제 진짜 마지막으로 아래 명령어를 해주면&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;catkin build segmapper&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;segmapper 및 segmapper 가 필요로 하는 디펜던시들이 모두 컴파일이 된다.&lt;/li&gt;
  &lt;li&gt;근데 pcl_catkin 이랑 gtsam이 진짜 개오래 걸린다. 그래서 조금이라도 더 빨리 해주기 위해서&lt;/li&gt;
  &lt;li&gt;나는 i9–9900 을 사용중이어서 코어가 16개 (virtual로는 총 32개) 여서&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;catkin build segmapper &lt;span class=&quot;nt&quot;&gt;-j32&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;라고 해주었다.&lt;/li&gt;
  &lt;li&gt;그러면 20분정도 걸린다. 램은 많이 쓸때는 40G까지 올라간듯 (64기가 장착중)&lt;/li&gt;
  &lt;li&gt;코어가 많아도 램딸리면 터질수있으니 이거는 자기 시스템에 맞게 조절바람.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그러면 마지막에&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;이런 화면이 뜨면 모두가 잘 빌드가 된것이다! ㅠㅠ&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
  &lt;img src=&quot;/figs/2021-03-02-segmap-build/img1.png&quot; alt=&quot;img1&quot; style=&quot;width:100%&quot; /&gt;
  &lt;figcaption&gt; &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;&lt;/span&gt;
&lt;!-- &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/figs/2021-03-02-segmap-build/img1.png&quot; width=700&gt;&lt;/p&gt; --&gt;
&lt;!-- ![dsf](/logo.png) --&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;source &lt;/span&gt;devel/setup.bash&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;를 해주고&lt;/li&gt;
  &lt;li&gt;저자가 올려둔 데이터 (KITTI 00 번과 05번, 각 16G, 10G) 를 http://robotics.ethz.ch/~asl-datasets/segmap/segmap_data/ 여기서 받아서 적절한 위치에 bag file 을 놓아두고&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;roslaunch segmapper kitti_loop_closure.launch&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;하면 되는데 gedit kitti_loop_closure.launch 해서 여기보면 bagfile 경로를 지정하게 되어있다. 그거를 내가 위치시킨 경로로 바꿔주면 아마 실행 될것임.&lt;/li&gt;
  &lt;li&gt;참고로 kitti_loop_closure.launch 가 17 ICRA의 segmatch이고 cnn_kitti_loop_closure.launch가 18 RSS의 segmap 이다. (암튼 segmatch를 쓰고싶어도 tensorflow_ros_cpp를 컴파일해야했던 ㅠㅠ)&lt;/li&gt;
  &lt;li&gt;근데 틀면 기본적으로 rviz 에서는 아무것도 안보여지는데 저자가 올려둔 rviz config 파일을 같이 틀어야 아마 보여질듯 http://robotics.ethz.ch/~asl-datasets/segmap/segmap_data/kitti/&lt;/li&gt;
  &lt;li&gt;근데 귀찮으니까 그냥 왼쪽 아래에서 add 해주고 target/representation인가…? 이거를 틀면 잘 실행되고 있음을 알 수 있다. 자세한건 정광욱님의 플레이 영상을 보면 참고가 됨. https://www.youtube.com/watch?v=Hb7Agk8fs10&amp;amp;t=3s&lt;/li&gt;
  &lt;li&gt;05번을 다 돌아보았다.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;img src=&quot;/figs/2021-03-02-segmap-build/img2.png&quot; alt=&quot;img2&quot; style=&quot;width:100%&quot; /&gt;
  &lt;figcaption&gt; &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;그나저나 다 돌고 런치파일을 실행한 터미널을 ctrl+C 하면 알아서 로그 파일과 결과파일을 /tmp/ 어딘가에 저장해준다. (어디 저장했다고 경로가 뜸)&lt;/li&gt;
  &lt;li&gt;암튼 돌리기 성공!&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;결론&quot;&gt;결론&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;segmatch를 돌려보았다.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>블로그 시작!</title>
   <link href="http://localhost:4000/blog/2021/03/01/blog-start.html"/>
   <updated>2021-03-01T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/03/01/blog-start</id>
   <content type="html">&lt;p&gt;블로그를 시작해봅니다.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Publications</title>
   <link href="http://localhost:4000/publications/"/>
   <updated>2021-01-01T00:00:00+09:00</updated>
   <id>http://localhost:4000/publications</id>
   <content type="html">&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px; float:right;&quot;&gt;
    NOTE: publications are sorted in time order, &lt;a href=&quot;https://www.semanticscholar.org/author/Giseop-Kim/66319300&quot;&gt; all bib files are found here. &lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dissertations&quot;&gt;Dissertations&lt;/h2&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;msthesis&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LiDAR-based Lifelong Robotic Mapping in Changing Environments&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt; &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Advised by  &lt;a href=&quot;https://ayoungk.github.io/&quot; target=&quot;_blank&quot;&gt; Dr. Ayoung Kim&lt;/a&gt; and Dr. Youngchul Kim&lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Ph.D., CEE, KAIST, 2022 —  
    Thesis (TBA), &lt;a href=&quot;https://docs.google.com/presentation/d/e/2PACX-1vQW1_LZZ6_RiOG2zmIZO9tR52ryRSDCDvJuK5YWrrHO5Y_iBNA-IB-_nsB8LcvZcl8Zfc6NU0A3nPWS/pub?start=false&amp;amp;loop=false&amp;amp;delayms=60000&amp;amp;slide=id.gfabac6aa73_0_21&quot; target=&quot;_blank&quot;&gt; Defence presentation material &lt;/a&gt;
    &lt;br /&gt;
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;msthesis&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Isovist-induced Robust LiDAR Localization&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt; &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Advised by &lt;a href=&quot;https://ayoungk.github.io/&quot; target=&quot;_blank&quot;&gt; Dr. Ayoung Kim&lt;/a&gt; &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; M.S., CEE, KAIST, 2019 —  
    &lt;a href=&quot;/publications/gkim-dissertation-ms.pdf&quot; target=&quot;_blank&quot;&gt; Thesis&lt;/a&gt;
    &lt;br /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;journals&quot;&gt;Journals&lt;/h2&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;j20sc2&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Scan Context++: Structural Place Recognition Robust to Rotation and Lateral Variations in Urban Environments&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Sunwook Choi$^{†}$, Ayoung Kim. (†: joint work with NAVER LABS)&lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; IEEE Transactions on Robotics (T-RO), accepted, 2021 — 
    &lt;a href=&quot;/publications/gkim-2021-tro.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;,  
    &lt;a href=&quot;https://youtu.be/ZWEqwYKQIeg&quot; target=&quot;_blank&quot;&gt; Video&lt;/a&gt;,  
    Code (TBA) &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;An Improved LiDAR Place Recognition&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;ral19&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1-Day Learning, 1-Year Localization: Long-term LiDAR Localization using Scan Context Image&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Byungjae Park$^{†}$, Ayoung Kim. (†: joint work with ETRI)&lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; IEEE Robotics and Automation Letters, 2019 (with ICRA 2019) —  
    &lt;a href=&quot;/publications/gkim-2019-ral.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;,  
    &lt;a href=&quot;https://www.youtube.com/watch?v=apmmduXTnaE&quot; target=&quot;_blank&quot;&gt; Video&lt;/a&gt;,
    &lt;a href=&quot;https://www.dropbox.com/sh/pn01awfz7huys45/AABOEz3hJ2FLuhUkfjrsJs3Fa?dl=0&quot; target=&quot;_blank&quot;&gt; Slide&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;Classification-based Long-term LiDAR Localization&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;ceus19&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A new 3D space syntax metric based on 3D isovist capture in urban space using remote sensing technology&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Ayoung Kim, Youngchul Kim$^{†}$. (†: joint work with KAIST Urban Design Lab)&lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Computer, Environment and Urban Systems, 2019 — &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0198971518301881&quot; target=&quot;_blank&quot;&gt; Paper (online)&lt;/a&gt; &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;LiDAR-based Urban Site Analysis&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;conferences&quot;&gt;Conferences&lt;/h2&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;icra22&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LT-mapper: A Modular Framework for LiDAR-based Lifelong Mapping&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot; id=&quot;iros20&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Ayoung Kim. &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ICRA 2022 (Accepted, To appear) —  
    &lt;a href=&quot;/publications/gkim-2021-ltmapper.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;,  
    &lt;a href=&quot;https://github.com/gisbi-kim/lt-mapper&quot; target=&quot;_blank&quot;&gt; Code&lt;/a&gt;,
    &lt;a href=&quot;https://youtu.be/vlYKfp1p2j8&quot; target=&quot;_blank&quot;&gt; Video &lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;Automatic LiDAR Map Management&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;iceic22&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SC-LiDAR-SLAM: a Front-end Agnostic Versatile LiDAR SLAM System&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot; id=&quot;iros20&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Seungsang Yun, Jeongyun Kim, and Ayoung Kim. &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ICEIC 2022 —  
    &lt;a href=&quot;/publications/gkim-2022-sclidarslam.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;,
    &lt;a href=&quot;https://youtu.be/XA6Q24GHQO0&quot; target=&quot;_blank&quot;&gt; Video&lt;/a&gt;,
    &lt;a href=&quot;/publications/gkim-2022-sclidarslam-poster.pdf&quot; target=&quot;_blank&quot;&gt; Poster&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;A set of Open source LiDAR SLAM systems.&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;iros20&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Remove, then Revert: Static Point cloud Map Construction using Multiresolution Range Images&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot; id=&quot;iros20&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Ayoung Kim. &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; IROS 2020 —  
    &lt;a href=&quot;/publications/gkim-2020-iros.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;,  
    &lt;a href=&quot;https://github.com/irapkaist/removert&quot; target=&quot;_blank&quot;&gt; Code&lt;/a&gt;,
    &lt;a href=&quot;https://www.youtube.com/watch?v=M9PEGi5fAq8&quot; target=&quot;_blank&quot;&gt; Video&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;Remove Dynamic Points in the Wild&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;icra20undeeplo&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Unsupervised Geometry-Aware Deep LiDAR Odometry&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Younggun Cho, &lt;u&gt;Giseop Kim&lt;/u&gt;, Ayoung Kim. &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ICRA 2020 —  
    &lt;a href=&quot;/publications/ycho-2020-icra.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;,  
    &lt;a href=&quot;https://www.youtube.com/watch?v=-imRJXq6ZuE&quot; target=&quot;_blank&quot;&gt; Video&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;Learning LiDAR Odometry without Ground-truth&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;icra20mulran&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;MulRan: Multimodal Range Dataset for Urban Place Recognition&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Yeong Sang Park, Younghun Cho, Jinyong Jeong, Ayoung Kim. &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ICRA 2020 —  
    &lt;a href=&quot;/publications/gkim-2020-icra.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;,  
    &lt;a href=&quot;https://sites.google.com/view/mulran-pr/home&quot; target=&quot;_blank&quot;&gt; Dataset website&lt;/a&gt;,
    &lt;a href=&quot;https://github.com/irapkaist/scancontext/tree/master/fast_evaluator_radar&quot; target=&quot;_blank&quot;&gt; Code (radar place recognition)&lt;/a&gt;,
    &lt;a href=&quot;https://www.youtube.com/watch?v=qJi1KJmrM2U&quot; target=&quot;_blank&quot;&gt; Video&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;LiDAR+Radar SLAM Dataset and Radar Place Recognition&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;icra18sc&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Scan Context: Egocentric Spatial Descriptor for Place Recognition within 3D Point Cloud Map&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Ayoung Kim. &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; IROS 2018 —  
    &lt;a href=&quot;/publications/gkim-2018-iros.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;,  
    &lt;a href=&quot;https://github.com/irapkaist/scancontext&quot; target=&quot;_blank&quot;&gt; Code&lt;/a&gt;,
    &lt;a href=&quot;https://www.youtube.com/watch?v=_etNafgQXoY&quot; target=&quot;_blank&quot;&gt; Video&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;A LiDAR Place Recognition: Robust, Fast, and Versatile&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;workshops&quot;&gt;Workshops&lt;/h2&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;icra19wsjang&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CNN-based Approach for Opti-Acoustic Reciprocal Feature Matching&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Hyesu Jang, &lt;u&gt;Giseop Kim&lt;/u&gt;, Yeongjun Lee$^{†}$, Ayoung Kim. (†: joint work with KRISO)&lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ICRA 2019 Workshop on Underwater Robotics Perception —  
    &lt;a href=&quot;/publications/hsjang-2019-icra-ws.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;A preliminary report of &lt;a href=&quot;#kros20&quot;&gt;the KRoS20&lt;/a&gt; paper&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;icra18ws&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Learning Scan Context toward Long-term LiDAR Localization&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Byungjae Park$^{†}$, Ayoung Kim. (†: joint work with ETRI)&lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ICRA 2018 Workshop on Long-term autonomy  (won the &lt;a href=&quot;https://blockchair.com/bitcoin/transaction/7d23c8a6b6ea6c4acc3d6625cfb0aa5d8b91e6ea873a551f306fe17cb1ffa144#o=1&quot; target=&quot;_blank&quot;&gt; Best paper award&lt;/a&gt;) —  
    &lt;a href=&quot;/publications/gkim-2018-icraws.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;A preliminary report of &lt;a href=&quot;#ral19&quot;&gt;the RAL19&lt;/a&gt; paper&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;domestic-korean&quot;&gt;Domestic (Korean)&lt;/h2&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;kros20&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CNN-based Opti-Acoustic Transformation for Underwater Feature Matching&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Hyesu Jang, Yeongjun Lee$^{†}$, &lt;u&gt;Giseop Kim&lt;/u&gt;, Ayoung Kim. (†: joint work with KRISO)&lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Journal of Korea Robotics Society 2020 (Special Issue: Underwater Robotics) —  
    &lt;a href=&quot;http://jkros.org/_common/do.php?a=full&amp;amp;b=33&amp;amp;bidx=2176&amp;amp;aidx=26014&quot; target=&quot;_blank&quot;&gt; Paper (online)&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;Camera-sonar style transfer-based multimodal matching&quot;  &lt;br /&gt;
&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Latest News</title>
   <link href="http://localhost:4000/latestnews/"/>
   <updated>2021-01-01T00:00:00+09:00</updated>
   <id>http://localhost:4000/latestnews</id>
   <content type="html">&lt;h1 id=&quot;2022&quot;&gt;2022&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Feb 2021: The &lt;a href=&quot;/publications/#iceic22&quot;&gt; SC-LiDAR-SLAM paper &lt;/a&gt; is accepted for and presented at ICEIC 2022.&lt;/li&gt;
  &lt;li&gt;Jan 2021: The &lt;a href=&quot;/publications/#icra22&quot;&gt; LT-mapper paper &lt;/a&gt; is accepted for ICRA 2022.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;2021&quot;&gt;2021&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Dec 2021: I joined NAVER LABS as a research engineer!&lt;/li&gt;
  &lt;li&gt;Nov 2021: I successfully defended my Ph.D. dissertation presentation!&lt;/li&gt;
  &lt;li&gt;Sep 2021: The &lt;a href=&quot;/publications/#j20sc2&quot;&gt; Scan Context++ paper &lt;/a&gt; is accepted for T-RO journal.&lt;/li&gt;
  &lt;li&gt;May 2021: I had a talk at the &lt;a href=&quot;https://sites.google.com/view/radar-robotics/home?authuser=0&quot;&gt;ICRA21 radar workshop&lt;/a&gt; and introduced radar data and MulRan dataset &lt;a href=&quot;https://youtu.be/jFvKW9Kj8Ts&quot;&gt;[talk video]&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Mar 2021: I passed the Ph.D. thesis proposal presentation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;2020&quot;&gt;2020&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Dec 2020: I released &lt;a href=&quot;https://github.com/gisbi-kim/SC-LIO-SAM&quot;&gt; LiDAR-inertial SLAM code (SC-LIO-SAM: Scan Context + LIO-SAM, C++)&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Nov 2020: I released &lt;a href=&quot;https://github.com/irapkaist/removert&quot;&gt; Removert code (C++)&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Oct 2020: I visited SOSLAB and gave a &lt;a href=&quot;https://www.dropbox.com/s/p30dpjqjbjhq69x/20201022%20SOSLAB%20talk%20Giseop%20Kim%20share.pdf?dl=0&quot;&gt; talk (slide) &lt;/a&gt; about LiDAR place recognition and SLAM.&lt;/li&gt;
  &lt;li&gt;Oct 2020: I presented &lt;a href=&quot;https://www.youtube.com/watch?v=V6OcdNVQRwg&amp;amp;t=231s&quot;&gt; Removert &lt;/a&gt; at (virtual) IROS 2020. 
&lt;!-- - May 2020: I had logged and finished MulRan-extended dataset for Sejong City during last 1 year. --&gt;&lt;/li&gt;
  &lt;li&gt;Jul 2020: I finished the CE481 (a.k.a SLAM 101) TA, Spring 2020.&lt;/li&gt;
  &lt;li&gt;Jun 2020: The &lt;a href=&quot;/publications/#iros20&quot;&gt; Removert paper &lt;/a&gt; is accepted for IROS 20.&lt;/li&gt;
  &lt;li&gt;Jun 2020: I presented &lt;a href=&quot;/publications/#icra20mulran&quot;&gt; MulRan dataset paper &lt;/a&gt; at (virtual) ICRA 2020. 
&lt;!-- - May 2020: I had logged and finished MulRan-extended dataset for Sejong City during last 1 year. --&gt;&lt;/li&gt;
  &lt;li&gt;May 2020: I made a portable backpack sensor system (call Jige) with lab members.&lt;/li&gt;
  &lt;li&gt;Apr 2020: I released &lt;a href=&quot;https://github.com/gisbi-kim&quot;&gt; Scan Context C++ and SC-LeGO-LOAM project codes &lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Jan 2020: I visited NAVER LABS and gave a talk about structural place recognition.&lt;/li&gt;
  &lt;li&gt;Jan 2020: I passed the Ph.D. qualification exam of the CEE dept, KAIST.&lt;/li&gt;
  &lt;li&gt;Jan 2020: The &lt;a href=&quot;/publications/#icra20undeeplo&quot;&gt; UndeepLO paper &lt;/a&gt; is accepted for ICRA 20 (I participated as a 2nd author and did comparison experiments).&lt;/li&gt;
  &lt;li&gt;Jan 2020: The &lt;a href=&quot;/publications/#icra20mulran&quot;&gt; MulRan dataset paper &lt;/a&gt; is accepted for ICRA 20.&lt;/li&gt;
  &lt;li&gt;Jan 2020: Our team is selected for Bilateral Research Exchange Support Program (2 years), co-work with prof. Tomas Krajnik.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;2019&quot;&gt;2019&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Oct 2019: I visited ICCV 2019 and met people of SLAM-Korea FB group.&lt;/li&gt;
  &lt;li&gt;Oct 2019: I released Scan Context Python and &lt;a href=&quot;https://github.com/gisbi-kim/PyICP-SLAM&quot;&gt; PyICP-SLAM project code&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Jun 2019: I initiated MulRan (Radar + LiDAR dataset) project and first used Ouster LiDAR. During this project, I was advised &lt;a href=&quot;http://jinyongjeong.github.io/about/&quot;&gt; Dr. Jinyong Jeong &lt;/a&gt; to build and extend a car-platform sensor system.&lt;/li&gt;
  &lt;li&gt;May 2019: I visited Montreal, Canada, and had a presentation of the RA-L paper at ICRA 2019 (my first 2-hour interaction session).&lt;/li&gt;
  &lt;li&gt;May 2019: I was awarded ICRA 2019 RAS Travel Grant.&lt;/li&gt;
  &lt;li&gt;Apr 2019: I visited Czech Republic to start a co-work with &lt;a href=&quot;http://labe.felk.cvut.cz/~tkrajnik/&quot;&gt;prof. Tomas Krajnik&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Mar 2019: I officially started to study as a Ph.D student.&lt;/li&gt;
  &lt;li&gt;Feb 2019: The &lt;a href=&quot;/publications/#ceus19&quot;&gt; urban analysis journal paper &lt;/a&gt; was accepted and published.&lt;/li&gt;
  &lt;li&gt;Jan 2019: I developed the &lt;a href=&quot;/publications/#icra18ws&quot;&gt; ICRA18-workshop paper &lt;/a&gt;  with Dr. Byungjae Park of ETRI, and it was accepted at &lt;a href=&quot;/publications/#ral19&quot;&gt; RA-Letter with ICRA19 presentation optioin &lt;/a&gt;. (I had my first robotics journal rebuttal process)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;2018&quot;&gt;2018&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Dec 2018: I successfully defended my &lt;a href=&quot;/publications/gkim-dissertation-ms.pdf&quot; target=&quot;_blank&quot;&gt; MS thesis&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Oct 2018: I visited University of Girona for 2 weeks to study underwater SLAM.&lt;/li&gt;
  &lt;li&gt;Oct 2018: I had a oral presentation of &lt;a href=&quot;/publications/#icra18sc&quot;&gt; LiDAR place recognition paper (Scan Context) &lt;/a&gt; paper at IROS 18, Madrid, Spain.&lt;/li&gt;
  &lt;li&gt;Jul 2018: I made a bike sensor system from scratch for Venture Research Program.&lt;/li&gt;
  &lt;li&gt;Jun 2018: My &lt;a href=&quot;/publications/#icra18sc&quot;&gt; LiDAR place recognition paper &lt;/a&gt; was accepted at IROS 2018.&lt;/li&gt;
  &lt;li&gt;Jun 2018: I was selected for KAIST College of Engineering’s MS/Ph.D student Venture Research Program.&lt;/li&gt;
  &lt;li&gt;May 2018: I visited Brisbane, Australia. I had my first oral presentation at an international conference (ICRA 2018), and &lt;a href=&quot;https://blockchair.com/bitcoin/transaction/7d23c8a6b6ea6c4acc3d6625cfb0aa5d8b91e6ea873a551f306fe17cb1ffa144#o=1&quot; target=&quot;_blank&quot;&gt; won the best paper award&lt;/a&gt;) of the Long-term Autonomy workshop.&lt;/li&gt;
  &lt;li&gt;Apr 2018: The &lt;a href=&quot;/publications/#icra18ws&quot;&gt; long-term localization paper &lt;/a&gt; was accepted at a ICRA 18 workshop.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;2017&quot;&gt;2017&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Dec 2017: I got a my first oral presentation (non-published) at RiTA 2017.
&lt;!-- - Aug 2017: I had studied about LiDAR-based city spatial analysis and submitted a first journal paper (non-robotics).   --&gt;&lt;/li&gt;
  &lt;li&gt;Jul 2017: I interned at ETRI and was advised by &lt;a href=&quot;https://sites.google.com/site/sunglok/&quot;&gt; Dr. Sunglok Choi &lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;May 2017: I attended ICRA 17 at Singapore and had my first ever poster session.&lt;/li&gt;
  &lt;li&gt;Mar 2017: I was accepted at a 1 page abstract for ICRA 2017 late-braking poster session.&lt;/li&gt;
  &lt;li&gt;Mar 2017: I officially joined Intelligent Robotic Autonomy and Perception (IRAP) lab, KAIST, as MS student, advised by &lt;a href=&quot;https://irap.kaist.ac.kr/~ayoung/index.html&quot;&gt;prof. Ayoung Kim &lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;2016&quot;&gt;2016&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Oct 2016: I visited IROS 2016 (Daejeon, South Korea).&lt;/li&gt;
  &lt;li&gt;Sep 2016: I started a undergraduate student researcher at IRAP lab.&lt;/li&gt;
  &lt;li&gt;Jun 2016: I finished the TA of CE352 (signal processing, lectured by prof. Ayoung Kim), Spring 2016.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>About Me (detail)</title>
   <link href="http://localhost:4000/aboutmedetail/"/>
   <updated>2021-01-01T00:00:00+09:00</updated>
   <id>http://localhost:4000/aboutmedetail</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;Please see this &lt;a href=&quot;http://bit.ly/gk_profile&quot;&gt; personal notion page  &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>About Me</title>
   <link href="http://localhost:4000/aboutme/"/>
   <updated>2021-01-01T00:00:00+09:00</updated>
   <id>http://localhost:4000/aboutme</id>
   <content type="html">&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;

&lt;p style=&quot;font-size:14px;&quot;&gt;
I obtained a Ph.D. (2022) at KAIST, South Korea. I was a member of the IRAP lab (currently moved to &lt;a href=&quot;https://rpm.snu.ac.kr/&quot; target=&quot;_blank&quot;&gt;RPM Robotics Lab&lt;/a&gt;, SNU), and advised by Prof. &lt;a href=&quot;https://ayoungk.github.io/&quot; target=&quot;_blank&quot;&gt; Ayoung Kim&lt;/a&gt;. 
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:14px;&quot;&gt;
My research topic during Ph.D. had been focused on &lt;a href=&quot;https://bit.ly/3HbkjEH&quot; target=&quot;_blank&quot;&gt;LiDAR-based robust 3D mapping in complex urban sites&lt;/a&gt;. I have experience with a full pipeline of robotic localization and mapping in real-world (from 3D perception, sensor fusion, calibration, odometry, place recognition, pose-graph optimization, multi-session localization, to long-term map management). 
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:14px;&quot;&gt;
I am currently a research engineer at &lt;a href=&quot;https://www.naverlabs.com/&quot; target=&quot;_blank&quot;&gt;NAVER LABS&lt;/a&gt;. I'm focusing on developing robust and scalable localization and mapping of autonomous vehicles in a complex city.
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:14px;&quot;&gt;
Please see this &lt;a href=&quot;http://bit.ly/gk_profile&quot; target=&quot;_blank&quot;&gt; notion page&lt;/a&gt; for more details about me.
&lt;/p&gt;

&lt;!-- &lt;p style=&quot;font-size:11px;&quot; align=&quot;right&quot;&gt;
  last update: 2022-02-06 
&lt;/p&gt; --&gt;
</content>
 </entry>
 
 
</feed>

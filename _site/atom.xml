<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>The blog of Giseop Kim</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000"/>
 <updated>2021-03-11T01:50:30+09:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Giseop Kim</name>
   <email>paulgkim@kaist.ac.kr</email>
 </author>

 
 <entry>
   <title>Bayesian Filtering 이야기 (1편): posterior 의 mean, covariance 구하기 (수식 유도)</title>
   <link href="http://localhost:4000/blog/2021/03/09/bayesfiltering-1.html"/>
   <updated>2021-03-09T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/03/09/bayesfiltering-1</id>
   <content type="html">&lt;h1 id=&quot;개요&quot;&gt;개요&lt;/h1&gt;

&lt;p&gt;SLAM은 filtering-based 와 optimization-based 로 편의상 나눌 수 있다.&lt;br /&gt;
(물론 두 방법이 robot 의 state 를 추정하는데 상보적으로 동시에 쓰일수도 있다.)&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:gray&quot;&gt; [Factor graph-based SLAM] &lt;/span&gt; 에서는&lt;br /&gt;
optimization-based 기반의 SLAM에 대해서 소개하고 있다.&lt;/p&gt;

&lt;p&gt;한편,  &lt;span style=&quot;color:gray&quot;&gt; [Filtering-based SLAM] &lt;/span&gt; 시리즈에서는&lt;br /&gt;
filtering-based SLAM 에 대해서 소개하려고 하는데,&lt;br /&gt;
이 포스트 (와 몇 편 더) 에서는, 그전에 먼저&lt;br /&gt;
filtering-based SLAM의 근간이 되는 Bayesian analysis 자체에 대해 소개하려고 한다.&lt;br /&gt;
mathmatical machinary 랄까, 기본을 알면 SLAM은 오히려 쉬울 것이다. 
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;bayesisan-analysis-란&quot;&gt;Bayesisan Analysis 란?&lt;/h1&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;p&gt;Bayesisan Analysis 이 무엇인지 설명하기 전에,&lt;br /&gt;
Bayesisan Analysis 이 왜 필요한지, 예시 상황으로부터 시작해보자.&lt;/p&gt;

&lt;p&gt;우리는 어떤 한 시점에서 로봇의 위치를 알고싶다.&lt;br /&gt;
이 경우 위치를 $\textbf{x}_{i} = (x, y)$ 의 두 값으로 표현할 수 있다. (heading 은 편의상 생략)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;상황 1:
    &lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
    &lt;p&gt;로봇은 위치를 추정하기 위해서 GPS를 사용했다. &lt;br /&gt;
GPS는 로봇의 위치가 $(1,2)$ 라고 알려주었다 (measurement, $h_1(\cdot)$).&lt;/p&gt;

    &lt;p&gt;그래서 로봇은 자기의 위치를 $(1,2)$ 이라고 생각하고 여러 미션을 수행하려고 한다.&lt;/p&gt;

    &lt;p&gt;근데 GPS는 로봇의 위치가 다시 $(1.5, 3)$ 라고 알려주었다 (measurement 2, $h_2(\cdot)$).&lt;br /&gt;
그래서 로봇은 자기의 위치를 다시 $(1.5, 3)$ 으로 바꿨다.&lt;br /&gt;
근데 GPS는 로봇의 위치가 다시 $(2.5, 1)$ 라고 알려주었다 (measurement 3, $h_3(\cdot)$).&lt;br /&gt;
그래서 로봇은 자기의 위치를 다시 $(2.5, 1)$ 으로 바꿨다.&lt;/p&gt;

    &lt;p&gt;… 무한반복 …&lt;br /&gt;
로봇은 확신없이 자기 위치를 결국 추정하지 못하고 결국 세월이 다갔다 …&lt;/p&gt;

    &lt;p&gt;이렇게, 매번 개별 measurement만의 가능도 (likelihood) 를 최대화 해주는 방식으로&lt;br /&gt;
위치를 추정하는 게 좋은 방법일까?&lt;br /&gt;
이러면 그때그때 들어오는 measurement 에 biased 된 예측이 내려질 것이다&lt;sup id=&quot;fnref:likelihood&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:likelihood&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

  &lt;/li&gt;
&lt;/ul&gt;

&lt;p id=&quot;situ2&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;상황 2:
    &lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
    &lt;p&gt;상황1과 똑같이, 로봇은 위치를 추정하기 위해서 GPS를 사용했다. &lt;br /&gt;
GPS는 로봇의 위치가 $(1,2)$ 라고 알려주었다 (measurement, $h_1(\cdot)$).&lt;/p&gt;

    &lt;p&gt;근데 이 로봇은 자기의 현재 위치가 $(0.5, 2)$ 라고 믿고 있는 상태이다 (prior).&lt;br /&gt;
이 값은 현재 시점에서 측정 (measurement) 한 것은 아니지만 로봇이 알고있는 어떤 믿음이다.&lt;/p&gt;

    &lt;p&gt;로봇은 이 경우에도 상황1에서와 같이 현재 측정된 정보만을 최대화 하도록&lt;br /&gt;
자기의 위치를 추정해야할까?&lt;br /&gt;
흠 근데 그건 좀 unreasonable 해 보인다.&lt;br /&gt;
prior 와 measurement 두 정보를 모두 고려하는 게 좋을 것 같다.&lt;/p&gt;

    &lt;p&gt;근데 만약 prior 와 measurement 두 정보를 모두 고려한다 하더라도,&lt;br /&gt;
수학적으로 어떻게 융합하여야 하는가?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이에 대답하기 전에 먼저 아주 간단한 수학을 recap하고 넘어가자.&lt;/p&gt;

&lt;h2 id=&quot;bayes-rule&quot;&gt;Bayes Rule&lt;/h2&gt;

&lt;p&gt;Bayesisan Analysis 의 기본 틀은 Bayes Rule 이다.&lt;br /&gt;
대학교 확률과 통계 수업을 들으면&lt;br /&gt;
거의 첫주에 배우는 아주 기초적이고, 제일 먼저 나오는 내용이라고 할 수 있다.&lt;br /&gt;
다들 알다시피 Bayes Rule 을 쓰면 다음과 같다.&lt;/p&gt;

\[\begin{align*}
  p(A \ | \ B) &amp;amp;\propto p(A) \cdot p( B \  | \ A) \\ 
\end{align*}\]

&lt;p&gt;그런데 robotics (or 여타 estimation 문제) 에서 실제로 중요한 것은&lt;br /&gt;
위에서 $A$와 $B$의 위치가 바뀌는게 베이즈어쩌구구나~ 음~ 외웠어~ 가 아니라&lt;/p&gt;

&lt;p&gt;$A$와 $B$ 자리에 들어가는 것이,&lt;br /&gt;
&lt;u&gt;실 세계에서 어떤 물리적 의미를 지니는지&lt;/u&gt;를 이해하는 것이 중요하다.&lt;/p&gt;

&lt;h2 id=&quot;bayesisan-analysis-란-1&quot;&gt;Bayesisan Analysis 란?&lt;/h2&gt;

&lt;p&gt;Bayesian analysis 는 우리가 알고싶은 parameter (들, variables as vector)의&lt;br /&gt;
posterior probability 를 maximize 하는&lt;br /&gt;
optimal parameter (그래서 maximum a posteriori, MAP) 를 찾는 방법을 말한다.&lt;/p&gt;

&lt;p&gt;먼 말인지 차근차근 살펴보자.&lt;/p&gt;

&lt;p&gt;우리는 시스템의 어떤 상태에 대해 알고 싶다&lt;br /&gt;
(예: 로봇의 위치, 로봇의 calibration parameter 등 어떤 것이든.)&lt;/p&gt;

&lt;p&gt;이것을 $\textbf{x}$ 라고 해보자 (state).&lt;/p&gt;

&lt;p&gt;우리는 $\textbf{x}$ 를 예측하기 위한 단서들을 가지고 있다 (measurement).&lt;br /&gt;
로봇은 센서를 가지고 있기 때문에 다양한 측정을 할 수 있다.&lt;br /&gt;
(예: laser 센서로 알려진 랜드마크까지의 거리를 재기, GPS로 위치를 바로 얻기 등)&lt;/p&gt;

&lt;p&gt;robotics 에서 measurement 는 문자로 적을 때는 $\textbf{z}$ 로 많이 적는 편이다.&lt;/p&gt;

&lt;p&gt;따라서 우리가 관심있는 SLAM (i.e., state estimation) 이란 문제는,&lt;/p&gt;

&lt;p&gt;로봇이 가진 센서를 이용해서 얻은 다양한 측정값이 주어졌을 때 (given),&lt;br /&gt;
$\textbf{x}$ 를 추론하는 과정 이 된다 (inference).&lt;/p&gt;

&lt;p&gt;이를 수학적으로 표현하면 $p ( \textbf{x} \ | \ \textbf{z}_{1:k} )$ 로 쓸 수 있다.&lt;/p&gt;

&lt;p&gt;중간의 $|$ 는 “given” 으로 읽는다.&lt;br /&gt;
$\textbf{z}$의 subscript 인 $1:k$는 총 k개의 measurement 가 있다고 가정하자, 라는 의미인데 간단히 생략되기도 한다.&lt;br /&gt;
문맥에 따라 때로는 time $= 1$ 부터 $t$ 까지의 측정값, 처럼 time 에 걸쳐 얻은 measurement 로 이해되기도 한다.&lt;/p&gt;

&lt;p&gt;따라서 SLAM (state estimation)이란 무엇인가?&lt;br /&gt;
수학적으로:&lt;br /&gt;
$\textbf{z}$가 given 일 때&lt;br /&gt;
$\textbf{x}$의 확률이 최대가 되는 $\textbf{x}$ 값을 구하는 것이다.&lt;/p&gt;

&lt;p&gt;그런데, Practically, 공학적으로는 어지간하면&lt;br /&gt;
우리는 관심있는 random variable 들 ($\textbf{x}$ 등)이&lt;br /&gt;
모두 Gaussian 이라고 가정한다.&lt;br /&gt;
Gaussian 분포는 여러가지 유용한 성질이 있기 때문이다 (후술함). $+\alpha$&lt;sup id=&quot;fnref:alpha&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:alpha&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;암튼 우리가 최대화하려는 이 확률 $p ( \textbf{x} \ | \ \textbf{z}_{1:k} )$ 은&lt;br /&gt;
measurement 가 측정이 완료되어서, 주어져야 계산할 수 있기 때문에&lt;br /&gt;
사후확률 (posterior) 이라고 부른다.&lt;/p&gt;

&lt;p&gt;그런데 이 posterior는 Bayes rule 에 의해,&lt;br /&gt;
다음과 같이 두개의 텀으로 분리가 되고, 각각을 prior 와 likelihood 라고 부른다.&lt;/p&gt;

\[\begin{align*}
  p(\textbf{x} \ | \ \textbf{z}_{1:k}) &amp;amp;\propto p(\textbf{x}) \cdot p(\textbf{z}_{1:k} \ | \ \textbf{x}) \\ 
  \text{posterior} &amp;amp;\propto \text{posterior} \cdot \text{likelihood}
\end{align*}\]

&lt;p&gt;여기서 보통 분모는 normalization 용도 (확률의 정의를 지켜주기 위해서 sum 을 1로 만들어주는 역할)로 생각되기 때문에, 통상적으로 practically 생략하는 편이 많다. 어짜피 우리가 관심이 있는 것은 최대확률값이 아니라, 최대확률이 될 때의 $\textbf{x}$값이기 때문이다 (상수의 곱에 무관하다).&lt;/p&gt;

&lt;p&gt;이 때 보통 우리는 prior 와 likelihood 가 둘 다 Gaussian 을 따른다고 가정한다.&lt;br /&gt;
그러면 Gaussian 의 좋은(!) 성질 덕분에 posterior 도 Gaussian이 된다.&lt;/p&gt;

&lt;p&gt;여기서 그럼 다시 물어야 할 것이, posterior가 Gaussian인 것이 (state estimation 관점에서) 왜 중요할까?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;일단 Gaussian은 optimal argmax 값을 얻기가 편하다 (median이 mean 과 같음).&lt;/li&gt;
  &lt;li&gt;그리고 Gaussian인 경우, 자연스럽게 recursive 한 estimation 이 가능해진다.&lt;br /&gt;
이게 뭔말이냐면, 현재 턴의 posterior 가 다음턴의 prior 로 쓰인다고 생각해보자.&lt;br /&gt;
앞서, 우리는 prior 가 Gaussian이라고 가정했었다. 그럼 posterior 도 Gaussian이 되고, 다음턴의 prior 도 다시 Gaussian이 되고, 그 턴의 posterior 도 다시 Gaussian이 되고, … 무한 반복.&lt;sup id=&quot;fnref:conju&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:conju&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, 현재 시점에서 얻은 사전 정보를 다음턴에 자연스럽게 활용할 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;심지어 가우시안의 유용한 성질들 덕분에 그 recursive update 조차도 closed form 으로 딱 떨어진다 (곧 증명해본다).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;또한, Gaussian은 mean (optimal) 과 더불어 covariance (uncertainty) 도 얻을 수 있다.&lt;br /&gt;
(robotics 는 거의 uncertainty 에 관한 학문이라고 할 수 있을만큼 이 uncertainty 는 다양하게 쓰이고 중요하다)&lt;br /&gt;
예를 들어 앞서 본 &lt;a href=&quot;#situ2&quot;&gt;예시상황2&lt;/a&gt; 에서 prior 와 likelihood 를 어떻게 융합해야하는지에 관한 문제가 있었다. 이 비율의 balancing 이 prior 와 likelihood 각각의 uncertainty 들에 의해 조절된다 (곧 증명해본다)!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;암튼-정리해보자면&quot;&gt;암튼 정리해보자면&lt;/h2&gt;
&lt;p&gt;우리가 state estimation 에서 Bayesian filtering 이라고 부르는 것은 다음과 같은 상황을 의미한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;로봇은 자신의 위치 (and 속도, 주변 맵 포인트들의 위치, 자신의 내부 상태 등 다양한 어떤 값들) $\textbf{x}$ 를 알고 싶다.&lt;/li&gt;
  &lt;li&gt;로봇은 센서측정데이터 (measurement) $\textbf{z}_{1:k}$ 를 가지고 있다.&lt;/li&gt;
  &lt;li&gt;우리는 $ p(\textbf{x} \ | \ \textbf{z}_{1:k}) $ 를 최대화 하고 싶다.&lt;br /&gt;
== 우리는 posterior probability 의 mean (== 확률을 최대로 하는 $\textbf{x}$) 과 covariance 를 알고 싶다.&lt;/li&gt;
  &lt;li&gt;우리는 posterior 를 직접 최대화하는 것이 아니라, 대신 우회적으로 prior 와 likelihood 의 곱을 최대화 한다.&lt;/li&gt;
  &lt;li&gt;prior 와 likelihood의 분포로 Gaussian 을 사용한다.&lt;/li&gt;
  &lt;li&gt;covariance는 pior 와 likelihood 를 융합할 때 기여도를 고려하는 역할을 한다.&lt;sup id=&quot;fnref:cov&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:cov&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;posterior-의-mean-covariance-구하기&quot;&gt;Posterior 의 mean, covariance 구하기&lt;/h1&gt;

&lt;p&gt;이번 포스팅에서 원래 하려고했던 걸 이제야 소개한다.&lt;/p&gt;

&lt;p&gt;WIP …&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;요약&quot;&gt;요약&lt;/h1&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;예고&quot;&gt;예고&lt;/h2&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;생각해보기&quot;&gt;생각해보기&lt;/h2&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;주석&quot;&gt;주석&lt;/h3&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:likelihood&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://dtransposed.github.io/blog/Bayesian-Linear-Regression.html&quot; target=&quot;_blank&quot;&gt; 이 블로그 &lt;/a&gt;의 gif 예제를 보면 이해하기 쉽다. &lt;a href=&quot;https://dtransposed.github.io/assets/9/batch_1/Data_Space.gif&quot; target=&quot;_blank&quot;&gt; data point들이 sequentially 들어올 때 &lt;/a&gt;, 그때그때마다의 measurement에만 최대로 fit하는 예측을 내리게 된다면 &lt;a href=&quot;https://dtransposed.github.io/assets/9/batch_1/Likelihood.gif&quot; target=&quot;_blank&quot;&gt; 이렇게 들쭉날쭉한 결과를 얻게 될 것이다. &lt;/a&gt; &lt;a href=&quot;#fnref:likelihood&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:alpha&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Factor graph-based SLAM 에서는 Gaussian noise 를 가정할 경우 Probability의 사뭇 추상적이었던 수식이 Least-square optimization 으로 과 동치가 되고, 문제를 iterative optimization으로 풀 수 있다는 장점이 있기도 하다. &lt;a href=&quot;https://link.springer.com/article/10.1023/A:1008854305733&quot;&gt; Lu and Milios 의 1997년 논문&lt;/a&gt; 이 optimization-based SLAM의 시초로 평가받는 듯하다. (하지만 이에 대해서 입문용으로는 Square Root SAM 논문이 제일 정석적이고 좋은 듯) &lt;a href=&quot;#fnref:alpha&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:conju&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;우리는 지금 posterior 가 prior 와 같은 분포 패밀리가 되는 것을 원하고 있었다. Gaussian = Gaussian $\times$ Gaussian 인 조합 외에, conjugate distribution이라고 검색해보면 몇개 더 다양한 분포의 조합이 가능함을 알 수 있다. 하지만 practically, Gaussian 말고 별로 다른 조합은 적어도 SLAM에서는 잘 써본 적이 없다 (== Gaussian에 대해서만 일단 잘 알면 된다). &lt;a href=&quot;#fnref:conju&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:cov&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;또한, covariance 는 uncertainty 의 의미로써 그 자체로도 solely 다양한 application에 활용 될 수 있다. 예를 들어 loop closing 을 수행하기 위해 어느 정도 반경 내의 후보 node 들만 검색하는 등 search space 를 줄여줄 수 있다. &lt;a href=&quot;#fnref:cov&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>SLAM back-end 이야기 (1편): SLAM은 Ax=b 를 푸는 것이다</title>
   <link href="http://localhost:4000/blog/2021/03/04/slambackend-1.html"/>
   <updated>2021-03-04T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/03/04/slambackend-1</id>
   <content type="html">&lt;!-- see using latex here: https://mkkim85.github.io/blog-apply-mathjax-to-jekyll-and-github-pages/ --&gt;
&lt;!-- --- --&gt;

&lt;h1 id=&quot;개요&quot;&gt;개요&lt;/h1&gt;
&lt;p&gt;SLAM은 세상의 모든것 &lt;em&gt;[1. 나 (robot)와 2.너 (world)]&lt;/em&gt; 의&lt;br /&gt;
state (e.g., position, orientation, velocity) 를 예측하는 학문이다.&lt;br /&gt;
그래서 state estimation 이라고 불리기도 한다.&lt;/p&gt;

&lt;p&gt;이런 최적 state 를 예측하는 solver 에 관한 연구를&lt;br /&gt;
SLAM back-end 라고 편의상 부르기도 한다.&lt;/p&gt;

&lt;p&gt;그런데 어떻게 예측할까? 어떻게 최적해를 얻을까?&lt;/p&gt;

&lt;p&gt;(SLAM back-end의 마일스톤 논문인) Square Root SAM 논문&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 에서 말했듯,&lt;br /&gt;
SLAM은 수학적으로 $Ax=b$ 를 푸는 문제이다.&lt;/p&gt;

&lt;p&gt;그래서 SLAM back-end 라고 하면 주로&lt;/p&gt;

&lt;p&gt;어떻게 더 $Ax=b$를&lt;br /&gt;
빠르고 (fast), 정확하고 (accurate), 안정적으로 (numerically stable) 풀 수 있는지에 대한 연구라고 생각하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:gray&quot;&gt; (다음 편들에서 그런 알고리즘들에 대해서 자세히 알아보도록 하고) &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;일단 왜 SLAM이 $Ax=b$ 를 푸는 문제인지 알아보자.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;slam-이란&quot;&gt;SLAM 이란&lt;/h1&gt;

&lt;p&gt;어느 마을에 건설로봇 (SCV)이 있었다.&lt;/p&gt;

&lt;p&gt;얘는 자기가 어디서 온 지 모른다.&lt;/p&gt;

&lt;p&gt;그래서 편의상 자기가 현재 Origin (e.g., [0, 0]) 에 있다고 생각한다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/figs/2021-03-04-slambackend-1/robot1.png&quot; alt=&quot;img1&quot; style=&quot;width:100%&quot; /&gt;
  &lt;figcaption&gt; &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;SCV는 길을 가고 있었다. 로봇인 이상 움직이지 않을 수 없으므로.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/figs/2021-03-04-slambackend-1/robot2.png&quot; alt=&quot;img2&quot; style=&quot;width:100%&quot; /&gt;
  &lt;figcaption&gt; &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;얘는 사이사이마다 얼마만큼 이동한지 (relative motion) 를 측정하는 능력 (&lt;em&gt;odometry&lt;/em&gt;) 이 있어서 (&lt;span style=&quot;color:blue&quot;&gt;파랑 화살표&lt;/span&gt;),&lt;/p&gt;

&lt;p&gt;자연스럽게 $t=2, 3, 4, 5$에서의 위치도 계산할 수 있었다 (localization).&lt;/p&gt;

&lt;p&gt;근데 그 능력 (odometry) 은 엄청 정확하지는 않아서 (sensor noise), &lt;br /&gt;
갈수록 자기가 어디에 있는지 덜 신뢰하게 될 수밖에 없었다 (uncertainty was propagated).&lt;/p&gt;

&lt;p&gt;그래서 얘는 주변의 포토캐논들 (landmark) 을 봐가면서 이동하기 시작했다.&lt;br /&gt;
SCV는 사실 laser 를 장착하고 있어서 포토캐논까지의 거리를 직접적으로 잴 수 있었다 (&lt;span style=&quot;color:orange&quot;&gt;주황 화살표&lt;/span&gt;).&lt;/p&gt;

&lt;figure id=&quot;BN&quot;&gt;
  &lt;img src=&quot;/figs/2021-03-04-slambackend-1/robot3.png&quot; alt=&quot;img3&quot; style=&quot;width:105%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; &lt;a href=&quot;#BN&quot; class=&quot;img3&quot;&gt; Figure: Bayes Net &lt;/a&gt; &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;그래서 파랑색 제약 (motion constraints) 도 다 만족시키는 와중에,&lt;br /&gt;
노란색 제약 (landmark constraints) 들도 다 만족시키도록 한다면&lt;br /&gt;
SCV는 자기의 위치를 조금 더 정확히 추정할 수 있을 것이다.&lt;/p&gt;

&lt;p&gt;통상적으로 파랑색 제약을 생성하는 모델을 motion model,&lt;br /&gt;
노란색 제약을 생성하는 모델을 observation model 이라고 부르긴 하는데&lt;br /&gt;
그냥 여기서는 둘 다 measurement (or factor) 라고 부르자.&lt;/p&gt;

&lt;p&gt;이런 관계 (방향성, directed) 그래프를 Bayes Net 이라고 부른다.&lt;br /&gt;
(ps. Bayes Net에 관한 좋은 강의 추천&lt;sup id=&quot;fnref:fgyoutube&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fgyoutube&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;)&lt;/p&gt;

&lt;p&gt;근데 우리는 그림이 필요한게 아니라 (iSAM2에 가면 필요하다. 언젠가 다음편에서 소개함&lt;sup id=&quot;fnref:link&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:link&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;),&lt;br /&gt;
Algebraically 풀 수 있는, 즉 대수적인 툴이 필요하기 때문에&lt;br /&gt;
이 Bayes Net의 measurement 들을 한 데 다 우겨넣으면&lt;/p&gt;

&lt;p&gt;이렇게 matrix 형태로 표현할 수 있다.&lt;/p&gt;

&lt;figure id=&quot;Axb&quot;&gt;
  &lt;img src=&quot;/figs/2021-03-04-slambackend-1/axb.png&quot; alt=&quot;img4&quot; style=&quot;width:95%&quot; /&gt;
  &lt;figcaption&gt; 
        &lt;center&gt; &lt;a href=&quot;#Axb&quot; class=&quot;img3&quot;&gt; Figure: SLAM system &lt;/a&gt; &lt;/center&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;SCV는 구비한 sensor (laser 등)들과  motion model 과 observation model 을 통해 $\textbf{b}$ 를 직접 측정해서 알고 있다.&lt;/p&gt;

&lt;p&gt;따라서 SCV의 정교한 위치(state) 는&lt;br /&gt;
위의 $Ax=b$ 를 풀어서 나오는 최적해일 것이다 (state estimation).&lt;/p&gt;

&lt;p&gt;덤으로 포토캐논들의 위치도 알 수 있게 된다 (mapping) &lt;del&gt;개이득&lt;/del&gt;.&lt;/p&gt;

&lt;p&gt;우리는 여기서 통상적인 사실 몇 가지를 알 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;matrix $A$는 $m \times n$의 형태인데 통상적으로 $m$이 $n$보다 크다는 것이다 ($m &amp;gt; n$). 이런 상황을 보고 overdetermined system 이라고 부른다. &lt;span style=&quot;color:gray&quot;&gt; ps. 심화과정&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; &lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;matrix $A$는 sparse 하다. 왜냐하면 하나의 measurement 는 적은 수의 entity들 사이의 관계만 규정하기 때문이다 &lt;span style=&quot;color:gray&quot;&gt;(여기서는 두개 사이의 관계들만 예시로 나오고 있지만 물론 당연히 둘 이상일 수 있다. ps. 심화과정&lt;sup id=&quot;fnref:ps2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ps2&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;slam이란-도대체-무엇인가&quot;&gt;SLAM이란 도대체 무엇인가?&lt;/h3&gt;

&lt;p&gt;이제는 명확해졌다!&lt;/p&gt;

&lt;p&gt;SCV는 이 연립방정식 $Ax=b$를 풀어야 한다!!!&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;어떻게-axb를-풀까&quot;&gt;어떻게 Ax=b를 풀까?&lt;/h1&gt;

&lt;p&gt;$Ax=b$를 풀 때, 
A의 inverse (혹은 pseudo inverse) 를 곱해서 바로 (deterministic) $x$ 를 쉽게 구할 수 있지~ (normal equation 이라고 부른다)&lt;br /&gt;
라고 말한다면 그것은 중학교 교과서에서만 가능한 연립방정식 예제에 불과하다…&lt;/p&gt;

&lt;p&gt;실제로는 SCV의 odometry와 laser sensor 가&lt;br /&gt;
정확하지 않기 때문에 (noisy measurements) &lt;del&gt;&lt;em&gt;SCV는 싼 유닛이다&lt;/em&gt;&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;해 (optimal solution) 가 deterministic 하게 존재하지 않을 수 있다. &lt;br /&gt;
즉, 완전히 $Ax == b$ 로 완벽히 같을 수는 없다는 말.&lt;/p&gt;

&lt;p&gt;대신 우리는 $ |Ax - b|_{2}^{2} $ 를 최소화하는 해를 찾게 된다 (least square optimization&lt;sup id=&quot;fnref:lsbook&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:lsbook&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;).&lt;br /&gt;
완벽히 같진 않더라도 비슷은 해지자고.&lt;/p&gt;

&lt;p&gt;이 경우 거의 iterative 하게 푸는 것이 국룰이다.&lt;br /&gt;
즉, optimal 한 $x^{*}$ 를 단번에 찾을 수는 없고, 대신,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$x_{0}$으로부터 출발해서 optimal 한 변화량 $\Delta^{*}$ 을 추정한다음에&lt;/li&gt;
  &lt;li&gt;$x_{\text{next}} = x_{\text{prev}} + \Delta^{*} $ 만큼 업데이트 해주는 방식으로 최적해를 향해 나아간다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;그러면 &lt;em&gt;어떻게 $Ax=b$ 를 풀까?&lt;/em&gt; 라는 문제는&lt;/p&gt;

&lt;p&gt;다시, &lt;strong&gt;&lt;em&gt;어떻게 $A\Delta=b$ 를 풀까?&lt;/em&gt;&lt;/strong&gt; 라는 문제가 된다 &lt;sup id=&quot;fnref:ps3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ps3&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;따라서 SLAM이란, $\underset{\Delta}{\mathrm{argmin}} \ ||A\Delta - b ||_{2}^{2}$ 인 $\Delta$를&lt;br /&gt;
어떻게 (효율적으로 w.r.t time and memory) 찾을까? 에 대답하는 문제이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;center&gt; *** 다음 시간에 계속 ... ***  &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;요약&quot;&gt;요약&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;SLAM back-end 입문으로 Factor graphs for robot perception 책 &lt;sup id=&quot;fnref:fgbook&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fgbook&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; 을 추천합니다. SAM, iSAM, iSAM2 세 논문의 내용을 쉬운 언어로 잘 서술하고 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;color:gray&quot;&gt; ps. 심화과정 – 팩트체크: 근데 사실 b는 … &lt;sup id=&quot;fnref:psb&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:psb&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; &lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;예고&quot;&gt;예고&lt;/h2&gt;
&lt;p&gt;다음으로&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;SLAM back-end 이야기 (2편): $Ax=b$ 풀기&lt;br /&gt;
— QR decomposition 이란? + Householder reflection &lt;em&gt;구현&lt;/em&gt; (Matlab 실습)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;편을 써보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:gray&quot;&gt; 그 다음으로 &lt;del&gt;제발&lt;/del&gt; &amp;lt;SLAM back-end 이야기 (2편): $Ax=b$ 풀기 — Householder reflection &lt;em&gt;이론&lt;/em&gt;&amp;gt; 편을 써보겠습니다. &lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;생각해보기&quot;&gt;생각해보기&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;SCV의 &lt;a href=&quot;#Axb&quot;&gt; SLAM system matrix &lt;/a&gt; 에서 지금 matrix 의 column 순서가 포토캐논부터 적혀져있다. 근데 이거를 SCV부터 적으면, 즉 column order 가 달라지면 estimation 할 때 어떤 점들이 달라질까? 해가 달라질까? 더 빨리 풀 수 있을까? 아니면 아무 상관 없을까?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SCV 는 $t=3$에서 포토캐논 a 와 c를 봤다고 생각했다. 하지만 알고보니 b와 d를 본 것이었다면? 즉 data association 의 outlier 가 존재할 때 SLAM 의 최적해는 어떤 영향을 받을까? 그리고 이런 false association 이 존재함에도 불구하고, 어떻게 해를 더 강건하게 예측할 수 있을까?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$||Ax - b ||_{2}^{2}$ 의 argmin 을 찾는 것은 일반적으로 SLAM외에도 다른 computer vision estimation 문제에도 매우 자주 등장하는 상황이다. SLAM에 있어 위 식을 푸는 것은 다른 computer vision 의 estimation 문제와 어떤점에서 특별하게 다를까?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;주석&quot;&gt;주석&lt;/h3&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Dellaert, Frank, and Michael Kaess. “Square Root SAM: Simultaneous localization and mapping via square root information smoothing.” The International Journal of Robotics Research 25.12 (2006): 1181-1203. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:fgyoutube&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Youtube lectures — &lt;a href=&quot;https://youtube.com/playlist?list=PLOJ3GF0x2_eWtGXfZ5Ne1Jul5L-6Q76Sz&quot;&gt;Factor graphs short course (Jan 2020, UAL)&lt;/a&gt; by Prof. Jose Luis Blanco &lt;a href=&quot;#fnref:fgyoutube&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:link&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;link — To be added &lt;a href=&quot;#fnref:link&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;(1) 실제로는 랜드마크 수가 훨씬 더 수백 수천개만큼 많을 수도 있고 (SfM), 없을 수도 있다 (Pose-graph SLAM) &lt;br /&gt; (2) 실제로는 measurement model들이 대부분 non-linear 하기 때문에 1차미분한 Jacobian matrix H가 쓰인다. 더 엄밀하게는 이 H에 covariance matrix (noise matrix) 의 inverse squared 가 곱해진 형태가 A이다. 자세한 것은 이 책&lt;sup id=&quot;fnref:fgbook:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fgbook&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;의 chapter 2 를 참고. &lt;br /&gt; (3) 최근에는 underdetermined system 일 때 SLAM을 어떻게 풀어야할지에 관한 연구도 이루어지고 있다 – 참고: Fourie, Dehann, et al. “Towards Real-Time Non-Gaussian SLAM for Underdetermined Navigation.” (IROS 2020). &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ps2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;여기서 예측하는 대상을 variable, 그 관계에 대해 factor 라고 부른다. factor 는 수학적으로는 n-ary function이다. 자세한 내용은 이 책&lt;sup id=&quot;fnref:fgbook:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fgbook&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; 의 Chapter 1장 (만 읽어도 됨) 참고. &lt;a href=&quot;#fnref:ps2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:lsbook&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;더 알아보고싶다면, 이 논문을 참고. Grisetti, Giorgio, et al. “Least squares optimization: From theory to practice.” Robotics (2020) &lt;a href=&quot;#fnref:lsbook&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ps3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;그래서 SLAM이 왕왕 &lt;strong&gt;iterative&lt;/strong&gt; non-linear least-square optimization 이라고 불리기도 한다 &lt;a href=&quot;#fnref:ps3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:fgbook&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Dellaert, Frank, and Michael Kaess. “Factor graphs for robot perception.” Foundations and Trends in Robotics (2017) &lt;a href=&quot;#fnref:fgbook&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:fgbook:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:fgbook:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:psb&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;#Axb&quot;&gt; SLAM system matrix figure &lt;/a&gt; 에서 $b$가 마치 measurement 값인것 처럼 일단 소개를 했었었다 (쉬운 이해를 위해). 하지만 사실 b는 prediction error vector이다. 즉 어떤 시점 $i$ 에서, measurement model 을 이용해서 예측된 (우리가 그 값일 거라고 기대하는) measurement $h_{i}(X_{i}^{o})$ 와 실제로 얻은 measurement 값 $z_{i}$ 의 차이가 $b$ vector 가 된다. 즉, $|z_{i} - h_{i}(X_{i}^{o})|$ 가 $b$ vector인 것이다. 더 엄밀하게는 prediction error vector에 whitening 까지 된 것이 $b$ vector가 되는 것인데, 자세한 내용은 Factor Graph Book&lt;sup id=&quot;fnref:fgbook:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fgbook&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; 의 챕터 2.3 을 참고. &lt;a href=&quot;#fnref:psb&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>SLAM의 뿌리를 찾아서</title>
   <link href="http://localhost:4000/blog/2021/03/02/slam-root.html"/>
   <updated>2021-03-02T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/03/02/slam-root</id>
   <content type="html">&lt;h3 id=&quot;slam의-뿌리를-찾아서-&quot;&gt;&lt;strong&gt;SLAM의 뿌리&lt;/strong&gt;를 찾아서 ...&lt;/h3&gt;
&lt;p&gt;떠나봅시다. &lt;/p&gt;
&lt;p&gt;NOTE: 이 글은 예전에 미디엄에 올린 &amp;lt;SLAM을 SLAM답게 만드는 건 무엇일까?: 내맘대로 SLAM 정의하기&amp;gt; 의 후속편 같은 느낌으로 적어보았습니다. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;참고: &amp;lt;SLAM을 SLAM답게 만드는 건 무엇일까?: 내맘대로 SLAM 정의하기&amp;gt;&lt;br /&gt;
&lt;a href=&quot;http://bit.ly/define-slam-myself-1&quot;&gt;http://bit.ly/define-slam-myself-1&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;SLAM의 본질이 어디에 있는지 파악하고&lt;br /&gt;
하나씩 끝말잇기 하듯이&lt;br /&gt;
그 다음 뿌리를 향해 나아가봅시다. &lt;/p&gt;
&lt;p&gt;레츠고 ...&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;1&quot;&gt;1.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 &lt;strong&gt;&lt;em&gt;Estimation&lt;/em&gt;&lt;/strong&gt; 이다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;따라서 MLE, MAP 와 같은 이야기가 나온다. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;더 나아가 SLAM문제를 Gaussian noise 기반으로 probabilistic 하게 formulating 하는 건 거의 시초라고도 (이자 SLAM문제의 코어) 할 수 있다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; R. C. Smith and P. Cheeseman. On the representation and estimation of spatial uncertainty. IJRR. 1986. &lt;/li&gt;
&lt;li&gt; Smith, Self, and Cheesemans, Estimating Uncertain Spatial Relationships in Robotics, 1990 &lt;/li&gt;
&lt;li&gt; filtering 으로 풀든 optimization으로 풀든 공통으로 해당되는 중요한 내용이다. &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;2&quot;&gt;2.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 &lt;strong&gt;&lt;em&gt;State&lt;/em&gt;&lt;/strong&gt; estimation 이다. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;여기서 state란 어떤 것이든 될 수 있지만, 통상적으로 우리는 알고싶은 것은 robot 과 world (map) 의 pose (즉, position + rotation) 이다. &lt;ul&gt;
&lt;li&gt; 근데 state estimation 은 1. filtering 기반, 2. optimization 기반으로 나눌 수 있는데, 특히 이 글에서는 optimization기반의 SLAM의 뿌리에 대해 찾아가보자 ... &lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;물론 velocity, bias, semantic class 등 추론하고 싶은 어떤 것도 state가 될수는 있다 (사실 state라는 것은 수학적으로 확률변수-randome variable- 이기 때문에!). &lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;3&quot;&gt;3.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 &lt;strong&gt;&lt;em&gt;Optimization-based&lt;/em&gt;&lt;/strong&gt; state estimation 이다. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;iteratively 해를 구하는 것이 골자. &lt;ul&gt;
&lt;li&gt; 특히 SLAM은 overdetermined system (measurement 수가 variable 수보다 많음) 이기 때문에 closed form으로 해를 구할 수 없어서 iterative optimization으로 답을 구해야 한다.  &lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;그래서 기본적으로 SLAM공부에 있어 (computer vision과 마찬가지로) GN, LM 등이 빠질 수 없다. &lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;4&quot;&gt;4.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 &lt;strong&gt;&lt;em&gt;Nonlinear&lt;/em&gt;&lt;/strong&gt; optimization-based state estimation 이다. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;센서데이터의 measurement model 이 주로 nonlinear 하기 때문&lt;/li&gt;
&lt;li&gt;따라서 linearize, Jacobian 등의 말이 많이 등장한다. &lt;/li&gt;
&lt;li&gt;이 정의와 같은 의미로 Factor graph-based SLAM 이다~ 라는 말도 자주 쓰인다. &lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;5&quot;&gt;5.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 &lt;strong&gt;&lt;em&gt;Sparse&lt;/em&gt;&lt;/strong&gt; nonlinear optimization-based state estimation 이다. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;SLAM에서는 하나의 measurement 에는 적은 수의 variable 만이 관여하기 때문. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; 예를 들어, 아주 많은 수의 node들이 system에 있다고 하더라도 하나의 odometry measurement는 직전 node와 현재 node 만이 관여한다. 따라서 전체 measurement block들을 쌓은 Jacobian matrix 는 매우 sparse 해진다. &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;또한, SLAM에서는 알고싶은 state (variable) 의 개수 보다 measurement 의 수가 훨씬 많기 때문에 더 sparse 해진다. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;따라서 sparsity 를 잘 이용하기 위해서 variable re-ordering 같은 이야기가 나온다. 여기 5번 항목까지가 Square-root SAM 논문의 내용이라고 봐도 무방하다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; Frank Dellaert, and Michael Kaess. &quot;Square Root SAM: Simultaneous localization and mapping via square root information smoothing.&quot;, IJRR 2006&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;6&quot;&gt;6.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 sparse nonlinear optimization-based state estimation &lt;strong&gt;&lt;em&gt;in an incremental setting&lt;/em&gt;&lt;/strong&gt; &amp;nbsp; 이다. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;센서데이터는 순차적으로 들어오기 때문에, 이전 계산 값을 이용해야 시간적으로 효율적이다. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;또한 SLAM은 nonlinear problem 이기 때문에 이전 계산 값을 이용해야 local minima 에 빠지는 것을 방지할 수 있어 성능적으로 효과적이다. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; 여기까지가 isam1 의 내용이다 (08 TRO iSAM: Incremental smoothing and mapping)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;7&quot;&gt;7.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 sparse nonlinear optimization-based state estimation in an incremental setting &lt;strong&gt;&lt;em&gt;by connecting between graphical model and sparse linear algebra perspective&lt;/em&gt;&lt;/strong&gt; &amp;nbsp; 이다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Squared Root SAM및 isam 까지는 실제 solver 는 선형대수적으로 풀었지만, Bayes tree기반의 isam2로 넘어올 수 있었던 것은 matrix 가 실제로 graphically 어떤 의미를 가지는 지를 (Kaess and Dellaert‬님께서) 이해하고 있었기 때문임.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Information matrix 는 그 의미가 사실 MRF이고, ... 이런 것에 대한 이야기들. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; 여기까지가 isam2 의 내용이다 (12 IJRR iSAM2: Incremental Smoothing and Mapping Using the Bayes Tree). 또한 Factor Graphs for Robot Perception 책에서도 잘 서술되어 있다. &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;8&quot;&gt;8.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 sparse nonlinear optimization-based state estimation in an incremental setting by connecting between graphical model and sparse linear algebra perspective &lt;strong&gt;&lt;em&gt;on a manifold space&lt;/em&gt;&lt;/strong&gt; &amp;nbsp; 이다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;SLAM에서 관심있는 state는 거의 pose인데 여기서 rotation 부분이 nonlinear 함. 따라서 해를 update 하는 공간을 manifold 로 삼을 필요가 있음.   &lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;9&quot;&gt;9.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 sparse nonlinear optimization-based state estimation in an incremental setting by connecting between graphical model and sparse linear algebra perspective on a manifold space &lt;strong&gt;&lt;em&gt;by integrating multiple sensors’ data&lt;/em&gt;&lt;/strong&gt; &amp;nbsp; 이다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;front-end에 관한 이야기이다. &lt;/li&gt;
&lt;li&gt;SLAM은 수중, 지하, 지상, 공중 등 다양한 환경에서 로봇이 활동(navigation)하는 것에 모두 관심사가 있음. 그런데 환경의 특성마다 적합한 센서가 다르고 (수중은 Sonar센서 등) 이들을 퓨전해야 할 필요가 있음. &lt;ul&gt;
&lt;li&gt; 따라서 SLAM 엔지니어는 어떤 센서가 본인의 로봇이 활동하고자 하는 환경에 적합한지 이해하고, 선정하고, 융합할 수 있어야 함. &lt;/li&gt;
&lt;li&gt; 특히 카메라 센서 기반의 SLAM을 하는경우 multiple-view geometry를 공부해야 하는데 그 이유가 여기 9번에 해당하는 내용. &lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;10&quot;&gt;10.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 sparse nonlinear optimization-based state estimation in an incremental setting by connecting between graphical model and sparse linear algebra perspective on a manifold space by integrating multiple sensors’ data &lt;strong&gt;&lt;em&gt;with robust data associations&lt;/em&gt;&lt;/strong&gt; &amp;nbsp; 이다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;front-end 에서는 data association (DA) 이 아닌 것이 없다. local feature matching 부터 loop detection 까지, 결국 모든 것이 이거랑 제일 가까운 애가 누구냐? 에 대답하는 문제. &lt;/li&gt;
&lt;li&gt;근데 이게 100% 정확할 수 없기 때문에 두 가지 방향에서의 노력이 요구된다. &lt;ul&gt;
&lt;li&gt;먼저 그럼에도 더 실수없이 잘 할 수 있도록 더 좋은 DA를 해야하고, &lt;/li&gt;
&lt;li&gt;두 번째로 DA에 실수가 있음에도 불구하고, solution이 망가지지 않도록 back-end에서 강건하게 막아줄 수 있어야 한다. MIT의 Luca Carlone 교수님이 이 분야에서 최근 많은 연구를 하고 있다. &lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;11&quot;&gt;11.&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;SLAM은 sparse nonlinear optimization-based state estimation in an incremental setting by connecting between graphical model and sparse linear algebra perspective on a manifold space by integrating multiple sensors’ data with robust data associations, &lt;strong&gt;&lt;em&gt;for multiple robots&lt;/em&gt;&lt;/strong&gt; &amp;nbsp; 이다. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;robot mission 이 결국 multi-robot 이 팀을 이루어 미션을 수행하는 경우가 많음. &lt;/li&gt;
&lt;li&gt;멀티 로봇의 경우, 또 communication, coordinate 등 다양한 것들이 고려되어야 함. &lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;12-to-be-added-tba-&quot;&gt;12. To be added (TBA) ...&lt;/h4&gt;
&lt;hr /&gt;

&lt;p&gt;대충 여기까지 와보았습니다.&lt;br /&gt;
깊고도 넓은 SLAM의 세계!&lt;br /&gt;
그럼 20000!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>SegMap 빌드하기</title>
   <link href="http://localhost:4000/blog/2021/03/02/segmap-build.html"/>
   <updated>2021-03-02T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/03/02/segmap-build</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;NOTE: &lt;a href=&quot;https://gisbi.medium.com/segmap-%EB%B9%8C%EB%93%9C%ED%95%98%EA%B8%B0-220d6d9b4ef6&quot;&gt;미디엄 블로그&lt;/a&gt; 이전중입니다&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;목표&quot;&gt;목표&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;segmap 을 빌드해보자.&lt;/li&gt;
  &lt;li&gt;작업환경: Ubuntu18.04, ROS 는 root (not virtual)로 설치한 상황이며 나머지 디펜던시는 모두 virtualenv에서 진행 (아래에 자세히 설명함)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;배경지식&quot;&gt;배경지식&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;원래는 segmatch라는 리포였다가 (17 ICRA 시절) 이름이 segmap (18 RSS 시절) 으로 바뀌었다.&lt;/li&gt;
  &lt;li&gt;segmap 공식 리포 (ethz-asl/segmap) 의 경우 빌드하려면 딥러닝 기반 segmap말고 안딥러닝인 segmatch를 쓰고 싶어도 tensorflow_ros_cpp 를 무적권 빌드해야해서 너무 불키하다.&lt;/li&gt;
  &lt;li&gt;그래서 인터넷에 segmatch 포크본들이 돌아다니고 있어서 해보니까 왠지 안된다.&lt;/li&gt;
  &lt;li&gt;그래서 그냥 공식리포본에서 tensorflow_ros_cpp 를 빌드를 어찌저찌 해내기로 결심하였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;과정&quot;&gt;과정&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;수많은 삽질들이 있었으나 작동하는 사항만을 아래 기술해보자.&lt;/li&gt;
  &lt;li&gt;일단 모든 과정들을 virtual env 에서 하면 된다. (모든 캣킨 빌드들 및 로스실행 포함). tf 를 virtualenv 에서 설치하는 게 편하기 때문.&lt;/li&gt;
  &lt;li&gt;일단 segmap 저자의 리드미에서 하라는 순서로 해도되지만, 내 기준에 먼저 해놓으면 좋은것부터 이제 설명해보자.&lt;/li&gt;
  &lt;li&gt;일단 작업할 virtual environment 를 만들자.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- ```  --&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;virtualenv ~/segmapenv &lt;span class=&quot;c&quot;&gt;# 하면 만들어지고&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/segmapenv/bin/activate &lt;span class=&quot;c&quot;&gt;# 하면 접속됨 (conda activate 같은 것 같다). 그러면 앞에 (env name) 이 뜬다&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# 예시 &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;!-- ```  --&gt;

&lt;ul&gt;
  &lt;li&gt;그럼 ~/segmapenv 에 대충 mkdir home 해서 home 밑에서 작업해보자&lt;/li&gt;
  &lt;li&gt;이제 tensorflow 를 설치해야 한다.&lt;/li&gt;
  &lt;li&gt;간편하게 pip install 하면 안되냐 싶겠지만 segmap 이 쓰고 있는 tensorflow_ros_cpp 라는 모듈이 그러면 안된다.&lt;/li&gt;
  &lt;li&gt;간단히 이유를 설명하자면 tf를 설치하는 방법에는 세가지가 있다. 1. pip, 2. bazel, 3. tensorflow_catkin&lt;/li&gt;
  &lt;li&gt;1은 잘 알테고 (하지만 지금 이걸로 설치하면 마지막에 API가 달라서? segmapper 가 링크가 안된다 — 다른 디펜던시 컴파일 20분(-j32의 경우)~2시간 겨우겨우 기다리고 마지막에 빌드 에러나는 환장하는 상황을 볼 수 있다), 3은 몰라도 되고 (궁금하면 따로 찾아보자)&lt;/li&gt;
  &lt;li&gt;결론은 2번 방법인 bazel 로 설치해야 한다. bazel은 구글이 만든 빌드툴 어쩌구 저쩌구… 그렇다고 한다. 뭐 자세히 알필요는 없고&lt;/li&gt;
  &lt;li&gt;암튼 2로 해야 하는이유는 tensorflow_ros_cpp 의 깃 리포에 가보면 표들이 있는데 https://github.com/tradr-project/tensorflow_ros_cpp&lt;/li&gt;
  &lt;li&gt;Ubuntu 18.04 64bits, Python 2.7.6, ROS Melodic 의 경우 1.8.0 tf version을 사용할 시 bazel cpu 및 gpu 에만 체크표시가 되어있다. 여기 보면 ABI difference problems 라는 말이 나오는데 이게 위에서 말한 20분 기다리고 마지막에 에러날때 볼 수 있는 현상이다. 이 고생을 안하려면 무적권 bazel 로 설치하자.&lt;/li&gt;
  &lt;li&gt;https://github.com/tradr-project/tensorflow_ros_cpp#custom-compilation-of-tensorflow-using-bazel 를 읽어보자.&lt;/li&gt;
  &lt;li&gt;그나저나 일단 tf src를 받아야 한다. https://www.tensorflow.org/install/source?hl=ko 여기 잘 나와있음.&lt;/li&gt;
  &lt;li&gt;그래도 굳이 명령어들을 다시 적어주자면&lt;/li&gt;
  &lt;li&gt;cd ~/sgmapenv/home하고, 여기다가 tensorflow 를 받자&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git clone https://github.com/tensorflow/tensorflow.git&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;segmap저자는 1.8.0 을 쓰라고 하니 (언제적 버전이지만…) 시키는 대로 하자. 해당 브랜치로 변경해주어야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git checkout r1.8&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;직접 빌드하는경우 configuration 을 정해줘야 하므로 ./configure 하자. 근데 계속 엔터를 갈기면 된다. 즉 디폴트 옵션으로 하면됨.&lt;/li&gt;
  &lt;li&gt;우리는 1.8.0 을 빌드할것이므로&lt;/li&gt;
  &lt;li&gt;그리고 cpu 버전으로 일단 빌드하고 있다. 내 목표는 segmap까지도 필요없고 segmatch만 쓰는 것이므로…&lt;/li&gt;
  &lt;li&gt;따라서 다음 명령어를 해주면 된다. 고 나와있다. (근데 일케 하지마시오)&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;bazel build &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;opt //tensorflow/tools/pip_package:build_pip_package &lt;span class=&quot;c&quot;&gt;# 이 때 저 // 까지 모두 포함해서 한줄로 써주면 된다. 헷갈리면 다시 여기를 보자 https://www.tensorflow.org/install/source?hl=ko&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;근데 이대로 하면 안됨. 안된다. 안된다. 안된다. 안된다. 안된다.&lt;/li&gt;
  &lt;li&gt;https://www.tensorflow.org/install/source?hl=ko 여기서 하려고 하는거는 빌드를 직접 내 입맛에 맞는 configuration 으로 해서 결국에는 pip 패키지를 만들려고 하는건데&lt;/li&gt;
  &lt;li&gt;우리의 목표는 아예 pip 패키지 아니기 때문. segmap 리드미에 pip 로 설치하라 되어있는데 안됨&lt;/li&gt;
  &lt;li&gt;그래서 tensorflow_ros_cpp 의 리드미를 잘 읽어야 한다. 진짜루&lt;/li&gt;
  &lt;li&gt;https://github.com/tradr-project/tensorflow_ros_cpp#custom-compilation-of-tensorflow-using-bazel&lt;/li&gt;
  &lt;li&gt;이걸 미리 읽지않고 이틀을 날린 내 자신에게 반성을…&lt;/li&gt;
  &lt;li&gt;여기보면 뭐라 되어있냐면&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Follow the Installing TensorFlow from Sources guide up to “Configure the installation” (including), and build the C++ library with the following command:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bazel build --config=opt --define framework_shared_object=false tensorflow:libtensorflow_cc.so&lt;/code&gt; You don’t need to continue with the guide building or installing the pip package (but you might be interested, because a custom-built tensorflow can provide you with higher performance even in Python).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;그니껜 bazel을 쓰긴 쓸건데 https://www.tensorflow.org/install/source?hl=ko 있는대로 할필요 없다는 뜻임&lt;/li&gt;
  &lt;li&gt;그리고 bazel build 뒤에 붙은 옵션들도 tf site에 있는 것과 다른데 절대 위의 명령어로 해주어야 한다.&lt;/li&gt;
  &lt;li&gt;중요하니까 한번더 복붙&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;bazel build &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;opt &lt;span class=&quot;nt&quot;&gt;--define&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;framework_shared_object&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false &lt;/span&gt;tensorflow:libtensorflow_cc.so&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;근데 이 bazel build 라는 놈이 안될수 있다. 왜냐하면 bazel 이라는게 2018년 부터 발전해와서 API가 엄청 달라진듯 버전마다 (뇌피셜)&lt;/li&gt;
  &lt;li&gt;암튼 여기 잘나와 있다 http://nblog.syszone.co.kr/archives/9751&lt;/li&gt;
  &lt;li&gt;한줄요약 하면 tensorflow-1.8.0 를 bazel로 빌드하려면 Bazel 0.10.0 을 써야됨&lt;/li&gt;
  &lt;li&gt;다양한 bazel 버전들은 bazel 공식 깃 리포에 있으며 0.10.0 버전은 https://github.com/bazelbuild/bazel/releases/tag/0.10.0&lt;/li&gt;
  &lt;li&gt;여기서 아마 bazel-0.10.0-without-jdk-installer-linux-x86_64.sh 이거 아니면 bazel-0.10.0-installer-linux-x86_64.sh 이거를 설치하면 됨&lt;/li&gt;
  &lt;li&gt;나는 앞에걸로 한듯 — 근데 암튼 bazel 디펜던시로 jdk 설치해줘야 하는거같다 https://docs.bazel.build/versions/3.3.0/install.html 여기 보면 sudo apt install openjdk-11-jdk 해주자&lt;/li&gt;
  &lt;li&gt;일단 지금 다시 상기하자면 지금 virtualenv 가 activate 된 home 에서 하고 있음&lt;/li&gt;
  &lt;li&gt;chmod +x bazel-0.10.0-without-jdk-installer-linux-x86_64.sh 하고 ./bazel-0.10.0-without-jdk-installer-linux-x86_64.sh 하면 금방 깔린다.&lt;/li&gt;
  &lt;li&gt;그리고 나서 이제 다시 아까 하려면 걸 다시해주면 (아래 명령어)&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;bazel build &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;opt &lt;span class=&quot;nt&quot;&gt;--define&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;framework_shared_object&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false &lt;/span&gt;tensorflow:libtensorflow_cc.so&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;이거는 거의 1분 안되어서 금방 끝난다. 컴퓨터 사양마다 다를수있음.&lt;/li&gt;
  &lt;li&gt;암튼 이러면 어딘가에 libtensorflow_cc.so 가 생성되어있다. tensorflow_ros_cpp 는 cmakelist.txt 에 보면 이 so 파일을 찾아다가 빌드하는 식이다.&lt;/li&gt;
  &lt;li&gt;ㅇㅋ 그럼 이제는 tensorflow_ros_cpp 를 빌드해야 하니까 catkin ws 밑에 src에 이 리포를 받아주자&lt;/li&gt;
  &lt;li&gt;근데 여기서부터는 segmap저자가 필요한 디펜던시들을 한방에 모조리 git clone 해올수 있도록 wstool 로 잘 해놨으니 tensorflow_ros_cpp 도 받는 김에 다 받아오자.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; YOUR_VIRTUAL_ENV_PATH/home/segmap_ws/src
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;YOUR_VIRTUAL_ENV_PATH/home/segmap_ws/
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;catkin init
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;catkin config &lt;span class=&quot;nt&quot;&gt;--merge-devel&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;catkin config &lt;span class=&quot;nt&quot;&gt;--cmake-args&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-DCMAKE_BUILD_TYPE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Release
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;src
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git clone https://github.com/ethz-asl/segmap.git
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;wstool init
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;wstool merge segmap/dependencies.rosinstall
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;wstool update&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;이제 segmap_ws위치에서 catkin build tensorflow_ros_cpp 를 해서 tensorflow_ros_cpp 만 컴파일 해준다.&lt;/li&gt;
  &lt;li&gt;segmap 저자가 시키는대로 하면 https://github.com/ethz-asl/segmap/wiki/FAQ#q-issues-compiling-tensorflow_ros_cpp&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmappyenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;catkin build tensorflow_ros_cpp &lt;span class=&quot;nt&quot;&gt;--cmake-args&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-DFORCE_TF_PIP_SEARCH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ON&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;라고 해서 뒤에 이상한 옵션들이 더 붙는데 이거는 pip 도 찾긴찾을래? 라는 거 므로 사실 ON으로 하면 안된다. segmap_ws/src/tensorflow_ros_cpp 밑에 CMakeLists.txt 가 있는데 거기서 bazel 말고 다른애들을 찾을 가능성을 다 off 해주자. 즉&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-cmake&quot; data-lang=&quot;cmake&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# variables affecting the search for the tensorflow library&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;FORCE_TF_PIP_SEARCH OFF CACHE BOOL “Whether to search for pip-installed Tensorflow even on systems using C++11 ABI”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;DISABLE_TF_PIP_SEARCH ON CACHE BOOL “Whether to skip search for pip-installed Tensorflow”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;FORCE_TF_BAZEL_SEARCH ON CACHE BOOL “Whether to search for bazel-compiled Tensorflow even if tensorflow was already found”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;DISABLE_TF_BAZEL_SEARCH OFF CACHE BOOL “Whether to skip search for bazel-compiled Tensorflow”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;FORCE_TF_CATKIN_SEARCH OFF CACHE BOOL “Whether to search for tensorflow_catkin even if tensorflow was already found”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;DISABLE_TF_CATKIN_SEARCH ON CACHE BOOL “Whether to skip search for tensorflow_catkin”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;그리고 그 좀 아래에 보면 bazel로 tensorflow_ros_cpp를 빌드 할때 두 가지를 얘가 끌어다가 쓰는 걸 알수있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-cmake&quot; data-lang=&quot;cmake&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# variables affecting bazel search&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;TF_BAZEL_LIBRARY “{CATKIN_DEVEL_PREFIX}/../libtensorflow_cc.so” CACHE STRING “Path to the bazel-compiled Tensorflow C++ library”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;TF_BAZEL_SRC_DIR “{CATKIN_DEVEL_PREFIX}/../tensorflow-include-base” CACHE STRING “Path to the Tensorflow sources directory”&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;즉 .so 파일과 tensorflow source 가 필요한건데 tensorflow source 의 경로는 지금 어쩌구…../tensorflow-include-base 라고 되어있는데 귀찮으니까 절대경로로 바꿔주자. 아까 tensorflow git clone 했던 그 디렉토리를 해주면 된다.&lt;/li&gt;
  &lt;li&gt;그럼 이제 아까 bazel build 로 만든 .so 파일이 어딘가 있는데 일단 대충 탐색기에서보면 못찾겠다. 이때 find 명령어를 써주자&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; / 
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;find &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-name&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;libtensorflow_cc.so&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;해주니까 이상한 bazel의 숨김폴더 밑에 존재하는 걸 확인할 수 있었다.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;./home/user/.cache/bazel/_bazel_user/f61ec775ae98149c983e28ce5aff1318/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/libtensorflow_cc.so.runfiles/org_tensorflow/tensorflow/libtensorflow_cc.so
./home/user/.cache/bazel/_bazel_user/f61ec775ae98149c983e28ce5aff1318/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/libtensorflow_cc.so&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo cp&lt;/span&gt; ./home/user/.cache/bazel/_bazel_user/f61ec775ae98149c983e28ce5aff1318/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/libtensorflow_cc.so &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;어딘가붙여넣을경로&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;해서 보기좋은 곳으로 옮겨주자&lt;/li&gt;
  &lt;li&gt;그리고 다시 segmap_ws/src/tensorflow_ros_cpp 의 CMakeLists.txt 로 돌아가서 .so 를 찾는 라인에 경로를 저 내가 복사해놓은 어딘가붙여넣을 경로 로 바꿔주면 된다.&lt;/li&gt;
  &lt;li&gt;근데 기본적으로 {CATKIN_DEVEL_PREFIX}/../libtensorflow_cc.so 라고 되어 있는데 이거는 catkin workspace (devel, build, src있는 그 경로) 경로를 의미한다. 나는 cmakelist 에 저 - - 라인을 바꾸기 귀찮아서 so파일을 아예 workspace (devel build src와 같은 위치) 에 복사해버림.&lt;/li&gt;
  &lt;li&gt;그러고 나서 (아참 catkin_tools 를 미리 설치해야 함 그래야 catkin build 를 쓸수있음)&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;catkin build tensorflow_ros_cpp&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;하니까 한 30초 안걸려서 빌드가 다 됐다.&lt;/li&gt;
  &lt;li&gt;이제 진짜 마지막으로 아래 명령어를 해주면&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;catkin build segmapper&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;segmapper 및 segmapper 가 필요로 하는 디펜던시들이 모두 컴파일이 된다.&lt;/li&gt;
  &lt;li&gt;근데 pcl_catkin 이랑 gtsam이 진짜 개오래 걸린다. 그래서 조금이라도 더 빨리 해주기 위해서&lt;/li&gt;
  &lt;li&gt;나는 i9–9900 을 사용중이어서 코어가 16개 (virtual로는 총 32개) 여서&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;catkin build segmapper &lt;span class=&quot;nt&quot;&gt;-j32&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;라고 해주었다.&lt;/li&gt;
  &lt;li&gt;그러면 20분정도 걸린다. 램은 많이 쓸때는 40G까지 올라간듯 (64기가 장착중)&lt;/li&gt;
  &lt;li&gt;코어가 많아도 램딸리면 터질수있으니 이거는 자기 시스템에 맞게 조절바람.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그러면 마지막에&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;이런 화면이 뜨면 모두가 잘 빌드가 된것이다! ㅠㅠ&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
  &lt;img src=&quot;/figs/2021-03-02-segmap-build/img1.png&quot; alt=&quot;img1&quot; style=&quot;width:100%&quot; /&gt;
  &lt;figcaption&gt; &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;&lt;/span&gt;
&lt;!-- &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/figs/2021-03-02-segmap-build/img1.png&quot; width=700&gt;&lt;/p&gt; --&gt;
&lt;!-- ![dsf](/logo.png) --&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;source &lt;/span&gt;devel/setup.bash&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;를 해주고&lt;/li&gt;
  &lt;li&gt;저자가 올려둔 데이터 (KITTI 00 번과 05번, 각 16G, 10G) 를 http://robotics.ethz.ch/~asl-datasets/segmap/segmap_data/ 여기서 받아서 적절한 위치에 bag file 을 놓아두고&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;segmapenv&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;roslaunch segmapper kitti_loop_closure.launch&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;하면 되는데 gedit kitti_loop_closure.launch 해서 여기보면 bagfile 경로를 지정하게 되어있다. 그거를 내가 위치시킨 경로로 바꿔주면 아마 실행 될것임.&lt;/li&gt;
  &lt;li&gt;참고로 kitti_loop_closure.launch 가 17 ICRA의 segmatch이고 cnn_kitti_loop_closure.launch가 18 RSS의 segmap 이다. (암튼 segmatch를 쓰고싶어도 tensorflow_ros_cpp를 컴파일해야했던 ㅠㅠ)&lt;/li&gt;
  &lt;li&gt;근데 틀면 기본적으로 rviz 에서는 아무것도 안보여지는데 저자가 올려둔 rviz config 파일을 같이 틀어야 아마 보여질듯 http://robotics.ethz.ch/~asl-datasets/segmap/segmap_data/kitti/&lt;/li&gt;
  &lt;li&gt;근데 귀찮으니까 그냥 왼쪽 아래에서 add 해주고 target/representation인가…? 이거를 틀면 잘 실행되고 있음을 알 수 있다. 자세한건 정광욱님의 플레이 영상을 보면 참고가 됨. https://www.youtube.com/watch?v=Hb7Agk8fs10&amp;amp;t=3s&lt;/li&gt;
  &lt;li&gt;05번을 다 돌아보았다.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;img src=&quot;/figs/2021-03-02-segmap-build/img2.png&quot; alt=&quot;img2&quot; style=&quot;width:100%&quot; /&gt;
  &lt;figcaption&gt; &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;그나저나 다 돌고 런치파일을 실행한 터미널을 ctrl+C 하면 알아서 로그 파일과 결과파일을 /tmp/ 어딘가에 저장해준다. (어디 저장했다고 경로가 뜸)&lt;/li&gt;
  &lt;li&gt;암튼 돌리기 성공!&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;결론&quot;&gt;결론&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;segmatch를 돌려보았다.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>블로그 시작!</title>
   <link href="http://localhost:4000/blog/2021/03/01/blog-start.html"/>
   <updated>2021-03-01T00:00:00+09:00</updated>
   <id>http://localhost:4000/blog/2021/03/01/blog-start</id>
   <content type="html">&lt;p&gt;블로그를 시작해봅니다.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Publications</title>
   <link href="http://localhost:4000/publications/"/>
   <updated>2021-01-01T00:00:00+09:00</updated>
   <id>http://localhost:4000/publications</id>
   <content type="html">&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px; float:right;&quot;&gt;
    NOTE: publications are sorted in time order 
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;journals&quot;&gt;Journals&lt;/h2&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;j20sc2&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Scan Context++: Rotation and Lateral Invariant Spatial Descriptor for Robust and Fast Structural Place Recognition&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Sunwook Choi$^{†}$, Ayoung Kim. (†: joint work with NAVER LABS)&lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; submitted and under review, journal 2021 — Paper (TBA), Code (TBA) &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;An Improved LiDAR Place Recognition&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;ral19&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1-Day Learning, 1-Year Localization: Long-term LiDAR Localization using Scan Context Image&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Byungjae Park$^{†}$, Ayoung Kim. (†: joint work with ETRI)&lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; IEEE Robotics and Automation Letters, 2019 (with ICRA 2019) —  
    &lt;a href=&quot;/publications/gkim-2019-ral.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;,  
    &lt;a href=&quot;https://www.youtube.com/watch?v=apmmduXTnaE&quot; target=&quot;_blank&quot;&gt; Video&lt;/a&gt;,
    &lt;a href=&quot;https://www.dropbox.com/sh/pn01awfz7huys45/AABOEz3hJ2FLuhUkfjrsJs3Fa?dl=0&quot; target=&quot;_blank&quot;&gt; Slide&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;Classification-based Long-term LiDAR Localization&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;ceus19&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A new 3D space syntax metric based on 3D isovist capture in urban space using remote sensing technology&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Ayoung Kim, Youngchul Kim$^{†}$. (†: joint work with KAIST Urban Design Lab)&lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Computer, Environment and Urban Systems, 2019 — &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0198971518301881&quot; target=&quot;_blank&quot;&gt; Paper (online)&lt;/a&gt; &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;LiDAR-based Urban Site Analysis&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;conferences&quot;&gt;Conferences&lt;/h2&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;iros20&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Remove, then Revert: Static Point cloud Map Construction using Multiresolution Range Images&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot; id=&quot;iros20&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Ayoung Kim. &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; IROS 2020 —  
    &lt;a href=&quot;/publications/gskim-2020-iros.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;,  
    &lt;a href=&quot;https://github.com/irapkaist/removert&quot; target=&quot;_blank&quot;&gt; Code&lt;/a&gt;,
    &lt;a href=&quot;https://www.youtube.com/watch?v=M9PEGi5fAq8&quot; target=&quot;_blank&quot;&gt; Video&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;Remove Dynamic Points in the Wild&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;icra20undeeplo&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Unsupervised Geometry-Aware Deep LiDAR Odometry&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Younggun Cho, &lt;u&gt;Giseop Kim&lt;/u&gt;, Ayoung Kim. &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ICRA 2020 —  
    &lt;a href=&quot;/publications/ycho-2020-icra.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;,  
    &lt;a href=&quot;https://www.youtube.com/watch?v=-imRJXq6ZuE&quot; target=&quot;_blank&quot;&gt; Video&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;Learning LiDAR Odometry without Ground-truth&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;icra20mulran&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;MulRan: Multimodal Range Dataset for Urban Place Recognition&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Yeong Sang Park, Younghun Cho, Jinyong Jeong, Ayoung Kim. &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ICRA 2020 —  
    &lt;a href=&quot;/publications/gskim-2020-icra.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;,  
    &lt;a href=&quot;https://sites.google.com/view/mulran-pr/home&quot; target=&quot;_blank&quot;&gt; Dataset website&lt;/a&gt;,
    &lt;a href=&quot;https://github.com/irapkaist/scancontext/tree/master/fast_evaluator_radar&quot; target=&quot;_blank&quot;&gt; Code (radar place recognition)&lt;/a&gt;,
    &lt;a href=&quot;https://www.youtube.com/watch?v=qJi1KJmrM2U&quot; target=&quot;_blank&quot;&gt; Video&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;LiDAR+Radar SLAM Dataset and Radar Place Recognition&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;icra18sc&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Scan Context: Egocentric Spatial Descriptor for Place Recognition within 3D Point Cloud Map&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Ayoung Kim. &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; IROS 2018 —  
    &lt;a href=&quot;/publications/gkim-2018-iros.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;,  
    &lt;a href=&quot;https://github.com/irapkaist/scancontext&quot; target=&quot;_blank&quot;&gt; Code&lt;/a&gt;,
    &lt;a href=&quot;https://www.youtube.com/watch?v=_etNafgQXoY&quot; target=&quot;_blank&quot;&gt; Video&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;A LiDAR Place Recognition: Robust, Fast, and Versatile&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;workshops&quot;&gt;Workshops&lt;/h2&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;icra19wsjang&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CNN-based Approach for Opti-Acoustic Reciprocal Feature Matching&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Hyesu Jang, &lt;u&gt;Giseop Kim&lt;/u&gt;, Yeongjun Lee$^{†}$, Ayoung Kim. (†: joint work with KRISO)&lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ICRA 2019 Workshop on Underwater Robotics Perception —  
    &lt;a href=&quot;/publications/hsjang-2019-icra-ws.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;A preliminary report of &lt;a href=&quot;#kros20&quot;&gt;the KRoS20&lt;/a&gt; paper&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;icra18ws&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Learning Scan Context toward Long-term LiDAR Localization&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;u&gt;Giseop Kim&lt;/u&gt;, Byungjae Park$^{†}$, Ayoung Kim. (†: joint work with ETRI)&lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ICRA 2018 Workshop on Long-term autonomy  (won the &lt;a href=&quot;https://blockchair.com/bitcoin/transaction/7d23c8a6b6ea6c4acc3d6625cfb0aa5d8b91e6ea873a551f306fe17cb1ffa144#o=1&quot; target=&quot;_blank&quot;&gt; Best paper award&lt;/a&gt;) —  
    &lt;a href=&quot;/publications/gkim-2018-icraws.pdf&quot; target=&quot;_blank&quot;&gt; Paper&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;A preliminary report of &lt;a href=&quot;#ral19&quot;&gt;the RAL19&lt;/a&gt; paper&quot;  &lt;br /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;domestic-korean&quot;&gt;Domestic (Korean)&lt;/h2&gt;

&lt;p style=&quot;margin-top:-10px&quot;&gt; &lt;/p&gt;
&lt;p id=&quot;kros20&quot;&gt; &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CNN-based Opti-Acoustic Transformation for Underwater Feature Matching&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:13px;&quot;&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Hyesu Jang, Yeongjun Lee$^{†}$, &lt;u&gt;Giseop Kim&lt;/u&gt;, Ayoung Kim. (†: joint work with KRISO)&lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Journal of Korea Robotics Society 2020 (Special Issue: Underwater Robotics) —  
    &lt;a href=&quot;http://jkros.org/_common/do.php?a=full&amp;amp;b=33&amp;amp;bidx=2176&amp;amp;aidx=26014&quot; target=&quot;_blank&quot;&gt; Paper (online)&lt;/a&gt;
    &lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; tldr: &quot;Camera-sonar style transfer-based multimodal matching&quot;  &lt;br /&gt;
&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Projects</title>
   <link href="http://localhost:4000/projects/"/>
   <updated>2021-01-01T00:00:00+09:00</updated>
   <id>http://localhost:4000/projects</id>
   <content type="html">&lt;p&gt;TODO&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Latest News</title>
   <link href="http://localhost:4000/latestnews/"/>
   <updated>2021-01-01T00:00:00+09:00</updated>
   <id>http://localhost:4000/latestnews</id>
   <content type="html">&lt;p&gt;TODO&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>About Me</title>
   <link href="http://localhost:4000/aboutme/"/>
   <updated>2021-01-01T00:00:00+09:00</updated>
   <id>http://localhost:4000/aboutme</id>
   <content type="html">&lt;p style=&quot;margin-top:-15px&quot;&gt; &lt;/p&gt;

&lt;p style=&quot;font-size:14px;&quot;&gt;
&amp;nbsp; I am a Ph.D. student at Korea Advanced Institute of Science and Technology (KAIST). I am a member of the &lt;a href=&quot;https://irap.kaist.ac.kr/&quot; target=&quot;_blank&quot;&gt; IRAP lab &lt;/a&gt;, where I'm advised by Prof. &lt;a href=&quot;https://irap.kaist.ac.kr/~ayoung/&quot; target=&quot;_blank&quot;&gt; Ayoung Kim&lt;/a&gt;.
&lt;/p&gt;

&lt;p style=&quot;margin-top:-17px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:14px;&quot;&gt;
&amp;nbsp; My main research topic during the Ph.D. has been focused on robust 3D mapping in complex urban sites using LiDAR sensors. To do so, I have interests in all mobile robot-related topics and spatial AI including from 3D perception, sensor fusion, and SLAM to deep learning.  
&lt;/p&gt;

&lt;p style=&quot;margin-top:-17px&quot;&gt; &lt;/p&gt;
&lt;p style=&quot;font-size:14px;&quot;&gt;
&amp;nbsp; I have experience with a full pipeline of SLAM in the real-world (i.e., from sensor fusion, calibration, odometry, place retrieval, pose-graph optimization, multi-session localization, to long-term map management).
&lt;/p&gt;
</content>
 </entry>
 
 
</feed>
